{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea107ba1-8d64-4983-8383-c9647dac0715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "     active environment : bert\n",
      "    active env location : /home/vzhekova/miniconda3/envs/bert\n",
      "            shell level : 1\n",
      "       user config file : /home/vzhekova/.condarc\n",
      " populated config files : /home/vzhekova/.condarc\n",
      "          conda version : 23.7.2\n",
      "    conda-build version : not installed\n",
      "         python version : 3.9.16.final.0\n",
      "       virtual packages : __archspec=1=x86_64\n",
      "                          __cuda=11.2=0\n",
      "                          __glibc=2.23=0\n",
      "                          __linux=4.4.0=0\n",
      "                          __unix=0=0\n",
      "       base environment : /home/vzhekova/miniconda3  (writable)\n",
      "      conda av data dir : /home/vzhekova/miniconda3/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /home/vzhekova/miniconda3/pkgs\n",
      "                          /home/vzhekova/.conda/pkgs\n",
      "       envs directories : /home/vzhekova/miniconda3/envs\n",
      "                          /home/vzhekova/.conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/23.7.2 requests/2.31.0 CPython/3.9.16 Linux/4.4.0-133-generic ubuntu/16.04.5 glibc/2.23\n",
      "                UID:GID : 12591:131\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536da668-5b89-4966-bbb2-37260cd18e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f93f58-b85b-4cc7-b82f-549cb93f7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/export/data4/vzhekova/biases-data/Test_De/Statistics/BERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d4dec-995b-42dd-b824-050d02d80cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "filled = unmasker(\"Hello I'm a [MASK] model.\")\n",
    "for r in filled:\n",
    "    print(r['token_str'], \"->\", r['score'])\n",
    "print(filled[0]['token_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0424e-2872-4563-a88c-db70bc90af3c",
   "metadata": {},
   "source": [
    "# Extract sentences from MuST-SHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ab101e-7d44-4e9c-a64a-bfc427189f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/BERT\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fc32b51-7307-4824-826b-370853158ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 4th category of gender phenomena: 14M, 20F => 34 sentences total\n",
    "!awk -F '\\t' '$10 == \"4F\" || $10 == \"4M\" {print $5, $10}' en-fr.tsv > data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bd409-9beb-4b36-aa19-fb7f97d462bc",
   "metadata": {},
   "source": [
    "# Masking with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f34040-fbed-44a1-857a-a12813e18c6e",
   "metadata": {},
   "source": [
    "- For each sentence unmask each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c66417-a2a5-47ab-8729-5243376992a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[MASK]', 'now', 'Thomson', 'becomes', 'the', 'more', 'likely', 'suspect.\\n']\n",
      "but\n",
      "['So', '[MASK]', 'Thomson', 'becomes', 'the', 'more', 'likely', 'suspect.\\n']\n",
      ",\n",
      "['So', 'now', '[MASK]', 'becomes', 'the', 'more', 'likely', 'suspect.\\n']\n",
      "he\n",
      "['So', 'now', 'Thomson', '[MASK]', 'the', 'more', 'likely', 'suspect.\\n']\n",
      "was\n",
      "['So', 'now', 'Thomson', 'becomes', '[MASK]', 'more', 'likely', 'suspect.\\n']\n",
      "the\n",
      "['So', 'now', 'Thomson', 'becomes', 'the', '[MASK]', 'likely', 'suspect.\\n']\n",
      "most\n",
      "['So', 'now', 'Thomson', 'becomes', 'the', 'more', '[MASK]', 'suspect.\\n']\n",
      "likely\n",
      "['So', 'now', 'Thomson', 'becomes', 'the', 'more', 'likely', '[MASK]']\n",
      ".\n",
      "['[MASK]', 'was', 'one', 'black', 'professor', 'and', 'one', 'black', 'assistant', 'dean.\\n']\n",
      "there\n",
      "['There', '[MASK]', 'one', 'black', 'professor', 'and', 'one', 'black', 'assistant', 'dean.\\n']\n",
      "was\n",
      "['There', 'was', '[MASK]', 'black', 'professor', 'and', 'one', 'black', 'assistant', 'dean.\\n']\n",
      "one\n",
      "['There', 'was', 'one', '[MASK]', 'professor', 'and', 'one', 'black', 'assistant', 'dean.\\n']\n",
      "black\n",
      "['There', 'was', 'one', 'black', '[MASK]', 'and', 'one', 'black', 'assistant', 'dean.\\n']\n",
      "dean\n",
      "['There', 'was', 'one', 'black', 'professor', '[MASK]', 'one', 'black', 'assistant', 'dean.\\n']\n",
      "and\n",
      "['There', 'was', 'one', 'black', 'professor', 'and', '[MASK]', 'black', 'assistant', 'dean.\\n']\n",
      "one\n",
      "['There', 'was', 'one', 'black', 'professor', 'and', 'one', '[MASK]', 'assistant', 'dean.\\n']\n",
      "black\n",
      "['There', 'was', 'one', 'black', 'professor', 'and', 'one', 'black', '[MASK]', 'dean.\\n']\n",
      "assistant\n",
      "['There', 'was', 'one', 'black', 'professor', 'and', 'one', 'black', 'assistant', '[MASK]']\n",
      ".\n",
      "['[MASK]', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "we\n",
      "['We', '[MASK]', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "use\n",
      "['We', 'have', '[MASK]', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "different\n",
      "['We', 'have', 'our', '[MASK]', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "own\n",
      "['We', 'have', 'our', 'cognitive', '[MASK]', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "skills\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', '[MASK]', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "so\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', '[MASK]', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "that\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', '[MASK]', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "we\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', '[MASK]', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "can\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', '[MASK]', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "get\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', '[MASK]', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "a\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', '[MASK]', 'history', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "brief\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', '[MASK]', 'on', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "impression\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', '[MASK]', 'a', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "with\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', '[MASK]', 'patient', 'with', 'chest', 'pain.\\n']\n",
      "a\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', '[MASK]', 'with', 'chest', 'pain.\\n']\n",
      "patient\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', '[MASK]', 'chest', 'pain.\\n']\n",
      "with\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', '[MASK]', 'pain.\\n']\n",
      "severe\n",
      "['We', 'have', 'our', 'cognitive', 'biases,', 'so', 'that', 'I', 'can', 'take', 'a', 'perfect', 'history', 'on', 'a', 'patient', 'with', 'chest', '[MASK]']\n",
      ".\n",
      "['[MASK]', 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "call\n",
      "[\"That's\", '[MASK]', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "the\n",
      "[\"That's\", 'the', '[MASK]', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "one\n",
      "[\"That's\", 'the', 'officer', '[MASK]', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "who\n",
      "[\"That's\", 'the', 'officer', 'who', '[MASK]', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "called\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', '[MASK]', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "me\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', '[MASK]', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      ",\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', '[MASK]', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "and\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', '[MASK]', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "you\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', '[MASK]', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "think\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', '[MASK]', 'can', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "you\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', '[MASK]', 'have', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "should\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', '[MASK]', 'a', 'few', 'classes', 'with', 'us.\\n']\n",
      "take\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', '[MASK]', 'few', 'classes', 'with', 'us.\\n']\n",
      "a\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', '[MASK]', 'classes', 'with', 'us.\\n']\n",
      "few\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', '[MASK]', 'with', 'us.\\n']\n",
      "drinks\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', '[MASK]', 'us.\\n']\n",
      "with\n",
      "[\"That's\", 'the', 'officer', 'who', 'emailed', 'me', 'back,', 'saying', 'I', 'think', 'you', 'can', 'have', 'a', 'few', 'classes', 'with', '[MASK]']\n",
      ".\n",
      "['[MASK]', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "\"\n",
      "['Steve,', '[MASK]', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "the\n",
      "['Steve,', 'a', '[MASK]', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "friend\n",
      "['Steve,', 'a', 'physician,', '[MASK]', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "told\n",
      "['Steve,', 'a', 'physician,', 'told', '[MASK]', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "me\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', '[MASK]', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "about\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', '[MASK]', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "a\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', '[MASK]', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "nurse\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', '[MASK]', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "that\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', '[MASK]', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "i\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', '[MASK]', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "worked\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', '[MASK]', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "with\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', '[MASK]', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "who\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', '[MASK]', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "was\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', '[MASK]', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "usually\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', '[MASK]', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "very\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', '[MASK]', 'especially', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "friendly\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', '[MASK]', 'to', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "even\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', '[MASK]', 'junior', 'staff', 'and', 'nurses.\\n']\n",
      "to\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', '[MASK]', 'staff', 'and', 'nurses.\\n']\n",
      "hospital\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', '[MASK]', 'and', 'nurses.\\n']\n",
      "doctors\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', '[MASK]', 'nurses.\\n']\n",
      "and\n",
      "['Steve,', 'a', 'physician,', 'told', 'me', 'about', 'a', 'doctor', 'that', 'he', 'worked', 'with', 'who', 'was', 'never', 'very', 'respectful,', 'especially', 'to', 'junior', 'staff', 'and', '[MASK]']\n",
      ".\n",
      "['[MASK]', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "what\n",
      "['What', '[MASK]', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "do\n",
      "['What', 'do', '[MASK]', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "you\n",
      "['What', 'do', 'you', '[MASK]', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "mean\n",
      "['What', 'do', 'you', 'think', '[MASK]', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "the\n",
      "['What', 'do', 'you', 'think', 'a', '[MASK]', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "weighted\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', '[MASK]', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "order\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', '[MASK]', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "for\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', '[MASK]', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "a\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', '[MASK]', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "plastic\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', '[MASK]', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "surgeon\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', '[MASK]', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      ",\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', '[MASK]', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "a\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', '[MASK]', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "general\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', '[MASK]', 'or', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      ",\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', '[MASK]', 'an', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      ",\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', '[MASK]', 'orthopedic', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "an\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', '[MASK]', 'surgeon,', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "eye\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', '[MASK]', 'an', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n",
      "or\n",
      "['What', 'do', 'you', 'think', 'a', 'batting', 'average', 'for', 'a', 'cardiac', 'surgeon', 'or', 'a', 'nurse', 'practitioner', 'or', 'an', 'orthopedic', 'surgeon,', '[MASK]', 'OBGYN,', 'a', 'paramedic', 'is', 'supposed', 'to', 'be?\\n']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m sentenceWithMask[index_token] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentenceWithMask)\n\u001b[0;32m---> 16\u001b[0m unmasked \u001b[38;5;241m=\u001b[39m \u001b[43munmasker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentenceWithMask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(unmasked[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_str\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m index_token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/pipelines/fill_mask.py:239\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/pipelines/fill_mask.py:101\u001b[0m, in \u001b[0;36mFillMaskPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 101\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     model_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1373\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/pytorch_utils.py:240\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "with open('samples.txt') as in_file: \n",
    "    line_counter = 1\n",
    "    for line in in_file:\n",
    "        index_token = 0\n",
    "        sentence = line.split(' ')\n",
    "        #print(sentence)\n",
    "        filename = 'sen' + str(line_counter) + '.txt'\n",
    "        with open(filename, 'w') as out_file:  # Generate a file for each original sentence\n",
    "            for token in sentence:\n",
    "                sentenceWithMask = sentence.copy()\n",
    "                sentenceWithMask[index_token] = '[MASK]'\n",
    "                #print(sentenceWithMask)\n",
    "                unmasked = unmasker(' '.join(sentenceWithMask))\n",
    "                #print(unmasked[0]['token_str'])\n",
    "                index_token += 1\n",
    "                newSentence = list(map(lambda x: x.replace('[MASK]', unmasked[0]['token_str']), sentenceWithMask))\n",
    "                #print(newSentence)\n",
    "                print(' '.join(newSentence).replace('\\n', ''), end='\\n', file=out_file)\n",
    "        line_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba342a-f2e3-4789-8ee7-cf405b69cb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
