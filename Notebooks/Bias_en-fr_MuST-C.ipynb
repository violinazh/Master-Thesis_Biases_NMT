{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e322256a-12ac-482e-b44a-ae8eb5bf4c88",
   "metadata": {},
   "source": [
    "# En-Fr Translation\n",
    "Generate translations on MuST-C dataset with WMT14 En-Fr Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7e843c-f187-4799-9a34-5c8309fb8e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if we can connect to the GPU with PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb50f64a-34c1-4bb4-9fd8-2f9c1675a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/En-Fr_MuST-C\n"
     ]
    }
   ],
   "source": [
    "PATH=\"/export/data4/vzhekova/biases-data/En-Fr_MuST-C\"\n",
    "\n",
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70d73cda-759a-45a5-ac92-d3a6a651f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of English:\n",
      "\n",
      "Back in New York, I am the head of development for a non-profit called Robin Hood.\n",
      "When I'm not fighting poverty, I'm fighting fires as the assistant captain of a volunteer fire company.\n",
      "Now in our town, where the volunteers supplement a highly skilled career staff, you have to get to the fire scene pretty early to get in on any action.\n",
      "I remember my first fire.\n",
      "I was the second volunteer on the scene, so there was a pretty good chance I was going to get in.\n",
      "But still it was a real footrace against the other volunteers to get to the captain in charge to find out what our assignments would be.\n",
      "When I found the captain, he was having a very engaging conversation with the homeowner, who was surely having one of the worst days of her life.\n",
      "Here it was, the middle of the night, she was standing outside in the pouring rain, under an umbrella, in her pajamas, barefoot, while her house was in flames.\n",
      "The other volunteer who had arrived just before me — let's call him Lex Luther — (Laughter) got to the captain first and was asked to go inside and save the homeowner's dog. The dog! I was stunned with jealousy.\n",
      "Here was some lawyer or money manager who, for the rest of his life, gets to tell people that he went into a burning building to save a living creature, just because he beat me by five seconds.\n",
      "\n",
      "First lines of French:\n",
      "\n",
      "A New York, je suis responsable du développment pour un organisme à but non lucratif appelé Robin Hood.\n",
      "Quand je ne suis pas en train de combattre la pauvreté, je combat des incendies en tant qu'assistant capitaine d'une compagnie de pompiers volontaires.\n",
      "Et dans notre ville, où les volontaires viennent renforcer une équipe professionnelle hautement qualifiée, il faut arriver sur le lieu de l'incendie très tôt pour prendre part à l'action.\n",
      "Je me souviens de mon premier incendie.\n",
      "J'étais le deuxième volontaire sur les lieux, et donc j'avais de bonnes chances d'y aller.\n",
      "Mais pourtant c'était une vrai course à pied contre les autres volontaires pour arriver jusqu'au capitaine responsable pour découvrir ce que seraient nos missions.\n",
      "Quand j'ai trouvé le capitaine, il était en pleine conversation avec la propriétaire, qui était surement en train de vivre la pire journée de sa vie.\n",
      "C'était en pleine nuit, elle était là dehors sous la pluie battante, sous un parapluie, en pyjama, pieds nus, pendant que sa maison était en flammes.\n",
      "L'autre volontaire qui était arrivé juste avant moi — appelons-le Lex Luther — (Rires) est arrivé le premier auprès du capitaine et on lui a demandé d'aller à l'intérieur et de sauver le chien de la propriétaire. Le chien ! J'étais abasourdi de jalousie.\n",
      "C'était un avocat ou un brasseur d'argent qui, pour le reste de sa vie, pourrait dire aux gens qu'il était entré dans un bâtiment en feu pour sauver une créature vivante, juste parce qu'il m'avait battu de 5 secondes.\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of English:\\n\"\n",
    "!head tst.en-fr.en\n",
    "!echo -e \"\\nFirst lines of French:\\n\"\n",
    "!head tst.en-fr.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f796928-2616-4b1a-87c2-791af617f1b0",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19bbf8-4629-4aba-a51a-7f6ea31aa102",
   "metadata": {},
   "source": [
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "493058f2-f1c6-4070-b224-6f7507402c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "mpn = MosesPunctNormalizer()\n",
    "\n",
    "# Preprocess the sentences from train/dev/test sets\n",
    "for partition in [\"train\", \"dev\", \"tst\"]:\n",
    "    for lang in [\"fr\", \"en\"]:\n",
    "        mt_fr = MosesTokenizer(lang=lang)\n",
    "        with open(f\"{partition}.en-fr.{lang}\") as fin, open(f\"tok.{partition}.en-fr.{lang}\",'w') as fout:\n",
    "            for line in fin:\n",
    "                tokens = mt_en.tokenize(mpn.normalize(line), return_str=True)\n",
    "                print(tokens, end='\\n', file=fout) \n",
    "\n",
    "        \n",
    "\n",
    "print('Finished tokenizing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f557d-45e9-49b3-8f5a-d1544ac36e9b",
   "metadata": {},
   "source": [
    "- Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c357bc-7d53-4bd1-bc86-c052a679e601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|#####################################| 32000/32000 [08:16<00:00, 64.48it/s]\n",
      "100%|#####################################| 32000/32000 [08:01<00:00, 66.39it/s]\n",
      "/bin/bash: fin: No such file or directory\n",
      "/bin/bash: fin: No such file or directory\n",
      "/bin/bash: fin: No such file or directory\n",
      "/bin/bash: fin: No such file or directory\n",
      "/bin/bash: fin: No such file or directory\n",
      "/bin/bash: fin: No such file or directory\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Training subword model\n",
    "!subword-nmt learn-bpe -s 32000 < tok.train.en-fr.en > sw.model.en\n",
    "!subword-nmt learn-bpe -s 32000 < tok.train.en-fr.fr > sw.model.fr\n",
    "\n",
    "print('Finished subword training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ff6693-5bcc-4c55-8292-56b2dae06eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apllied subword model.\n"
     ]
    }
   ],
   "source": [
    "# Applying subword model\n",
    "for partition in [\"train\", \"dev\", \"tst\"]:\n",
    "    for lang in [\"fr\", \"en\"]:\n",
    "        sw = f\"sw.model.{lang}\"\n",
    "        fin = f\"tok.{partition}.en-fr.{lang}\"\n",
    "        fout = f\"sw.{partition}.en-fr.{lang}\"\n",
    "        !subword-nmt apply-bpe -c $sw < $fin > $fout\n",
    "        \n",
    "print('Apllied subword model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94d57d8a-48b3-4773-8755-0027fee444ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of tokenized English:\n",
      "\n",
      "Thank you so much , Chris . And it &apos;s truly a great honor to have the opportunity to come to this stage twice ; I &apos;m extremely grateful .\n",
      "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
      "And I say that sincerely , partly because ( Mo@@ ck so@@ b ) I need that . ( Laughter )\n",
      "( Laughter ) I flew on Air Force Two for eight years .\n",
      "( Laughter ) Now I have to take off my shoes or boots to get on an airplane ! ( Laughter ) ( Applause )\n",
      "I &apos;ll tell you one quick story to illustrate what that &apos;s been like for me . ( Laughter )\n",
      "It &apos;s a true story - every bit of this is true .\n",
      "Soon after Ti@@ pper and I left the - ( Mo@@ ck so@@ b ) White House - ( Laughter ) we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville . Dri@@ ving ourselves . ( Laughter )\n",
      "( Laughter ) I looked in the re@@ ar-@@ view mirror and all of a sudden it just hit me .\n",
      "There was no motor@@ cade back there .\n",
      "\n",
      "First lines of tokenized French:\n",
      "\n",
      "Merci beaucoup , Chris . C &apos;est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois .\n",
      "J &apos;ai été très impressionné par cette conférence , et je tiens à vous remercier tous pour vos nombreux et sympathiques commentaires sur ce que j &apos;ai dit l &apos;autre soir .\n",
      "Et je dis çà sincèrement , en autres parce que - Faux sang@@ lot - j &apos;en ai besoin ! - Rires - Met@@ tez-vous à ma place !\n",
      "J &apos;ai volé avec Air Force 2 pendant huit ans .\n",
      "Et maintenant , je dois enlever mes chaussures pour monter à bord d &apos;un avion ! - Rires Applaudissements -\n",
      "Je vais vous raconter une petite histoire pour vous montrer ce que çà a été pour moi .\n",
      "C &apos;est une histoire vraie , dans tous ses détails .\n",
      "Après que Ti@@ pper et moi avons quitté la - Faux sang@@ lot - Maison Blanche - Rires - nous étions en route pour une petite ferme que nous avons à 80 km à l &apos;est de Nashville - conduisant nous-mêmes .\n",
      "Je sais que ça doit vous paraître sans importance mais - Rires - en jetant un oeil sur le rétro@@ viseur , j &apos;ai réalisé tout à coup .\n",
      "Il n &apos;y avait pas d &apos;es@@ cor@@ te derrière .\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of tokenized English:\\n\"\n",
    "!head sw.train.en-fr.en\n",
    "\n",
    "!echo -e \"\\nFirst lines of tokenized French:\\n\"\n",
    "!head sw.train.en-fr.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a0cb1-a80c-4c8e-92eb-b20136ceea4f",
   "metadata": {},
   "source": [
    "- Binarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e184c4-cbae-4c7a-ac52-fd8b6537398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the data for training\n",
    "\n",
    "# map words appearing less than threshold times to unknown \n",
    "# reuse model dict\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --trainpref sw.train.en-fr \\\n",
    "    --validpref sw.dev.en-fr \\\n",
    "    --testpref sw.tst.en-fr \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/dict.en.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/dict.fr.txt \\\n",
    "    --destdir data-bin \\\n",
    "    --thresholdtgt 0 \\\n",
    "    --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe29a8e-b0ed-4950-bbe6-84b53d20aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 14:35:14 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin-test', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict='/export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/dict.en.txt', suppress_crashes=False, target_lang='fr', task='translation', tensorboard_logdir=None, testpref='sw.tst.en-fr', tgtdict='/export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/dict.fr.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='sw.train.en-fr', use_plasma_view=False, user_dir=None, validpref='sw.dev.en-fr', wandb_project=None, workers=8)\n",
      "2023-03-08 14:35:15 | INFO | fairseq_cli.preprocess | [en] Dictionary: 44512 types\n",
      "2023-03-08 14:36:07 | INFO | fairseq_cli.preprocess | [en] sw.train.en-fr.en: 275085 sents, 6447032 tokens, 5.55% replaced (by <unk>)\n",
      "2023-03-08 14:36:07 | INFO | fairseq_cli.preprocess | [en] Dictionary: 44512 types\n",
      "2023-03-08 14:36:09 | INFO | fairseq_cli.preprocess | [en] sw.dev.en-fr.en: 1412 sents, 34541 tokens, 5.11% replaced (by <unk>)\n",
      "2023-03-08 14:36:09 | INFO | fairseq_cli.preprocess | [en] Dictionary: 44512 types\n",
      "2023-03-08 14:36:11 | INFO | fairseq_cli.preprocess | [en] sw.tst.en-fr.en: 2632 sents, 58717 tokens, 4.94% replaced (by <unk>)\n",
      "2023-03-08 14:36:11 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 44512 types\n",
      "2023-03-08 14:37:13 | INFO | fairseq_cli.preprocess | [fr] sw.train.en-fr.fr: 275085 sents, 6857439 tokens, 9.6% replaced (by <unk>)\n",
      "2023-03-08 14:37:13 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 44512 types\n",
      "2023-03-08 14:37:16 | INFO | fairseq_cli.preprocess | [fr] sw.dev.en-fr.fr: 1412 sents, 36697 tokens, 9.1% replaced (by <unk>)\n",
      "2023-03-08 14:37:16 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 44512 types\n",
      "2023-03-08 14:37:19 | INFO | fairseq_cli.preprocess | [fr] sw.tst.en-fr.fr: 2632 sents, 63660 tokens, 8.79% replaced (by <unk>)\n",
      "2023-03-08 14:37:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin-test\n"
     ]
    }
   ],
   "source": [
    "# Binarize the data for test\n",
    "\n",
    "# map words appearing less than threshold times to unknown \n",
    "# reuse model dict; solution for dictionary size to match test dataset\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --trainpref sw.train.en-fr \\\n",
    "    --validpref sw.dev.en-fr \\\n",
    "    --testpref sw.tst.en-fr \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/dict.en.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/dict.fr.txt \\\n",
    "    --destdir data-bin-test \\\n",
    "    --thresholdtgt 0 \\\n",
    "    --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358bc05-4e64-4e72-adb6-4a7992c40534",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33db12c3-c5f0-4558-979d-b9609c7db520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/En-Fr_MuST-C\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1f87b-662c-492c-b931-582300a4b53d",
   "metadata": {},
   "source": [
    "- Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a85474f-5b19-4375-8a2a-89538a0b878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 14:38:05 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin-test', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-03-08 14:38:08 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2023-03-08 14:38:08 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2023-03-08 14:38:08 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt\n",
      "2023-03-08 14:39:07 | INFO | fairseq.data.data_utils | loaded 2,632 examples from: data-bin-test/test.en-fr.en\n",
      "2023-03-08 14:39:07 | INFO | fairseq.data.data_utils | loaded 2,632 examples from: data-bin-test/test.en-fr.fr\n",
      "2023-03-08 14:39:07 | INFO | fairseq.tasks.translation | data-bin-test test en-fr 2632 examples\n",
      "2023-03-08 14:42:11 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-03-08 14:42:11 | INFO | fairseq_cli.generate | Translated 2,632 sentences (65,514 tokens) in 67.5s (39.02 sentences/s, 971.17 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "!fairseq-generate data-bin-test  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --path /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt \\\n",
    "    --beam 5 \\\n",
    "    --batch-size 256 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe=subword_nmt > en-fr.decode.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2fd49-e7d2-4d9c-907c-b71787a41f6c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c94eda2-41da-41b5-8ed3-5592e04508f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C&apos; est pour compléter l&apos; expérience .\n",
      "lane : Je peux vous aider .\n",
      "Man : Cause , ma marque ?\n",
      "Pourquoi est @-@ il important d&apos; être prudent ?\n",
      "Quelles sont les autres marques comme ça ?\n",
      "De Seattle ... après une semaine .\n",
      "Vous pouvez donc avoir ce nuage .\n",
      "Vous pouvez zoomer très simplement .\n",
      "Et apparemment , il était assez populaire .\n",
      "Je vous remercie de votre attention .\n",
      "..........\n",
      "C &apos;est pour compléter l <<unk>> .\n",
      "<<unk>> : Je peux vous aider .\n",
      "Man : <<unk>> , ma marque ?\n",
      "Pourquoi <<unk>> important pour redémarrer ?\n",
      "Quelles autres marques sont comme ça ?\n",
      "De Seattle ... après une semaine .\n",
      "Alors vous pouvez avoir ce <<unk>> .\n",
      "Vous pouvez <<unk>> dessus très simplement .\n",
      "<<unk>> , c <<unk>> plutôt populaire .\n",
      "Mark Bezos : Merci .\n"
     ]
    }
   ],
   "source": [
    "# Extract the hypotheses and references from the decoding log file\n",
    "!grep ^H en-fr.decode.log | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp.txt\n",
    "!grep ^T en-fr.decode.log | sed 's/^T-//g' | cut -f 2 | sed 's/ @@//g' > ./ref.txt\n",
    "\n",
    "!head ./hyp.txt\n",
    "print(\"..........\")\n",
    "!head ./ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8560756f-60b3-4af2-ac38-897e030e5da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished detokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize text        \n",
    "\n",
    "md_en = MosesDetokenizer(lang='en')\n",
    "md_fr = MosesDetokenizer(lang='fr')\n",
    "\n",
    "with open('hyp.txt', encoding='utf8') as fin, open('hyp_detok.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('ref.txt', encoding='utf8') as fin, open('ref_detok.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_fr.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished detokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9274ae1-a009-4e59-aea1-951d390ae87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C' est pour compléter l' expérience.\n",
      "lane: Je peux vous aider.\n",
      "Man: Cause, ma marque?\n",
      "Pourquoi est-il important d' être prudent?\n",
      "Quelles sont les autres marques comme ça?\n",
      "De Seattle... après une semaine.\n",
      "Vous pouvez donc avoir ce nuage.\n",
      "Vous pouvez zoomer très simplement.\n",
      "Et apparemment, il était assez populaire.\n",
      "Je vous remercie de votre attention.\n",
      "..........\n",
      "C 'est pour compléter l <<unk>>.\n",
      "<<unk>> : Je peux vous aider.\n",
      "Man : <<unk>>, ma marque ?\n",
      "Pourquoi <<unk>> important pour redémarrer ?\n",
      "Quelles autres marques sont comme ça ?\n",
      "De Seattle... après une semaine.\n",
      "Alors vous pouvez avoir ce <<unk>>.\n",
      "Vous pouvez <<unk>> dessus très simplement.\n",
      "<<unk>>, c <<unk>> plutôt populaire.\n",
      "Mark Bezos : Merci.\n"
     ]
    }
   ],
   "source": [
    "!head ./hyp_detok.txt\n",
    "print(\"..........\")\n",
    "!head ./ref_detok.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84d0e08b-5b01-47e5-bec7-cd2305b164b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 20.3,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
      " \"verbose_score\": \"58.4/35.8/24.3/16.6 (BP = 0.668 ratio = 0.713 hyp_len = 57927 ref_len = 81282)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.3.1\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# BLEU score of 20.3 (beam=5)\n",
    "!cat ./hyp_detok.txt | sacrebleu ./ref_detok.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2543292-a7e0-42bf-8edb-f747ea3c9ea2",
   "metadata": {},
   "source": [
    "# Finetuning WMT14 En-Fr model on MuST-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca00a0-dbc8-4d50-be3b-9a537e56ca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 17:16:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_vaswani_wmt_en_fr_big', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_vaswani_wmt_en_fr_big', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='/export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=5, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-03-08 17:16:04 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2023-03-08 17:16:04 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2023-03-08 17:16:25 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=1024, out_features=44512, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-03-08 17:16:25 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2023-03-08 17:16:25 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2023-03-08 17:16:25 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-03-08 17:16:25 | INFO | fairseq_cli.train | num. shared model params: 267,517,952 (num. trained: 267,517,952)\n",
      "2023-03-08 17:16:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-03-08 17:16:25 | INFO | fairseq.data.data_utils | loaded 1,412 examples from: data-bin/valid.en-fr.en\n",
      "2023-03-08 17:16:25 | INFO | fairseq.data.data_utils | loaded 1,412 examples from: data-bin/valid.en-fr.fr\n",
      "2023-03-08 17:16:25 | INFO | fairseq.tasks.translation | data-bin valid en-fr 1412 examples\n",
      "2023-03-08 17:16:47 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-03-08 17:16:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-03-08 17:16:47 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     \n",
      "2023-03-08 17:16:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-03-08 17:16:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-03-08 17:16:47 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2023-03-08 17:16:47 | INFO | fairseq.checkpoint_utils | loading pretrained model from /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt: optimizer, lr scheduler, meters, dataloader will be reset\n",
      "2023-03-08 17:16:47 | INFO | fairseq.trainer | Preparing to load checkpoint /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt\n",
      "2023-03-08 17:17:28 | INFO | fairseq.trainer | Loaded checkpoint /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt (epoch 29 @ 0 updates)\n",
      "2023-03-08 17:17:28 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-03-08 17:17:28 | INFO | fairseq.data.data_utils | loaded 275,085 examples from: data-bin/train.en-fr.en\n",
      "2023-03-08 17:17:28 | INFO | fairseq.data.data_utils | loaded 275,085 examples from: data-bin/train.en-fr.fr\n",
      "2023-03-08 17:17:28 | INFO | fairseq.tasks.translation | data-bin train en-fr 275085 examples\n",
      "2023-03-08 17:17:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1886\n",
      "epoch 001:   0%|                                       | 0/1886 [00:00<?, ?it/s]2023-03-08 17:17:29 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-03-08 17:17:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/vzhekova/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001: 100%|▉| 1885/1886 [31:49<00:00,  1.00it/s, loss=4.066, nll_loss=2.4172023-03-08 17:49:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   6%|▍      | 1/17 [00:00<00:14,  1.11it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  12%|▊      | 2/17 [00:01<00:07,  1.90it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  18%|█▏     | 3/17 [00:01<00:05,  2.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24%|█▋     | 4/17 [00:01<00:04,  2.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29%|██     | 5/17 [00:01<00:03,  3.03it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  35%|██▍    | 6/17 [00:02<00:03,  3.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  41%|██▉    | 7/17 [00:02<00:03,  3.26it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  47%|███▎   | 8/17 [00:02<00:02,  3.35it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  53%|███▋   | 9/17 [00:03<00:02,  3.48it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  59%|███▌  | 10/17 [00:03<00:02,  3.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  65%|███▉  | 11/17 [00:03<00:01,  3.28it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71%|████▏ | 12/17 [00:04<00:01,  3.25it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  76%|████▌ | 13/17 [00:04<00:01,  3.29it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82%|████▉ | 14/17 [00:04<00:00,  3.26it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|█████▎| 15/17 [00:04<00:00,  3.34it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  94%|█████▋| 16/17 [00:05<00:00,  3.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 17/17 [00:05<00:00,  4.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-03-08 17:49:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.907 | nll_loss 2.184 | ppl 4.54 | wps 7971.6 | wpb 2158.6 | bsz 83.1 | num_updates 1886\n",
      "2023-03-08 17:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1886 updates\n",
      "2023-03-08 17:49:25 | INFO | fairseq.trainer | Saving checkpoint to /export/data4/vzhekova/biases-data/En-Fr_MuST-C/checkpoints/checkpoint1.pt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin \\\n",
    "    --arch transformer_vaswani_wmt_en_fr_big --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --max-epoch 5 \\\n",
    "    --finetune-from-model /export/data4/vzhekova/biases-data/En-Fr/wmt14.en-fr.joined-dict.transformer/model.pt\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30652b59-81ec-4e35-b3b2-c0da8ad8120b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
