{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416b2005-931f-4e35-ac54-ec1f1857c0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vzhekova/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bart.base',\n",
       " 'bart.large',\n",
       " 'bart.large.cnn',\n",
       " 'bart.large.mnli',\n",
       " 'bart.large.xsum',\n",
       " 'bpe',\n",
       " 'camembert',\n",
       " 'camembert-base',\n",
       " 'camembert-base-ccnet',\n",
       " 'camembert-base-ccnet-4gb',\n",
       " 'camembert-base-oscar-4gb',\n",
       " 'camembert-base-wikipedia-4gb',\n",
       " 'camembert-large',\n",
       " 'camembert.v0',\n",
       " 'conv.stories',\n",
       " 'conv.stories.pretrained',\n",
       " 'conv.wmt14.en-de',\n",
       " 'conv.wmt14.en-fr',\n",
       " 'conv.wmt17.en-de',\n",
       " 'data.stories',\n",
       " 'dynamicconv.glu.wmt14.en-fr',\n",
       " 'dynamicconv.glu.wmt16.en-de',\n",
       " 'dynamicconv.glu.wmt17.en-de',\n",
       " 'dynamicconv.glu.wmt17.zh-en',\n",
       " 'dynamicconv.no_glu.iwslt14.de-en',\n",
       " 'dynamicconv.no_glu.wmt16.en-de',\n",
       " 'fastspeech2-en-200_speaker-cv4',\n",
       " 'fastspeech2-en-ljspeech',\n",
       " 'gottbert-base',\n",
       " 'lightconv.glu.wmt14.en-fr',\n",
       " 'lightconv.glu.wmt16.en-de',\n",
       " 'lightconv.glu.wmt17.en-de',\n",
       " 'lightconv.glu.wmt17.zh-en',\n",
       " 'lightconv.no_glu.iwslt14.de-en',\n",
       " 'lightconv.no_glu.wmt16.en-de',\n",
       " 'roberta.base',\n",
       " 'roberta.large',\n",
       " 'roberta.large.mnli',\n",
       " 'roberta.large.wsc',\n",
       " 's2t_transformer_l-en-asr-librispeech',\n",
       " 's2t_transformer_m-en-asr-librispeech',\n",
       " 's2t_transformer_s-en-asr-librispeech',\n",
       " 'tokenizer',\n",
       " 'transformer.flores101.mm100.175M',\n",
       " 'transformer.flores101.mm100.615M',\n",
       " 'transformer.wmt14.en-fr',\n",
       " 'transformer.wmt16.en-de',\n",
       " 'transformer.wmt18.en-de',\n",
       " 'transformer.wmt19.de-en',\n",
       " 'transformer.wmt19.de-en.single_model',\n",
       " 'transformer.wmt19.en-de',\n",
       " 'transformer.wmt19.en-de.single_model',\n",
       " 'transformer.wmt19.en-ru',\n",
       " 'transformer.wmt19.en-ru.single_model',\n",
       " 'transformer.wmt19.ru-en',\n",
       " 'transformer.wmt19.ru-en.single_model',\n",
       " 'transformer.wmt20.en-iu.news',\n",
       " 'transformer.wmt20.en-iu.nh',\n",
       " 'transformer.wmt20.en-ta',\n",
       " 'transformer.wmt20.iu-en.news',\n",
       " 'transformer.wmt20.iu-en.nh',\n",
       " 'transformer.wmt20.ta-en',\n",
       " 'transformer_lm.gbw.adaptive_huge',\n",
       " 'transformer_lm.wiki103.adaptive',\n",
       " 'transformer_lm.wmt19.de',\n",
       " 'transformer_lm.wmt19.en',\n",
       " 'transformer_lm.wmt19.ru',\n",
       " 'transformer_lm.wmt20.en',\n",
       " 'transformer_lm.wmt20.iu.news',\n",
       " 'transformer_lm.wmt20.iu.nh',\n",
       " 'transformer_lm.wmt20.ta',\n",
       " 'tts_transformer-ar-cv7_css10',\n",
       " 'tts_transformer-en-200_speaker-cv4',\n",
       " 'tts_transformer-en-ljspeech',\n",
       " 'tts_transformer-es-css10',\n",
       " 'tts_transformer-fr-cv7_css10',\n",
       " 'tts_transformer-ru-cv7_css10',\n",
       " 'tts_transformer-tr-cv7_css10',\n",
       " 'tts_transformer-vi-cv7',\n",
       " 'tts_transformer-zh-cv7_css10',\n",
       " 'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS',\n",
       " 'unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10_dur',\n",
       " 'unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur',\n",
       " 'xlmr.base',\n",
       " 'xlmr.large',\n",
       " 'xlmr.xl',\n",
       " 'xlmr.xxl',\n",
       " 'xm_transformer-21_en-xls_r_1b',\n",
       " 'xm_transformer-21_en-xls_r_2b',\n",
       " 'xm_transformer-21_en-xls_r_300m',\n",
       " 'xm_transformer-22_16-xls_r_2b',\n",
       " 'xm_transformer-en_15-xls_r_1b',\n",
       " 'xm_transformer-en_15-xls_r_2b',\n",
       " 'xm_transformer-en_15-xls_r_300m',\n",
       " 'xm_transformer_600m-en_ar-multi_domain',\n",
       " 'xm_transformer_600m-en_es-multi_domain',\n",
       " 'xm_transformer_600m-en_fr-multi_domain',\n",
       " 'xm_transformer_600m-en_ru-multi_domain',\n",
       " 'xm_transformer_600m-en_tr-multi_domain',\n",
       " 'xm_transformer_600m-en_vi-multi_domain',\n",
       " 'xm_transformer_600m-en_zh-multi_domain',\n",
       " 'xm_transformer_600m-es_en-multi_domain',\n",
       " 'xm_transformer_600m-fr_en-multi_domain',\n",
       " 'xm_transformer_600m-ru_en-multi_domain',\n",
       " 'xm_transformer_s2ut_800m-en-es-st_plus_asr',\n",
       " 'xm_transformer_s2ut_800m-en-hk-h1_2022',\n",
       " 'xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022',\n",
       " 'xm_transformer_s2ut_800m-hk-en-h1_2022',\n",
       " 'xmod.base',\n",
       " 'xmod.base.13.125k',\n",
       " 'xmod.base.30.125k',\n",
       " 'xmod.base.30.195k',\n",
       " 'xmod.base.60.125k',\n",
       " 'xmod.base.60.265k',\n",
       " 'xmod.base.75.125k',\n",
       " 'xmod.base.75.269k',\n",
       " 'xmod.large.prenorm']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List available models\n",
    "torch.hub.list('pytorch/fairseq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033e5fbf-850a-4102-86a5-fc7b6141830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vzhekova/.cache/torch/hub/pytorch_fairseq_main\n",
      "2022-11-25 11:57:27 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 not found in cache, downloading to /tmp/tmphmqsorl3\n",
      "100%|██████████████████████████████████████████████████████████████| 2316140317/2316140317 [01:51<00:00, 20854854.40B/s]\n",
      "2022-11-25 11:59:20 | INFO | fairseq.file_utils | copying /tmp/tmphmqsorl3 to cache at /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2022-11-25 12:00:09 | INFO | fairseq.file_utils | creating metadata file for /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2022-11-25 12:00:09 | INFO | fairseq.file_utils | removing temp file /tmp/tmphmqsorl3\n",
      "2022-11-25 12:00:10 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2022-11-25 12:00:10 | INFO | fairseq.file_utils | extracting archive file /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae to temp dir /tmp/tmpnbh5jfjt\n",
      "2022-11-25 12:19:13 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2022-11-25 12:19:13 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2022-11-25 12:19:27 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_vaswani_wmt_en_de_big', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_vaswani_wmt_en_de_big', attention_dropout=0.0, bpe='subword_nmt', bpe_codes='/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', checkpoint_activations=False, clip_norm=0.0, criterion='label_smoothed_cross_entropy', cross_self_attention=False, data='/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, distributed_rank=0, distributed_world_size=128, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fp16=True, label_smoothing=0.1, layernorm_embedding=False, log_format='json', log_interval=10, lr=[0.0007], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=80000, merge_src_tgt_embed=False, momentum=0.99, no_cross_attention=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_scale_embedding=False, no_token_positional_embeddings=False, offload_activations=False, optimizer='adam', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, relu_dropout=0.0, restore_file='checkpoint_last.pt', sample_without_replacement=0, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', save_interval=1, seed=2, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang='en', stop_min_lr=1e-09, target_lang='fr', task='translation', tie_adaptive_weights=False, tokenizer='moses', train_subset='train', update_freq=[1.0], valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0), 'task': {'_name': 'translation', 'data': '/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    }
   ],
   "source": [
    "# Note: WMT'19 models use fastBPE instead of subword_nmt\n",
    "# Load an En-Fr Transformer model trained on WMT'14 data:\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "en2de.eval()  # disable dropout\n",
    "\n",
    "# Access the underlying TransformerModel\n",
    "assert isinstance(en2de.models[0], torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30f09fa-65d3-43ea-8f01-99c81c418029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerModel(\n",
       "      (encoder): TransformerEncoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): TransformerDecoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (output_projection): Linear(in_features=1024, out_features=44512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU for faster translation\n",
    "en2de.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0de4b6-361c-4fff-b78e-1aad4a574007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour à tous !'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate a sentence\n",
    "en2de.translate('Hello world!')\n",
    "# 'Hallo Welt!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf088f9-35f7-4742-9da7-32d89d1214c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour à tous !', 'Le chat était assis sur le tapis.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched translation\n",
    "en2de.translate(['Hello world!', 'The cat sat on the mat.'])\n",
    "# ['Hallo Welt!', 'Die Katze saß auf der Matte.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66b5663e-9b69-4e32-98f0-9a626b80991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data\n"
     ]
    }
   ],
   "source": [
    "%cd /export/data4/vzhekova/biases-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9dca8c96-5bd9-4d3f-9a1f-f7a388c944d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of English:\n",
      "\n",
      "﻿Now, I thought, \"How could I really capture this?\n",
      "I mean, from this entry, it would seem that I was born into a world that perceived someone like me to have nothing positive whatsoever going for them, when in fact, today I'm celebrated for the opportunities and adventures my life has procured.\n",
      "So, I immediately went to look up the 2009 online edition, expecting to find a revision worth noting.\n",
      "His name was Dr. Pizzutillo, an Italian American, whose name, apparently, was too difficult for most Americans to pronounce, so he went by Dr. P. And Dr. P always wore really colorful bow ties and had the very perfect disposition to work with children.\n",
      "And, one day, he came in to my session — exhaustive and unforgiving, these sessions — and he said to me, \"Wow.\n",
      "Now, of course, this was a simple ploy on Dr. P's part to get me to do the exercises I didn't want to do before the prospect of being the richest five-year-old in the second floor ward, but what he effectively did for me was reshape an awful daily occurrence into a new and promising experience for me.\n",
      "And I have to wonder today to what extent his vision and his declaration of me as a strong and powerful little girl shaped my own view of myself as an inherently strong, powerful and athletic person well into the future.\n",
      "And so my mother's prenatal physician had gone on vacation, so the man who delivered me was a complete stranger to my parents.\n",
      "And, because I was born without the fibula bones, and had feet turned in, and a few toes in this foot and a few toes in that, he had to be the bearer — this stranger had to be the bearer of bad news.\n",
      "And, because I was born without the fibula bones, and had feet turned in, and a few toes in this foot and a few toes in that, he had to be the bearer — this stranger had to be the bearer of bad news.\n",
      "\n",
      "First lines of French:\n",
      "\n",
      "﻿Je me suis demandé comment je pourrais résumer visuellement cela.\n",
      "Je veux dire, d'après cette entrée, il semblerait que je sois née dans un monde qui percevrait les gens comme moi comme n'ayant absolument rien pour eux, alors qu'en fait, aujourd'hui, je suis célébrée pour les opportunités et les aventures que ma vie m'a apportée.\n",
      "Alors je suis tout de suite allée regarder dans l'édition 2009 en m'attendant à trouver une révision notable.\n",
      "Son nom est Dr Pizzutillo. Un Italo-Américain, dont le nom, apparemment, était trop difficile à prononcer pour la plupart des Américains, alors il est devenu Dr P. Et Dr. P portait toujours des noeuds papillon très colorés et avait le plus parfait tempérament pour travailler avec des enfants.\n",
      "Et un jour il est venu à ma séance - des séances épuisantes et sans pitié — et il m'a dit \"Wahou.\n",
      "Bien sûr, c'était juste un stratagème de la part de Dr P ♪ pour me faire faire ces exercices que je ne voulais pas faire dans la perspective de devenir la fille de 5 ans la plus riche de l'étage, mais ce qu'il a vraiment fait pour moi, ça a été de transformer un affreux événement quotidien en une expérience nouvelle et prometteuse.\n",
      "Et je me demande aujourd'hui, dans quelle mesure sa vision, et le fait qu'il m'ait déclaré une petite fille forte et puissante, ont dessiné ma propre vision de moi-même loin dans le futur, comme une personne par nature forte, puissante et athlétique.\n",
      "Et donc le médecin prénatal de ma mère était parti en vacances, et l'homme qui m'a mise au monde était un étranger complet pour mes parents.\n",
      "Et, comme je suis née sans tibia, et que j'avais les pieds tournés vers l'intérieur, et quelques orteils sur ce pied-ci, et quelques orteils sur ce pied-là, il a dû être le porteur, cet étranger a dû être le porteur de la mauvaise nouvelle.\n",
      "Et, comme je suis née sans tibia, et que j'avais les pieds tournés vers l'intérieur, et quelques orteils sur ce pied-ci, et quelques orteils sur ce pied-là, il a dû être le porteur, cet étranger a dû être le porteur de la mauvaise nouvelle.\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of English:\\n\"\n",
    "!head data.en-fr.en\n",
    "!echo -e \"\\nFirst lines of French:\\n\"\n",
    "!head data.en-fr.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "49a31954-1bac-4f40-92a9-bce45df99f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "mt = MosesTokenizer(lang='en')\n",
    "\n",
    "with open('data.en-fr.en', encoding='utf8') as fin, open('data.en-fr.tok.en','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt.tokenize(line, return_str=True)\n",
    "        print(tokens, end='\\n', file=fout) \n",
    "        \n",
    "mt = MosesTokenizer(lang='fr')\n",
    "\n",
    "with open('data.en-fr.fr', encoding='utf8') as fin, open('data.en-fr.tok.fr','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt.tokenize(line, return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished tokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "44a05e66-8fce-416a-b8e1-098bc2da3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|###############7                    | 4366/10000 [00:06<00:03, 1534.10it/s]no pair has frequency >= 2. Stopping\n",
      " 45%|################7                    | 4536/10000 [00:07<00:08, 640.34it/s]\n",
      " 50%|##################4                  | 4995/10000 [00:10<00:14, 336.16it/s]no pair has frequency >= 2. Stopping\n",
      " 50%|##################5                  | 5016/10000 [00:10<00:10, 471.65it/s]\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Dividing tokenized text into subword units\n",
    "\n",
    "!subword-nmt learn-bpe -s 10000 < data.en-fr.tok.en > sw.model.en\n",
    "!subword-nmt apply-bpe -c sw.model.en < data.en-fr.tok.en > sw.data.en-fr.en\n",
    "\n",
    "!subword-nmt learn-bpe -s 10000 < data.en-fr.tok.fr > sw.model.fr\n",
    "!subword-nmt apply-bpe -c sw.model.fr < data.en-fr.tok.fr > sw.data.en-fr.fr\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e36c9ea3-3a74-4eb2-9e19-960ccec689a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of tokenized English:\n",
      "\n",
      "﻿ Now , I thought , &quot; How could I really capture this ?\n",
      "I mean , from this ent@@ ry , it would seem that I was born into a world that percei@@ ved someone like me to have nothing positive whatsoever going for them , when in fact , today I &apos;m celebr@@ ated for the oppor@@ t@@ uni@@ ties and adventures my life has pro@@ cur@@ ed .\n",
      "So , I immediately went to look up the 200@@ 9 online e@@ dition , expec@@ ting to find a re@@ vision wor@@ th n@@ oting .\n",
      "His name was Dr. P@@ i@@ zz@@ u@@ til@@ lo , an Italian American , whose name , apparently , was too difficult for most Americ@@ ans to pr@@ on@@ ounce , so he went by Dr. P@@ . And Dr. P always wor@@ e really colorful bow ties and had the very perfect disp@@ os@@ i@@ tion to work with children .\n",
      "And , one day , he came in to my session — exha@@ us@@ tive and un@@ for@@ giving , these sessions — and he said to me , &quot; W@@ ow .\n",
      "Now , of course , this was a simple p@@ loy on Dr. P &apos;s part to get me to do the exer@@ cis@@ es I didn &apos;t want to do before the prospect of being the richest five-year-old in the second floor ward , but what he effec@@ tively did for me was re@@ shape an aw@@ ful daily occur@@ r@@ ence into a new and promising experience for me .\n",
      "And I have to won@@ der today to what ext@@ ent his vision and his declar@@ ation of me as a strong and powerful little girl shaped my own view of myself as an in@@ her@@ ently strong , powerful and athletic person well into the future .\n",
      "And so my mother &apos;s pr@@ en@@ at@@ al physician had gone on vac@@ ation , so the man who deliver@@ ed me was a complete stranger to my parents .\n",
      "And , because I was born without the fibula bones , and had feet turned in , and a few toes in this foot and a few toes in that , he had to be the bearer — this stranger had to be the bearer of bad news .\n",
      "And , because I was born without the fibula bones , and had feet turned in , and a few toes in this foot and a few toes in that , he had to be the bearer — this stranger had to be the bearer of bad news .\n",
      "\n",
      "First lines of tokenized French:\n",
      "\n",
      "﻿ Je me suis demandé comment je pourrais ré@@ su@@ mer vi@@ su@@ ellement cela .\n",
      "Je veux dire , d&apos; après cette entrée , il sembl@@ erait que je sois née dans un monde qui perce@@ vrait les gens comme moi comme n&apos; ayant absolument rien pour eux , alors qu&apos; en fait , aujourd&apos; hui , je suis céléb@@ rée pour les opportuni@@ tés et les aventu@@ res que ma vie m&apos; a ap@@ port@@ ée .\n",
      "Alors je suis tout de suite allée regarder dans l&apos; édi@@ tion 2009 en m&apos; att@@ endant à trouver une ré@@ vision n@@ ot@@ able .\n",
      "Son nom est Dr P@@ i@@ zz@@ uti@@ ll@@ o . Un I@@ tal@@ o-@@ Américain , dont le nom , apparemment , était trop difficile à pron@@ oncer pour la plupart des Améric@@ ains , alors il est devenu Dr P@@ . Et Dr . P portait toujours des no@@ eu@@ ds papi@@ llon très co@@ lor@@ és et avait le plus parfait tempér@@ am@@ ent pour travailler avec des enfants .\n",
      "Et un jour il est venu à ma sé@@ ance - des séances épuis@@ antes et sans pi@@ tié — et il m&apos; a dit &quot; W@@ ah@@ ou .\n",
      "Bien sûr , c&apos; était juste un str@@ at@@ ag@@ ème de la part de Dr P ♪ pour me faire faire ces exerci@@ ces que je ne voulais pas faire dans la pers@@ pe@@ cti@@ ve de devenir la fille de 5 ans la plus riche de l&apos; ét@@ age , mais ce qu&apos; il a vraiment fait pour moi , ça a été de transformer un aff@@ r@@ eux événement quotidien en une expérience nouvelle et prometteuse .\n",
      "Et je me demande aujourd&apos; hui , dans quelle mesure sa vision , et le fait qu&apos; il m&apos; ait déclaré une petite fille forte et puissante , ont dess@@ iné ma propre vision de moi-même loin dans le futur , comme une personne par nature forte , puissante et athlétique .\n",
      "Et donc le médecin pré@@ n@@ at@@ al de ma mère était parti en vac@@ ances , et l&apos; homme qui m&apos; a mise au monde était un étranger compl@@ et pour mes parents .\n",
      "Et , comme je suis née sans tibia , et que j&apos; avais les pieds tournés vers l&apos; intérieur , et quelques orteils sur ce pied-ci , et quelques orteils sur ce pied-là , il a dû être le porteur , cet étranger a dû être le porteur de la mauvaise nouvelle .\n",
      "Et , comme je suis née sans tibia , et que j&apos; avais les pieds tournés vers l&apos; intérieur , et quelques orteils sur ce pied-ci , et quelques orteils sur ce pied-là , il a dû être le porteur , cet étranger a dû être le porteur de la mauvaise nouvelle .\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of tokenized English:\\n\"\n",
    "!head sw.data.en-fr.en\n",
    "\n",
    "!echo -e \"\\nFirst lines of tokenized French:\\n\"\n",
    "!head sw.data.en-fr.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "035e9211-725c-486f-ace6-0cc622f9fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:32:38 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict='wmt14.en-fr.fconv-py/dict.en.txt', suppress_crashes=False, target_lang='fr', task='translation', tensorboard_logdir=None, testpref='sw.data.en-fr', tgtdict='wmt14.en-fr.fconv-py/dict.fr.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=1)\n",
      "2023-01-13 12:32:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 43771 types\n",
      "2023-01-13 12:32:39 | INFO | fairseq_cli.preprocess | [en] sw.data.en-fr.en: 1108 sents, 38883 tokens, 4.77% replaced (by <unk>)\n",
      "2023-01-13 12:32:39 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 43807 types\n",
      "2023-01-13 12:32:40 | INFO | fairseq_cli.preprocess | [fr] sw.data.en-fr.fr: 1108 sents, 40580 tokens, 6.92% replaced (by <unk>)\n",
      "2023-01-13 12:32:40 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
     ]
    }
   ],
   "source": [
    "# Binarize the data for training; test with moses tokenizer and subword_nmt failed -> BLEU score of 4\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --testpref sw.data.en-fr \\\n",
    "    --srcdict wmt14.en-fr.fconv-py/dict.en.txt \\\n",
    "    --tgtdict wmt14.en-fr.fconv-py/dict.fr.txt \\\n",
    "    # --tokenizer\tmoses \\\n",
    "    # --bpe subword_nmt \\\n",
    "    --destdir data-bin \\\n",
    "    --thresholdtgt 0 \\\n",
    "    --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc698b33-d31f-4046-98ce-76c1cc617ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data\n"
     ]
    }
   ],
   "source": [
    "%cd /export/data4/vzhekova/biases-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b0264b10-bdb8-439e-bd09-856241d499b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-13 15:12:58 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'wmt14.en-fr.fconv-py/model.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-01-13 15:13:00 | INFO | fairseq.tasks.translation | [en] dictionary: 43771 types\n",
      "2023-01-13 15:13:00 | INFO | fairseq.tasks.translation | [fr] dictionary: 43807 types\n",
      "2023-01-13 15:13:00 | INFO | fairseq_cli.generate | loading model(s) from wmt14.en-fr.fconv-py/model.pt\n",
      "2023-01-13 15:13:24 | INFO | fairseq.data.data_utils | loaded 1,108 examples from: data-bin/test.en-fr.en\n",
      "2023-01-13 15:13:24 | INFO | fairseq.data.data_utils | loaded 1,108 examples from: data-bin/test.en-fr.fr\n",
      "2023-01-13 15:13:24 | INFO | fairseq.tasks.translation | data-bin test en-fr 1108 examples\n",
      "2023-01-13 15:14:20 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-01-13 15:14:20 | INFO | fairseq_cli.generate | Translated 1,108 sentences (42,853 tokens) in 27.7s (39.93 sentences/s, 1544.30 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "!fairseq-generate data-bin  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --path wmt14.en-fr.fconv-py/model.pt \\\n",
    "    --beam 1 \\\n",
    "    --batch-size 256 \\\n",
    "    --post-process \\\n",
    "    --remove-bpe=subword_nmt > en-fr.decode.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0f16baf3-949a-4346-8d5c-3c78c0a9de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je voulais ouvrir ces jeux comme un allié de la communauté LGBTQ .\n",
      "Au début de mon séjour là @-@ bas en 2000 , j&apos; ai été très intéressé par les communautés .\n",
      "Après ça , j&apos; ai été un peu de la bonne façon d&apos; aller à ma prochaine date de zero .\n",
      "Je suis allé à la maison de la femme pour la célébrer et la congratuler .\n",
      "Et je pensais que c&apos; était super cool , alors je l&apos; ai montré à mon ami .\n",
      "Je m&apos; appelle Dan , je suis un partenaire dans une consultance créative mondiale .\n",
      "Je suis retourné à mon laboratoire à Boston et j&apos; ai fait une petite expérience .\n",
      "Et le juge l&apos; a cerné comme un adulte , mais je vois cela .\n",
      "Ici , nous avons une jeune femme et un homme qui jouent à un jeu d&apos; aumôniers .\n",
      "J&apos; ai eu le grand privilège , lorsque j&apos; étais un jeune avocat , de rencontrer des membres du Groupe des parcs .\n",
      "..........\n",
      "Je <<unk>> ouvrir ces portes en tant qu&apos; alliée de la communauté LGBTQ .\n",
      "À mes débuts en 2000 , j&apos; étais très intéressé par les communautés .\n",
      "Après lui , j&apos; étais un peu <<unk>> pour mon prochain <<unk>> zéro .\n",
      "Je suis allée chez <<unk>> pour célébrer cela , pour la fé<<unk>> ter .\n",
      "Je <<unk>> ça super et donc j&apos; étais <<unk>> de le montrer à mon ami .\n",
      "Je suis Dan , un associé dans un cabinet international de conseil en créativité .\n",
      "Je suis <<unk>> dans mon <<unk>> à Boston et fait une petite expérience .\n",
      "Et le juge l&apos; a certifié adulte , mais je vois ce <<unk>> .\n",
      "Voici ici , une jeune <<unk>> et un male qui s&apos; adonnent à une course poursuite .\n",
      "J&apos; ai eu le grand privilège , quand j&apos; étais un jeune avocat , de rencontrer <<unk>> Parks .\n"
     ]
    }
   ],
   "source": [
    "# Extract the hypotheses and references from the decoding log file\n",
    "!grep ^H en-fr.decode.log | sed 's/^H-//g' | cut -f 3 | sed 's/ ##//g' > ./hyp.txt\n",
    "!grep ^T en-fr.decode.log | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > ./ref.txt\n",
    "\n",
    "!head ./hyp.txt\n",
    "print(\"..........\")\n",
    "!head ./ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "04b7966d-1948-4c16-bc1d-924c1f34181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 26.2,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
      " \"verbose_score\": \"55.8/36.3/26.2/18.8 (BP = 0.828 ratio = 0.841 hyp_len = 44280 ref_len = 52639)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.3.1\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Evaluate the model; BLEU score of 26.7 (beam=5), BLEU score of 26.2 (beam=1)\n",
    "!cat ./hyp.txt | sacrebleu ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a333bb5-9758-48fb-9dc1-380af6a4e70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
