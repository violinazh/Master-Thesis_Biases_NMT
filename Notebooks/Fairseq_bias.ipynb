{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416b2005-931f-4e35-ac54-ec1f1857c0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vzhekova/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bart.base',\n",
       " 'bart.large',\n",
       " 'bart.large.cnn',\n",
       " 'bart.large.mnli',\n",
       " 'bart.large.xsum',\n",
       " 'bpe',\n",
       " 'camembert',\n",
       " 'camembert-base',\n",
       " 'camembert-base-ccnet',\n",
       " 'camembert-base-ccnet-4gb',\n",
       " 'camembert-base-oscar-4gb',\n",
       " 'camembert-base-wikipedia-4gb',\n",
       " 'camembert-large',\n",
       " 'camembert.v0',\n",
       " 'conv.stories',\n",
       " 'conv.stories.pretrained',\n",
       " 'conv.wmt14.en-de',\n",
       " 'conv.wmt14.en-fr',\n",
       " 'conv.wmt17.en-de',\n",
       " 'data.stories',\n",
       " 'dynamicconv.glu.wmt14.en-fr',\n",
       " 'dynamicconv.glu.wmt16.en-de',\n",
       " 'dynamicconv.glu.wmt17.en-de',\n",
       " 'dynamicconv.glu.wmt17.zh-en',\n",
       " 'dynamicconv.no_glu.iwslt14.de-en',\n",
       " 'dynamicconv.no_glu.wmt16.en-de',\n",
       " 'fastspeech2-en-200_speaker-cv4',\n",
       " 'fastspeech2-en-ljspeech',\n",
       " 'gottbert-base',\n",
       " 'lightconv.glu.wmt14.en-fr',\n",
       " 'lightconv.glu.wmt16.en-de',\n",
       " 'lightconv.glu.wmt17.en-de',\n",
       " 'lightconv.glu.wmt17.zh-en',\n",
       " 'lightconv.no_glu.iwslt14.de-en',\n",
       " 'lightconv.no_glu.wmt16.en-de',\n",
       " 'roberta.base',\n",
       " 'roberta.large',\n",
       " 'roberta.large.mnli',\n",
       " 'roberta.large.wsc',\n",
       " 's2t_transformer_l-en-asr-librispeech',\n",
       " 's2t_transformer_m-en-asr-librispeech',\n",
       " 's2t_transformer_s-en-asr-librispeech',\n",
       " 'tokenizer',\n",
       " 'transformer.flores101.mm100.175M',\n",
       " 'transformer.flores101.mm100.615M',\n",
       " 'transformer.wmt14.en-fr',\n",
       " 'transformer.wmt16.en-de',\n",
       " 'transformer.wmt18.en-de',\n",
       " 'transformer.wmt19.de-en',\n",
       " 'transformer.wmt19.de-en.single_model',\n",
       " 'transformer.wmt19.en-de',\n",
       " 'transformer.wmt19.en-de.single_model',\n",
       " 'transformer.wmt19.en-ru',\n",
       " 'transformer.wmt19.en-ru.single_model',\n",
       " 'transformer.wmt19.ru-en',\n",
       " 'transformer.wmt19.ru-en.single_model',\n",
       " 'transformer.wmt20.en-iu.news',\n",
       " 'transformer.wmt20.en-iu.nh',\n",
       " 'transformer.wmt20.en-ta',\n",
       " 'transformer.wmt20.iu-en.news',\n",
       " 'transformer.wmt20.iu-en.nh',\n",
       " 'transformer.wmt20.ta-en',\n",
       " 'transformer_lm.gbw.adaptive_huge',\n",
       " 'transformer_lm.wiki103.adaptive',\n",
       " 'transformer_lm.wmt19.de',\n",
       " 'transformer_lm.wmt19.en',\n",
       " 'transformer_lm.wmt19.ru',\n",
       " 'transformer_lm.wmt20.en',\n",
       " 'transformer_lm.wmt20.iu.news',\n",
       " 'transformer_lm.wmt20.iu.nh',\n",
       " 'transformer_lm.wmt20.ta',\n",
       " 'tts_transformer-ar-cv7_css10',\n",
       " 'tts_transformer-en-200_speaker-cv4',\n",
       " 'tts_transformer-en-ljspeech',\n",
       " 'tts_transformer-es-css10',\n",
       " 'tts_transformer-fr-cv7_css10',\n",
       " 'tts_transformer-ru-cv7_css10',\n",
       " 'tts_transformer-tr-cv7_css10',\n",
       " 'tts_transformer-vi-cv7',\n",
       " 'tts_transformer-zh-cv7_css10',\n",
       " 'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS',\n",
       " 'unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_es_css10_dur',\n",
       " 'unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur',\n",
       " 'xlmr.base',\n",
       " 'xlmr.large',\n",
       " 'xlmr.xl',\n",
       " 'xlmr.xxl',\n",
       " 'xm_transformer-21_en-xls_r_1b',\n",
       " 'xm_transformer-21_en-xls_r_2b',\n",
       " 'xm_transformer-21_en-xls_r_300m',\n",
       " 'xm_transformer-22_16-xls_r_2b',\n",
       " 'xm_transformer-en_15-xls_r_1b',\n",
       " 'xm_transformer-en_15-xls_r_2b',\n",
       " 'xm_transformer-en_15-xls_r_300m',\n",
       " 'xm_transformer_600m-en_ar-multi_domain',\n",
       " 'xm_transformer_600m-en_es-multi_domain',\n",
       " 'xm_transformer_600m-en_fr-multi_domain',\n",
       " 'xm_transformer_600m-en_ru-multi_domain',\n",
       " 'xm_transformer_600m-en_tr-multi_domain',\n",
       " 'xm_transformer_600m-en_vi-multi_domain',\n",
       " 'xm_transformer_600m-en_zh-multi_domain',\n",
       " 'xm_transformer_600m-es_en-multi_domain',\n",
       " 'xm_transformer_600m-fr_en-multi_domain',\n",
       " 'xm_transformer_600m-ru_en-multi_domain',\n",
       " 'xm_transformer_s2ut_800m-en-es-st_plus_asr',\n",
       " 'xm_transformer_s2ut_800m-en-hk-h1_2022',\n",
       " 'xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022',\n",
       " 'xm_transformer_s2ut_800m-hk-en-h1_2022',\n",
       " 'xmod.base',\n",
       " 'xmod.base.13.125k',\n",
       " 'xmod.base.30.125k',\n",
       " 'xmod.base.30.195k',\n",
       " 'xmod.base.60.125k',\n",
       " 'xmod.base.60.265k',\n",
       " 'xmod.base.75.125k',\n",
       " 'xmod.base.75.269k',\n",
       " 'xmod.large.prenorm']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List available models\n",
    "torch.hub.list('pytorch/fairseq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033e5fbf-850a-4102-86a5-fc7b6141830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vzhekova/.cache/torch/hub/pytorch_fairseq_main\n",
      "2022-11-25 11:57:27 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 not found in cache, downloading to /tmp/tmphmqsorl3\n",
      "100%|██████████████████████████████████████████████████████████████| 2316140317/2316140317 [01:51<00:00, 20854854.40B/s]\n",
      "2022-11-25 11:59:20 | INFO | fairseq.file_utils | copying /tmp/tmphmqsorl3 to cache at /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2022-11-25 12:00:09 | INFO | fairseq.file_utils | creating metadata file for /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2022-11-25 12:00:09 | INFO | fairseq.file_utils | removing temp file /tmp/tmphmqsorl3\n",
      "2022-11-25 12:00:10 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2022-11-25 12:00:10 | INFO | fairseq.file_utils | extracting archive file /home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae to temp dir /tmp/tmpnbh5jfjt\n",
      "2022-11-25 12:19:13 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2022-11-25 12:19:13 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2022-11-25 12:19:27 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_vaswani_wmt_en_de_big', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_vaswani_wmt_en_de_big', attention_dropout=0.0, bpe='subword_nmt', bpe_codes='/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', checkpoint_activations=False, clip_norm=0.0, criterion='label_smoothed_cross_entropy', cross_self_attention=False, data='/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, distributed_rank=0, distributed_world_size=128, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fp16=True, label_smoothing=0.1, layernorm_embedding=False, log_format='json', log_interval=10, lr=[0.0007], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=80000, merge_src_tgt_embed=False, momentum=0.99, no_cross_attention=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_scale_embedding=False, no_token_positional_embeddings=False, offload_activations=False, optimizer='adam', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, relu_dropout=0.0, restore_file='checkpoint_last.pt', sample_without_replacement=0, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', save_interval=1, seed=2, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang='en', stop_min_lr=1e-09, target_lang='fr', task='translation', tie_adaptive_weights=False, tokenizer='moses', train_subset='train', update_freq=[1.0], valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0), 'task': {'_name': 'translation', 'data': '/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/home/vzhekova/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    }
   ],
   "source": [
    "# Note: WMT'19 models use fastBPE instead of subword_nmt\n",
    "# Load an En-Fr Transformer model trained on WMT'14 data:\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "en2de.eval()  # disable dropout\n",
    "\n",
    "# Access the underlying TransformerModel\n",
    "assert isinstance(en2de.models[0], torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30f09fa-65d3-43ea-8f01-99c81c418029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerModel(\n",
       "      (encoder): TransformerEncoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): TransformerDecoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (output_projection): Linear(in_features=1024, out_features=44512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU for faster translation\n",
    "en2de.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0de4b6-361c-4fff-b78e-1aad4a574007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour à tous !'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate a sentence\n",
    "en2de.translate('Hello world!')\n",
    "# 'Hallo Welt!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf088f9-35f7-4742-9da7-32d89d1214c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour à tous !', 'Le chat était assis sur le tapis.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched translation\n",
    "en2de.translate(['Hello world!', 'The cat sat on the mat.'])\n",
    "# ['Hallo Welt!', 'Die Katze saß auf der Matte.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66b5663e-9b69-4e32-98f0-9a626b80991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data\n"
     ]
    }
   ],
   "source": [
    "%cd /export/data4/vzhekova/biases-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dca8c96-5bd9-4d3f-9a1f-f7a388c944d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of English:\n",
      "\n",
      "﻿Now, I thought, \"How could I really capture this?\n",
      "I mean, from this entry, it would seem that I was born into a world that perceived someone like me to have nothing positive whatsoever going for them, when in fact, today I'm celebrated for the opportunities and adventures my life has procured.\n",
      "So, I immediately went to look up the 2009 online edition, expecting to find a revision worth noting.\n",
      "His name was Dr. Pizzutillo, an Italian American, whose name, apparently, was too difficult for most Americans to pronounce, so he went by Dr. P. And Dr. P always wore really colorful bow ties and had the very perfect disposition to work with children.\n",
      "And, one day, he came in to my session — exhaustive and unforgiving, these sessions — and he said to me, \"Wow.\n",
      "Now, of course, this was a simple ploy on Dr. P's part to get me to do the exercises I didn't want to do before the prospect of being the richest five-year-old in the second floor ward, but what he effectively did for me was reshape an awful daily occurrence into a new and promising experience for me.\n",
      "And I have to wonder today to what extent his vision and his declaration of me as a strong and powerful little girl shaped my own view of myself as an inherently strong, powerful and athletic person well into the future.\n",
      "And so my mother's prenatal physician had gone on vacation, so the man who delivered me was a complete stranger to my parents.\n",
      "And, because I was born without the fibula bones, and had feet turned in, and a few toes in this foot and a few toes in that, he had to be the bearer — this stranger had to be the bearer of bad news.\n",
      "And, because I was born without the fibula bones, and had feet turned in, and a few toes in this foot and a few toes in that, he had to be the bearer — this stranger had to be the bearer of bad news.\n",
      "\n",
      "First lines of French:\n",
      "\n",
      "﻿Je me suis demandé comment je pourrais résumer visuellement cela.\n",
      "Je veux dire, d'après cette entrée, il semblerait que je sois née dans un monde qui percevrait les gens comme moi comme n'ayant absolument rien pour eux, alors qu'en fait, aujourd'hui, je suis célébrée pour les opportunités et les aventures que ma vie m'a apportée.\n",
      "Alors je suis tout de suite allée regarder dans l'édition 2009 en m'attendant à trouver une révision notable.\n",
      "Son nom est Dr Pizzutillo. Un Italo-Américain, dont le nom, apparemment, était trop difficile à prononcer pour la plupart des Américains, alors il est devenu Dr P. Et Dr. P portait toujours des noeuds papillon très colorés et avait le plus parfait tempérament pour travailler avec des enfants.\n",
      "Et un jour il est venu à ma séance - des séances épuisantes et sans pitié — et il m'a dit \"Wahou.\n",
      "Bien sûr, c'était juste un stratagème de la part de Dr P ♪ pour me faire faire ces exercices que je ne voulais pas faire dans la perspective de devenir la fille de 5 ans la plus riche de l'étage, mais ce qu'il a vraiment fait pour moi, ça a été de transformer un affreux événement quotidien en une expérience nouvelle et prometteuse.\n",
      "Et je me demande aujourd'hui, dans quelle mesure sa vision, et le fait qu'il m'ait déclaré une petite fille forte et puissante, ont dessiné ma propre vision de moi-même loin dans le futur, comme une personne par nature forte, puissante et athlétique.\n",
      "Et donc le médecin prénatal de ma mère était parti en vacances, et l'homme qui m'a mise au monde était un étranger complet pour mes parents.\n",
      "Et, comme je suis née sans tibia, et que j'avais les pieds tournés vers l'intérieur, et quelques orteils sur ce pied-ci, et quelques orteils sur ce pied-là, il a dû être le porteur, cet étranger a dû être le porteur de la mauvaise nouvelle.\n",
      "Et, comme je suis née sans tibia, et que j'avais les pieds tournés vers l'intérieur, et quelques orteils sur ce pied-ci, et quelques orteils sur ce pied-là, il a dû être le porteur, cet étranger a dû être le porteur de la mauvaise nouvelle.\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of English:\\n\"\n",
    "!head data.en-fr.en\n",
    "!echo -e \"\\nFirst lines of French:\\n\"\n",
    "!head data.en-fr.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49a31954-1bac-4f40-92a9-bce45df99f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "mt = MosesTokenizer(lang='en')\n",
    "\n",
    "with open('data.en-fr.en', encoding='utf8') as fin, open('data.en-fr.tok.en','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt.tokenize(line, return_str=True)\n",
    "        print(tokens, end='\\n', file=fout) \n",
    "        \n",
    "mt = MosesTokenizer(lang='fr')\n",
    "\n",
    "with open('data.en-fr.fr', encoding='utf8') as fin, open('data.en-fr.tok.fr','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt.tokenize(line, return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished tokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44a05e66-8fce-416a-b8e1-098bc2da3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|###############9                    | 4440/10000 [00:07<00:03, 1577.89it/s]no pair has frequency >= 2. Stopping\n",
      " 45%|################7                    | 4536/10000 [00:07<00:08, 617.52it/s]\n",
      " 50%|##################5                  | 5016/10000 [00:10<00:15, 328.78it/s]no pair has frequency >= 2. Stopping\n",
      " 50%|##################5                  | 5016/10000 [00:10<00:10, 459.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dividing tokenized text into subword units\n",
    "\n",
    "!subword-nmt learn-bpe -s 10000 < data.en-fr.tok.en > sw.model.en\n",
    "!subword-nmt apply-bpe -c sw.model.en < data.en-fr.en > sw.data.en-fr.en\n",
    "\n",
    "!subword-nmt learn-bpe -s 10000 < data.en-fr.tok.fr > sw.model.fr\n",
    "!subword-nmt apply-bpe -c sw.model.fr < data.en-fr.fr > sw.data.en-fr.fr\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e36c9ea3-3a74-4eb2-9e19-960ccec689a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of tokenized English:\n",
      "\n",
      "﻿@@ N@@ ow@@ , I thou@@ ght@@ , \"@@ How could I really capture th@@ is@@ ?\n",
      "I mean@@ , from this ent@@ ry@@ , it would seem that I was born into a world that percei@@ ved someone like me to have nothing positive whatsoever going for them@@ , when in fac@@ t@@ , today I@@ '@@ m celebr@@ ated for the oppor@@ t@@ uni@@ ties and adventures my life has pro@@ c@@ u@@ re@@ d@@ .\n",
      "S@@ o@@ , I immediately went to look up the 200@@ 9 online e@@ di@@ tion@@ , expec@@ ting to find a re@@ vision wor@@ th n@@ ot@@ ing@@ .\n",
      "His name was Dr. P@@ i@@ zz@@ u@@ til@@ lo@@ , an Italian Americ@@ an@@ , whose na@@ me@@ , appar@@ ent@@ l@@ y@@ , was too difficult for most Americ@@ ans to pr@@ on@@ oun@@ c@@ e@@ , so he went by Dr. P@@ . And Dr. P always wor@@ e really colorful bow ties and had the very perfect disp@@ os@@ i@@ tion to work with chil@@ d@@ ren@@ .\n",
      "An@@ d@@ , one da@@ y@@ , he came in to my session — exha@@ us@@ tive and un@@ for@@ gi@@ v@@ ing@@ , these sessions — and he said to me@@ , \"@@ W@@ ow@@ .\n",
      "N@@ ow@@ , of cour@@ se@@ , this was a simple p@@ loy on Dr. P@@ '@@ s part to get me to do the exer@@ cis@@ es I di@@ d@@ n@@ '@@ t want to do before the prospect of being the richest five-year-old in the second floor war@@ d@@ , but what he effec@@ tively did for me was re@@ shape an aw@@ ful daily occur@@ r@@ ence into a new and promising experience for me@@ .\n",
      "And I have to won@@ der today to what ext@@ ent his vision and his declar@@ ation of me as a strong and powerful little girl shaped my own view of myself as an in@@ her@@ ently str@@ ong@@ , powerful and athletic person well into the f@@ ut@@ u@@ re@@ .\n",
      "And so my mo@@ ther@@ '@@ s pr@@ en@@ at@@ al physician had gone on vac@@ a@@ tion@@ , so the man who deliver@@ ed me was a complete stranger to my par@@ ent@@ s.\n",
      "An@@ d@@ , because I was born without the fibula b@@ on@@ es@@ , and had feet turned in@@ , and a few toes in this foot and a few toes in th@@ at@@ , he had to be the bearer — this stranger had to be the bearer of bad new@@ s.\n",
      "An@@ d@@ , because I was born without the fibula b@@ on@@ es@@ , and had feet turned in@@ , and a few toes in this foot and a few toes in th@@ at@@ , he had to be the bearer — this stranger had to be the bearer of bad new@@ s.\n",
      "\n",
      "First lines of tokenized French:\n",
      "\n",
      "﻿@@ Je me suis demandé comment je pourrais ré@@ su@@ mer vi@@ su@@ ellement cel@@ a@@ .\n",
      "Je veux dire@@ , d@@ '@@ après cette entr@@ é@@ e@@ , il sembl@@ erait que je sois née dans un monde qui perce@@ vrait les gens comme moi comme n@@ '@@ ayant absolument rien pour eu@@ x@@ , alors qu@@ '@@ en fai@@ t@@ , au@@ jour@@ d@@ '@@ hu@@ i@@ , je suis céléb@@ rée pour les opportuni@@ tés et les aventu@@ res que ma vie m@@ '@@ a ap@@ port@@ é@@ e@@ .\n",
      "Alors je suis tout de suite allée regarder dans l@@ '@@ édi@@ tion 2009 en m@@ '@@ att@@ endant à trouver une ré@@ vision n@@ ot@@ ab@@ le@@ .\n",
      "Son nom est Dr P@@ i@@ zz@@ uti@@ ll@@ o@@ . Un I@@ tal@@ o-@@ Améric@@ ain@@ , dont le nom@@ , appar@@ em@@ ment@@ , était trop difficile à pron@@ oncer pour la plupart des Améric@@ ain@@ s@@ , alors il est devenu Dr P@@ . Et D@@ r@@ . P portait toujours des no@@ eu@@ ds papi@@ llon très co@@ lor@@ és et avait le plus parfait tempér@@ am@@ ent pour travailler avec des enf@@ ant@@ s@@ .\n",
      "Et un jour il est venu à ma sé@@ ance - des séances épuis@@ antes et sans pi@@ tié — et il m@@ '@@ a dit \"@@ W@@ ah@@ ou@@ .\n",
      "Bien sû@@ r@@ , c@@ '@@ était juste un str@@ at@@ ag@@ ème de la part de Dr P ♪ pour me faire faire ces exerci@@ ces que je ne voulais pas faire dans la pers@@ pe@@ cti@@ ve de devenir la fille de 5 ans la plus riche de l@@ '@@ ét@@ ag@@ e@@ , mais ce qu@@ '@@ il a vraiment fait pour moi@@ , ça a été de transformer un aff@@ r@@ eux événement quotidien en une expérience nouvelle et promet@@ t@@ eu@@ se@@ .\n",
      "Et je me demande au@@ jour@@ d@@ '@@ hu@@ i@@ , dans quelle mesure sa vi@@ si@@ on@@ , et le fait qu@@ '@@ il m@@ '@@ ait déclaré une petite fille forte et puiss@@ an@@ te@@ , ont dess@@ iné ma propre vision de moi-même loin dans le fu@@ tur@@ , comme une personne par nature for@@ te@@ , puissante et ath@@ lé@@ ti@@ qu@@ e@@ .\n",
      "Et donc le médecin pré@@ n@@ at@@ al de ma mère était parti en vac@@ anc@@ es@@ , et l@@ '@@ homme qui m@@ '@@ a mise au monde était un étranger compl@@ et pour mes par@@ ent@@ s@@ .\n",
      "E@@ t@@ , comme je suis née sans tibi@@ a@@ , et que j@@ '@@ avais les pieds tournés vers l@@ '@@ int@@ éri@@ eur@@ , et quelques orteils sur ce pied-@@ ci@@ , et quelques orteils sur ce pied-@@ l@@ à@@ , il a dû être le port@@ eur@@ , cet étranger a dû être le porteur de la mauvaise nouv@@ el@@ le@@ .\n",
      "E@@ t@@ , comme je suis née sans tibi@@ a@@ , et que j@@ '@@ avais les pieds tournés vers l@@ '@@ int@@ éri@@ eur@@ , et quelques orteils sur ce pied-@@ ci@@ , et quelques orteils sur ce pied-@@ l@@ à@@ , il a dû être le port@@ eur@@ , cet étranger a dû être le porteur de la mauvaise nouv@@ el@@ le@@ .\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of tokenized English:\\n\"\n",
    "!head sw.data.en-fr.en\n",
    "\n",
    "!echo -e \"\\nFirst lines of tokenized French:\\n\"\n",
    "!head sw.data.en-fr.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "035e9211-725c-486f-ace6-0cc622f9fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-02 16:52:55 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='fr', task='translation', tensorboard_logdir=None, testpref='sw.data.en-fr', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='sw.data.en-fr', use_plasma_view=False, user_dir=None, validpref='sw.data.en-fr', wandb_project=None, workers=8)\n",
      "2022-12-02 16:52:55 | INFO | fairseq_cli.preprocess | [en] Dictionary: 3856 types\n",
      "2022-12-02 16:52:56 | INFO | fairseq_cli.preprocess | [en] sw.data.en-fr.en: 1108 sents, 44128 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-02 16:52:56 | INFO | fairseq_cli.preprocess | [en] Dictionary: 3856 types\n",
      "2022-12-02 16:52:56 | INFO | fairseq_cli.preprocess | [en] sw.data.en-fr.en: 1108 sents, 44128 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-02 16:52:56 | INFO | fairseq_cli.preprocess | [en] Dictionary: 3856 types\n",
      "2022-12-02 16:52:56 | INFO | fairseq_cli.preprocess | [en] sw.data.en-fr.en: 1108 sents, 44128 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-02 16:52:56 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 4328 types\n",
      "2022-12-02 16:52:57 | INFO | fairseq_cli.preprocess | [fr] sw.data.en-fr.fr: 1108 sents, 47004 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-02 16:52:57 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 4328 types\n",
      "2022-12-02 16:52:57 | INFO | fairseq_cli.preprocess | [fr] sw.data.en-fr.fr: 1108 sents, 47004 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-02 16:52:57 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 4328 types\n",
      "2022-12-02 16:52:58 | INFO | fairseq_cli.preprocess | [fr] sw.data.en-fr.fr: 1108 sents, 47004 tokens, 0.0% replaced (by <unk>)\n",
      "2022-12-02 16:52:58 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
     ]
    }
   ],
   "source": [
    "# Binarize the data for training\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en --target-lang fr \\\n",
    "    --trainpref sw.data.en-fr \\\n",
    "    --validpref sw.data.en-fr \\\n",
    "    --testpref sw.data.en-fr \\\n",
    "    --destdir data-bin \\\n",
    "    --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc698b33-d31f-4046-98ce-76c1cc617ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0264b10-bdb8-439e-bd09-856241d499b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-02 18:12:18 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'wmt14.en-fr.fconv-py/model.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-12-02 18:12:19 | INFO | fairseq.tasks.translation | [en] dictionary: 3856 types\n",
      "2022-12-02 18:12:19 | INFO | fairseq.tasks.translation | [fr] dictionary: 4328 types\n",
      "2022-12-02 18:12:19 | INFO | fairseq_cli.generate | loading model(s) from wmt14.en-fr.fconv-py/model.pt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/bin/fairseq-generate\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 413, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 50, in main\n",
      "    return _main(cfg, sys.stdout)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 96, in _main\n",
      "    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/checkpoint_utils.py\", line 367, in load_model_ensemble\n",
      "    ensemble, args, _task = load_model_ensemble_and_task(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/checkpoint_utils.py\", line 482, in load_model_ensemble_and_task\n",
      "    model.load_state_dict(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/models/fairseq_model.py\", line 128, in load_state_dict\n",
      "    return super().load_state_dict(new_state_dict, strict)\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1667, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for FConvModel:\n",
      "\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([43771, 768]) from checkpoint, the shape in current model is torch.Size([3856, 768]).\n",
      "\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([43807, 768]) from checkpoint, the shape in current model is torch.Size([4328, 768]).\n",
      "\tsize mismatch for decoder.fc3.bias: copying a param with shape torch.Size([43807]) from checkpoint, the shape in current model is torch.Size([4328]).\n",
      "\tsize mismatch for decoder.fc3.weight_g: copying a param with shape torch.Size([43807, 1]) from checkpoint, the shape in current model is torch.Size([4328, 1]).\n",
      "\tsize mismatch for decoder.fc3.weight_v: copying a param with shape torch.Size([43807, 512]) from checkpoint, the shape in current model is torch.Size([4328, 512]).\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "!fairseq-generate data-bin  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --path wmt14.en-fr.fconv-py/model.pt \\\n",
    "    --beam 5 \\\n",
    "    --batch-size 256 \\\n",
    "    --remove-bpe > en-fr.decode.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16baf3-949a-4346-8d5c-3c78c0a9de7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
