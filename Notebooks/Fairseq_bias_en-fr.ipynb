{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b2005-931f-4e35-ac54-ec1f1857c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66b5663e-9b69-4e32-98f0-9a626b80991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data\n"
     ]
    }
   ],
   "source": [
    "%cd /export/data4/vzhekova/biases-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9dca8c96-5bd9-4d3f-9a1f-f7a388c944d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of English:\n",
      "\n",
      "﻿Now, I thought, \"How could I really capture this?\n",
      "I mean, from this entry, it would seem that I was born into a world that perceived someone like me to have nothing positive whatsoever going for them, when in fact, today I'm celebrated for the opportunities and adventures my life has procured.\n",
      "So, I immediately went to look up the 2009 online edition, expecting to find a revision worth noting.\n",
      "His name was Dr. Pizzutillo, an Italian American, whose name, apparently, was too difficult for most Americans to pronounce, so he went by Dr. P. And Dr. P always wore really colorful bow ties and had the very perfect disposition to work with children.\n",
      "And, one day, he came in to my session — exhaustive and unforgiving, these sessions — and he said to me, \"Wow.\n",
      "Now, of course, this was a simple ploy on Dr. P's part to get me to do the exercises I didn't want to do before the prospect of being the richest five-year-old in the second floor ward, but what he effectively did for me was reshape an awful daily occurrence into a new and promising experience for me.\n",
      "And I have to wonder today to what extent his vision and his declaration of me as a strong and powerful little girl shaped my own view of myself as an inherently strong, powerful and athletic person well into the future.\n",
      "And so my mother's prenatal physician had gone on vacation, so the man who delivered me was a complete stranger to my parents.\n",
      "And, because I was born without the fibula bones, and had feet turned in, and a few toes in this foot and a few toes in that, he had to be the bearer — this stranger had to be the bearer of bad news.\n",
      "And, because I was born without the fibula bones, and had feet turned in, and a few toes in this foot and a few toes in that, he had to be the bearer — this stranger had to be the bearer of bad news.\n",
      "\n",
      "First lines of French:\n",
      "\n",
      "﻿Je me suis demandé comment je pourrais résumer visuellement cela.\n",
      "Je veux dire, d'après cette entrée, il semblerait que je sois née dans un monde qui percevrait les gens comme moi comme n'ayant absolument rien pour eux, alors qu'en fait, aujourd'hui, je suis célébrée pour les opportunités et les aventures que ma vie m'a apportée.\n",
      "Alors je suis tout de suite allée regarder dans l'édition 2009 en m'attendant à trouver une révision notable.\n",
      "Son nom est Dr Pizzutillo. Un Italo-Américain, dont le nom, apparemment, était trop difficile à prononcer pour la plupart des Américains, alors il est devenu Dr P. Et Dr. P portait toujours des noeuds papillon très colorés et avait le plus parfait tempérament pour travailler avec des enfants.\n",
      "Et un jour il est venu à ma séance - des séances épuisantes et sans pitié — et il m'a dit \"Wahou.\n",
      "Bien sûr, c'était juste un stratagème de la part de Dr P ♪ pour me faire faire ces exercices que je ne voulais pas faire dans la perspective de devenir la fille de 5 ans la plus riche de l'étage, mais ce qu'il a vraiment fait pour moi, ça a été de transformer un affreux événement quotidien en une expérience nouvelle et prometteuse.\n",
      "Et je me demande aujourd'hui, dans quelle mesure sa vision, et le fait qu'il m'ait déclaré une petite fille forte et puissante, ont dessiné ma propre vision de moi-même loin dans le futur, comme une personne par nature forte, puissante et athlétique.\n",
      "Et donc le médecin prénatal de ma mère était parti en vacances, et l'homme qui m'a mise au monde était un étranger complet pour mes parents.\n",
      "Et, comme je suis née sans tibia, et que j'avais les pieds tournés vers l'intérieur, et quelques orteils sur ce pied-ci, et quelques orteils sur ce pied-là, il a dû être le porteur, cet étranger a dû être le porteur de la mauvaise nouvelle.\n",
      "Et, comme je suis née sans tibia, et que j'avais les pieds tournés vers l'intérieur, et quelques orteils sur ce pied-ci, et quelques orteils sur ce pied-là, il a dû être le porteur, cet étranger a dû être le porteur de la mauvaise nouvelle.\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of English:\\n\"\n",
    "!head data.en-fr.en\n",
    "!echo -e \"\\nFirst lines of French:\\n\"\n",
    "!head data.en-fr.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "49a31954-1bac-4f40-92a9-bce45df99f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text; MuST-SHE dataset\n",
    "# TODO: write script for preprocessing\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "mpn = MosesPunctNormalizer()\n",
    "mt_en = MosesTokenizer(lang='en')\n",
    "md_en = MosesDetokenizer(lang='en')\n",
    "\n",
    "with open('data.en-fr.en') as fin, open('data.en-fr.tok.en','w') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt_en.tokenize(md_en.detokenize(mpn.normalize(line)), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout) \n",
    "        \n",
    "mt_fr = MosesTokenizer(lang='fr')\n",
    "md_fr = MosesDetokenizer(lang='fr')\n",
    "\n",
    "with open('data.en-fr.fr') as fin, open('data.en-fr.tok.fr','w') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt_fr.tokenize(md_fr.detokenize(mpn.normalize(line)), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished tokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3fda767b-fb58-4bb1-9ecd-64f79f73afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished detokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize text\n",
    "\n",
    "# mpn = MosesPunctNormalizer()\n",
    "# mt = MosesTokenizer(lang='en')\n",
    "# md = MosesDetokenizer(lang='en')\n",
    "\n",
    "# md.detokenize(mt.tokenize(md.detokenize(mpn.normalize('So, I immediately went to look up the 2009 online edition, expecting to find a revision worth noting.').split())))\n",
    "\n",
    "with open('data.en-fr.tok.en') as fin, open('data.en-fr.detok.en','w') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout) \n",
    "\n",
    "with open('data.en-fr.tok.fr') as fin, open('data.en-fr.detok.fr','w') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_fr.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished detokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "44a05e66-8fce-416a-b8e1-098bc2da3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13%|####8                               | 4298/32000 [00:06<00:19, 1443.12it/s]no pair has frequency >= 2. Stopping\n",
      " 14%|#####2                               | 4536/32000 [00:07<00:43, 637.80it/s]\n",
      " 16%|#####7                               | 5003/32000 [00:10<01:20, 336.94it/s]no pair has frequency >= 2. Stopping\n",
      " 16%|#####7                               | 5014/32000 [00:10<00:56, 475.25it/s]\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Dividing tokenized text into subword units\n",
    "\n",
    "!subword-nmt learn-bpe -s 32000 < data.en-fr.tok.en > sw.model.en\n",
    "!subword-nmt apply-bpe -c sw.model.en < data.en-fr.tok.en > sw.data.en\n",
    "\n",
    "!subword-nmt learn-bpe -s 32000 < data.en-fr.tok.fr > sw.model.fr\n",
    "!subword-nmt apply-bpe -c sw.model.fr < data.en-fr.tok.fr > sw.data.fr\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e36c9ea3-3a74-4eb2-9e19-960ccec689a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of tokenized English:\n",
      "\n",
      "﻿ Now , I thought , &quot; How could I really capture this ?\n",
      "I mean , from this ent@@ ry , it would seem that I was born into a world that percei@@ ved someone like me to have nothing positive whatsoever going for them , when in fact , today I &apos;m celebr@@ ated for the oppor@@ t@@ uni@@ ties and adventures my life has pro@@ cur@@ ed .\n",
      "So , I immediately went to look up the 200@@ 9 online e@@ dition , expec@@ ting to find a re@@ vision wor@@ th n@@ oting .\n",
      "His name was Dr. P@@ i@@ zz@@ u@@ til@@ lo , an Italian American , whose name , apparently , was too difficult for most Americ@@ ans to pr@@ on@@ ounce , so he went by Dr. P@@ . And Dr. P always wor@@ e really colorful bow ties and had the very perfect disp@@ os@@ i@@ tion to work with children .\n",
      "And , one day , he came in to my session - exha@@ us@@ tive and un@@ for@@ giving , these sessions - and he said to me , &quot; W@@ ow .\n",
      "Now , of course , this was a simple p@@ loy on Dr. P &apos;s part to get me to do the exer@@ cis@@ es I didn &apos;t want to do before the prospect of being the richest five-year-old in the second floor ward , but what he effec@@ tively did for me was re@@ shape an aw@@ ful daily occur@@ r@@ ence into a new and promising experience for me .\n",
      "And I have to won@@ der today to what ext@@ ent his vision and his declar@@ ation of me as a strong and powerful little girl shaped my own view of myself as an in@@ her@@ ently strong , powerful and athletic person well into the future .\n",
      "And so my mother &apos;s pr@@ en@@ at@@ al physician had gone on vac@@ ation , so the man who deliver@@ ed me was a complete stranger to my parents .\n",
      "And , because I was born without the fibula bones , and had feet turned in , and a few toes in this foot and a few toes in that , he had to be the bearer - this stranger had to be the bearer of bad news .\n",
      "And , because I was born without the fibula bones , and had feet turned in , and a few toes in this foot and a few toes in that , he had to be the bearer - this stranger had to be the bearer of bad news .\n",
      "\n",
      "First lines of tokenized French:\n",
      "\n",
      "﻿ Je me suis demandé comment je pourrais ré@@ su@@ mer vi@@ su@@ ellement cela .\n",
      "Je veux dire , d&apos; après cette entrée , il sembl@@ erait que je sois née dans un monde qui perce@@ vrait les gens comme moi comme n&apos; ayant absolument rien pour eux , alors qu&apos; en fait , aujourd&apos; hui , je suis céléb@@ rée pour les opportuni@@ tés et les aventu@@ res que ma vie m&apos; a ap@@ port@@ ée .\n",
      "Alors je suis tout de suite allée regarder dans l&apos; édi@@ tion 2009 en m&apos; att@@ endant à trouver une ré@@ vision not@@ able .\n",
      "Son nom est Dr P@@ i@@ zz@@ uti@@ ll@@ o . Un I@@ tal@@ o-@@ Américain , dont le nom , apparemment , était trop difficile à pron@@ oncer pour la plupart des Améric@@ ains , alors il est devenu Dr P@@ . Et Dr . P portait toujours des no@@ eu@@ ds papi@@ llon très co@@ lor@@ és et avait le plus parfait tempér@@ am@@ ent pour travailler avec des enfants .\n",
      "Et un jour il est venu à ma sé@@ ance - des séances épuis@@ antes et sans pi@@ tié - et il m&apos; a dit &quot; W@@ ah@@ ou .\n",
      "Bien sûr , c&apos; était juste un str@@ at@@ ag@@ ème de la part de Dr P ♪ pour me faire faire ces exerci@@ ces que je ne voulais pas faire dans la pers@@ pec@@ tive de devenir la fille de 5 ans la plus riche de l&apos; ét@@ age , mais ce qu&apos; il a vraiment fait pour moi , ça a été de transformer un aff@@ r@@ eux événement quotidien en une expérience nouvelle et prometteuse .\n",
      "Et je me demande aujourd&apos; hui , dans quelle mesure sa vision , et le fait qu&apos; il m&apos; ait déclaré une petite fille forte et puissante , ont dess@@ iné ma propre vision de moi-même loin dans le futur , comme une personne par nature forte , puissante et athlétique .\n",
      "Et donc le médecin pré@@ n@@ at@@ al de ma mère était parti en vac@@ ances , et l&apos; homme qui m&apos; a mise au monde était un étranger compl@@ et pour mes parents .\n",
      "Et , comme je suis née sans tibia , et que j&apos; avais les pieds tournés vers l&apos; intérieur , et quelques orteils sur ce pied-ci , et quelques orteils sur ce pied-là , il a dû être le porteur , cet étranger a dû être le porteur de la mauvaise nouvelle .\n",
      "Et , comme je suis née sans tibia , et que j&apos; avais les pieds tournés vers l&apos; intérieur , et quelques orteils sur ce pied-ci , et quelques orteils sur ce pied-là , il a dû être le porteur , cet étranger a dû être le porteur de la mauvaise nouvelle .\n"
     ]
    }
   ],
   "source": [
    "!echo -e \"\\nFirst lines of tokenized English:\\n\"\n",
    "!head sw.data.en\n",
    "\n",
    "!echo -e \"\\nFirst lines of tokenized French:\\n\"\n",
    "!head sw.data.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "035e9211-725c-486f-ace6-0cc622f9fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 15:44:45 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict='wmt14.en-fr.joined-dict.transformer/dict.en.txt', suppress_crashes=False, target_lang='fr', task='translation', tensorboard_logdir=None, testpref='sw.data.en-fr', tgtdict='wmt14.en-fr.joined-dict.transformer/dict.fr.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-01-30 15:44:45 | INFO | fairseq_cli.preprocess | [en] Dictionary: 44512 types\n",
      "2023-01-30 15:44:46 | INFO | fairseq_cli.preprocess | [en] sw.data.en-fr.en: 1108 sents, 38883 tokens, 4.37% replaced (by <unk>)\n",
      "2023-01-30 15:44:46 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 44512 types\n",
      "2023-01-30 15:44:47 | INFO | fairseq_cli.preprocess | [fr] sw.data.en-fr.fr: 1108 sents, 40581 tokens, 5.64% replaced (by <unk>)\n",
      "2023-01-30 15:44:47 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
     ]
    }
   ],
   "source": [
    "# Binarize the data for training; test with moses tokenizer and subword_nmt failed -> BLEU score of 4\n",
    "\n",
    "# reuse model dict; solution for dictionary size to match test dataset\n",
    "# map words appearing less than threshold times to unknown \n",
    "# --tokenizer moses \\\n",
    "# --bpe subword_nmt \\\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --testpref sw.data.en-fr \\\n",
    "    --srcdict wmt14.en-fr.joined-dict.transformer/dict.en.txt \\\n",
    "    --tgtdict wmt14.en-fr.joined-dict.transformer/dict.fr.txt \\\n",
    "    --destdir data-bin_en-fr \\\n",
    "    --thresholdtgt 0 \\\n",
    "    --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cc698b33-d31f-4046-98ce-76c1cc617ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data\n"
     ]
    }
   ],
   "source": [
    "%cd /export/data4/vzhekova/biases-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b0264b10-bdb8-439e-bd09-856241d499b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 15:56:35 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'wmt14.en-fr.joined-dict.transformer/model.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-01-30 15:56:37 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2023-01-30 15:56:37 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2023-01-30 15:56:37 | INFO | fairseq_cli.generate | loading model(s) from wmt14.en-fr.joined-dict.transformer/model.pt\n",
      "2023-01-30 15:57:02 | INFO | fairseq.data.data_utils | loaded 1,108 examples from: data-bin/test.en-fr.en\n",
      "2023-01-30 15:57:02 | INFO | fairseq.data.data_utils | loaded 1,108 examples from: data-bin/test.en-fr.fr\n",
      "2023-01-30 15:57:02 | INFO | fairseq.tasks.translation | data-bin test en-fr 1108 examples\n",
      "2023-01-30 15:58:01 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-01-30 15:58:01 | INFO | fairseq_cli.generate | Translated 1,108 sentences (42,301 tokens) in 32.1s (34.56 sentences/s, 1319.39 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "!fairseq-generate data-bin_en-fr  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang fr \\\n",
    "    --path wmt14.en-fr.joined-dict.transformer/model.pt \\\n",
    "    --beam 5 \\\n",
    "    --batch-size 256 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe=subword_nmt > en-fr.decode.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0f16baf3-949a-4346-8d5c-3c78c0a9de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je voulais les ouvrir en tant qu&apos; alliés de la communauté de la LGBTQ .\n",
      "Au début de mon séjour là @-@ bas , en 2000 , je m&apos; intéressais beaucoup aux collectivités .\n",
      "Après ça , j &quot; étais un peu impatient d&apos; aller à mon prochain rendez @-@ vous zéros .\n",
      "Je suis allé à la maison pour célébrer , pour la conquérir .\n",
      "Et je pensais que c &quot; était super cool , alors je le montrais à mon ami .\n",
      "Je m&apos; appelle Dan , je suis partenaire d&apos; une conquête créative mondiale .\n",
      "Je suis retourné à mon laboratoire de Boston et j&apos; ai mené une petite expérience .\n",
      "Et le juge l&apos; a reconnu comme un adulte , mais je vois cela .\n",
      "Nous avons donc ici une jeune femme et un homme engagés dans un jeu d&apos; aumônerie .\n",
      "J&apos; ai eu le grand privilège , lorsque j &quot; étais jeune avocat , de faire la connaissance de &quot; Parks &quot; .\n",
      "..........\n",
      "Je <<unk>> ouvrir ces portes en tant qu&apos; alliée de la communauté LGBTQ .\n",
      "À mes débuts en 2000 , j&apos; étais très intéressé par les communautés .\n",
      "Après lui , j&apos; étais un peu <<unk>> pour mon prochain <<unk>> zéro .\n",
      "Je suis allée chez <<unk>> pour célébrer cela , pour la fé<<unk>> ter .\n",
      "Je <<unk>> ça super et donc j&apos; étais <<unk>> de le montrer à mon ami .\n",
      "Je suis Dan , un associé dans un cabinet international de conseil en créativité .\n",
      "Je suis <<unk>> dans mon <<unk>> à Boston et fait une petite expérience .\n",
      "Et le juge l&apos; a certifié adulte , mais je vois ce <<unk>> .\n",
      "Voici ici , une jeune <<unk>> et un male qui s&apos; adonnent à une course poursuite .\n",
      "J&apos; ai eu le grand privilège , quand j&apos; étais un jeune avocat , de rencontrer <<unk>> Parks .\n"
     ]
    }
   ],
   "source": [
    "# Extract the hypotheses and references from the decoding log file\n",
    "!grep ^H en-fr.decode.log | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp.en-fr.txt\n",
    "!grep ^T en-fr.decode.log | sed 's/^T-//g' | cut -f 2 | sed 's/ @@//g' > ./ref.en-fr.txt\n",
    "\n",
    "!head ./hyp.en-fr.txt\n",
    "print(\"..........\")\n",
    "!head ./ref.en-fr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "def5dcc4-b30b-43a6-b19e-92f135357a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished detokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize text        \n",
    "\n",
    "with open('hyp.en-fr.txt', encoding='utf8') as fin, open('hyp_detok.en-fr.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('ref.en-fr.txt', encoding='utf8') as fin, open('ref_detok.en-fr.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_fr.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished detokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "aff664f1-625b-48a2-8b7d-26f8329e367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je voulais les ouvrir en tant qu' alliés de la communauté de la LGBTQ.\n",
      "Au début de mon séjour là-bas, en 2000, je m' intéressais beaucoup aux collectivités.\n",
      "Après ça, j \"étais un peu impatient d' aller à mon prochain rendez-vous zéros.\n",
      "Je suis allé à la maison pour célébrer, pour la conquérir.\n",
      "Et je pensais que c \"était super cool, alors je le montrais à mon ami.\n",
      "Je m' appelle Dan, je suis partenaire d' une conquête créative mondiale.\n",
      "Je suis retourné à mon laboratoire de Boston et j' ai mené une petite expérience.\n",
      "Et le juge l' a reconnu comme un adulte, mais je vois cela.\n",
      "Nous avons donc ici une jeune femme et un homme engagés dans un jeu d' aumônerie.\n",
      "J' ai eu le grand privilège, lorsque j \"étais jeune avocat, de faire la connaissance de\" Parks \".\n",
      "..........\n",
      "Je <<unk>> ouvrir ces portes en tant qu'alliée de la communauté LGBTQ.\n",
      "À mes débuts en 2000, j'étais très intéressé par les communautés.\n",
      "Après lui, j'étais un peu <<unk>> pour mon prochain <<unk>> zéro.\n",
      "Je suis allée chez <<unk>> pour célébrer cela, pour la fé<<unk>> ter.\n",
      "Je <<unk>> ça super et donc j'étais <<unk>> de le montrer à mon ami.\n",
      "Je suis Dan, un associé dans un cabinet international de conseil en créativité.\n",
      "Je suis <<unk>> dans mon <<unk>> à Boston et fait une petite expérience.\n",
      "Et le juge l'a certifié adulte, mais je vois ce <<unk>>.\n",
      "Voici ici, une jeune <<unk>> et un male qui s'adonnent à une course poursuite.\n",
      "J'ai eu le grand privilège, quand j'étais un jeune avocat, de rencontrer <<unk>> Parks.\n"
     ]
    }
   ],
   "source": [
    "!head ./hyp_detok.en-fr.txt\n",
    "print(\"..........\")\n",
    "!head ./ref_detok.en-fr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "04b7966d-1948-4c16-bc1d-924c1f34181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 18.7,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
      " \"verbose_score\": \"50.3/27.7/17.0/10.5 (BP = 0.839 ratio = 0.851 hyp_len = 36079 ref_len = 42405)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.3.1\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# BLEU score of 28.9 (beam=5), BLEU score of 28.2 (beam=1) before detokenization\n",
    "# BLEU score of 18.7 (beam=5), BLEU score of 18.2 (beam=1) after detokenization\n",
    "!cat ./hyp_detok.en-fr.txt | sacrebleu ./ref_detok.en-fr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "88a021a9-42b5-4938-bea3-6c8250e46261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ignore_case=False, order=4, ref='./ref_detok.en-fr.txt', sacrebleu=True, sentence_bleu=False, sys='./hyp_detok.en-fr.txt')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/bin/fairseq-score\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/score.py\", line 98, in cli_main\n",
      "    score(f)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/score.py\", line 61, in score\n",
      "    print(sacrebleu.corpus_bleu(fdsys, [fdref]).format())\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/sacrebleu/compat.py\", line 37, in corpus_bleu\n",
      "    return metric.corpus_score(hypotheses, references)\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/sacrebleu/metrics/base.py\", line 414, in corpus_score\n",
      "    self._check_corpus_score_args(hypotheses, references)\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/sacrebleu/metrics/base.py\", line 258, in _check_corpus_score_args\n",
      "    raise TypeError(f'{prefix}: {err_msg}')\n",
      "TypeError: BLEU: Each element of `refs` should be a sequence of strings.\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score \\\n",
    "    --sacrebleu \\\n",
    "    --sys ./hyp_detok.en-fr.txt \\\n",
    "    --ref ./ref_detok.en-fr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1c7c3861-f7a2-4d13-bbd0-7fe6e0c5237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\tTerm Coverage\tGender Accuracy\n",
      "-------------------------------------------------\n",
      "1F\t0.03773584905660377\t0.16666666666666666\n",
      "1M\t0.07804878048780488\t0.5777777777777777\n",
      "2F\t0.12447257383966245\t0.3787878787878788\n",
      "2M\t0.1203883495145631\t0.5405405405405406\n",
      "3F\t0.0\t0.0\n",
      "3M\t0.0\t0.0\n",
      "4F\t0.07407407407407407\t0.0\n",
      "4M\t0.16666666666666666\t0.5\n",
      "-------------------------------------------------\n",
      "Global\t0.09391395592864638\t0.4604651162790698\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate gender accuracy; 0.46\n",
    "!python ./mustshe_acc_v1.1.py \\\n",
    "    --input hyp.en-fr.txt \\\n",
    "    --tsv-definition /export/data4/vzhekova/MuST-SHE_v1.2/MuST-SHE-v1.2-data/tsv/MONOLINGUAL.fr_v1.2.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ac27c-11de-4606-96c7-82b36050b7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
