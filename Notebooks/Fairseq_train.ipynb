{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ace930-4937-4b09-830e-499bb4e5bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if we can connect to the GPU with PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30f384c-c0de-4417-b048-dfe88ef8a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vzhekova/fairseq/examples/translation\n"
     ]
    }
   ],
   "source": [
    "%cd /home/vzhekova/fairseq/examples/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "562e42d5-a18a-4e2c-aa49-018587a9efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48856\n",
      "-rw-r--r-- 1 vzhekova input   199632 Okt 24 16:30 dev.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   223089 Okt 24 16:30 dev.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   418647 Okt 24 16:30 tst.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   473219 Okt 24 16:30 tst.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 18496311 Okt 24 16:30 train.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 16663248 Okt 24 16:30 train.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input 13535860 Okt 24 16:37 data.zip\n",
      "\n",
      "First lines of English:\n",
      "\n",
      "It can be a very complicated thing, the ocean.\n",
      "And it can be a very complicated thing, what human health is.\n",
      "And bringing those two together might seem a very daunting task, but what I'm going to try to say is that even in that complexity, there's some simple themes that I think, if we understand, we can really move forward.\n",
      "And those simple themes aren't really themes about the complex science of what's going on, but things that we all pretty well know.\n",
      "And I'm going to start with this one: If momma ain't happy, ain't nobody happy.\n",
      "We know that, right? We've experienced that.\n",
      "And if we just take that and we build from there, then we can go to the next step, which is that if the ocean ain't happy, ain't nobody happy.\n",
      "That's the theme of my talk.\n",
      "And we're making the ocean pretty unhappy in a lot of different ways.\n",
      "This is a shot of Cannery Row in 1932.\n",
      "\n",
      "First lines of German:\n",
      "\n",
      "Das Meer kann ziemlich kompliziert sein.\n",
      "Und was menschliche Gesundheit ist, kann auch ziemlich kompliziert sein.\n",
      "Und diese zwei zusammen zu bringen, erscheint vielleicht wie eine gewaltige Aufgabe. Aber was ich Ihnen zu sagen versuche ist, dass es trotz dieser Komplexität einige einfache Themen gibt, von denen ich denke, wenn wir diese verstehen, können wir uns wirklich weiter entwickeln.\n",
      "Und diese einfachen Themen sind eigentlich keine komplexen wissenschaftlichen Zusammenhänge, sondern Tatsachen,die wir alle gut kennen.\n",
      "Und ich werde mit dieser hier anfangen: Wenn die Mama nicht glücklich ist, ist keiner glücklich.\n",
      "Kennen wir das nicht alle? Das haben wir alle schon erlebt.\n",
      "Wenn wir das nehmen und darauf aufbauen, dann können wir einen Schritt weiter gehen: Wenn das Meer nicht glücklich ist, ist keiner glücklich.\n",
      "Darum geht es in meinem Vortrag.\n",
      "Wir machen das Meer auf viele verschiedene Arten ziemlich unglücklich.\n",
      "Das ist ein Bild der Cannery Row von 1932.\n"
     ]
    }
   ],
   "source": [
    "# List files in downloaded `sample_data`\n",
    "!ls -ltr sample_data\n",
    "\n",
    "!echo -e \"\\nFirst lines of English:\\n\"\n",
    "!head sample_data/train.de-en.en\n",
    "!echo -e \"\\nFirst lines of German:\\n\"\n",
    "!head sample_data/train.de-en.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8866d30e-685d-4b71-9022-b7d95844f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training sentencepiece model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sample_data/train.de-en.en\n",
      "  input: sample_data/train.de-en.de\n",
      "  input_format: \n",
      "  model_prefix: bpe\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: sample_data/train.de-en.en\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: sample_data/train.de-en.de\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 348862 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=34918444\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9504% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 348862 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 507320 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 348862\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 292588\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 292588 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=139634 obj=11.7092 num_tokens=603926 num_tokens/piece=4.32506\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=118725 obj=9.20969 num_tokens=606833 num_tokens/piece=5.11125\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=89012 obj=9.19675 num_tokens=643408 num_tokens/piece=7.22833\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=88926 obj=9.18331 num_tokens=643549 num_tokens/piece=7.2369\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=66687 obj=9.26725 num_tokens=697181 num_tokens/piece=10.4545\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=66686 obj=9.25067 num_tokens=697168 num_tokens/piece=10.4545\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=50014 obj=9.36363 num_tokens=756842 num_tokens/piece=15.1326\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=50013 obj=9.34353 num_tokens=756760 num_tokens/piece=15.1313\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=37508 obj=9.48497 num_tokens=819410 num_tokens/piece=21.8463\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=37508 obj=9.46172 num_tokens=819314 num_tokens/piece=21.8437\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=28131 obj=9.63262 num_tokens=883095 num_tokens/piece=31.3922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=28131 obj=9.60543 num_tokens=883000 num_tokens/piece=31.3889\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21098 obj=9.80532 num_tokens=946429 num_tokens/piece=44.8587\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21098 obj=9.77326 num_tokens=946409 num_tokens/piece=44.8578\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=15823 obj=10.0061 num_tokens=1011334 num_tokens/piece=63.9154\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=15823 obj=9.96889 num_tokens=1011373 num_tokens/piece=63.9179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11867 obj=10.2377 num_tokens=1074641 num_tokens/piece=90.5571\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11867 obj=10.1939 num_tokens=1074640 num_tokens/piece=90.557\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11000 obj=10.2648 num_tokens=1090898 num_tokens/piece=99.1725\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11000 obj=10.2534 num_tokens=1090946 num_tokens/piece=99.1769\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: bpe.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# segment the subwords\n",
    "spm.SentencePieceTrainer.train(input=\"sample_data/train.de-en.en,sample_data/train.de-en.de\", \n",
    "                               model_prefix=\"bpe\", \n",
    "                               vocab_size=10000)\n",
    "\n",
    "print('Finished training sentencepiece model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a86cbf28-2cae-417e-8026-22c7e1e16fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained sentencepiece model\n",
    "spm_model = spm.SentencePieceProcessor(model_file=\"bpe.model\")\n",
    "\n",
    "# preprocess the sentences from train/dev/test sets\n",
    "for partition in [\"train\", \"dev\", \"tst\"]:\n",
    "    for lang in [\"de\", \"en\"]:\n",
    "        f_out = open(f\"sample_data/spm.{partition}.de-en.{lang}\", \"w\")\n",
    "\n",
    "        with open(f\"sample_data/{partition}.de-en.{lang}\", \"r\") as f_in:\n",
    "            for line_idx, line in enumerate(f_in.readlines()):\n",
    "                # Segmented into subwords\n",
    "                line_segmented = spm_model.encode(line.strip(), out_type=str)\n",
    "                # Join the subwords into a string\n",
    "                line_segmented = \" \".join(line_segmented)\n",
    "                f_out.write(line_segmented + \"\\n\")\n",
    "\n",
    "        f_out.close()\n",
    "        \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cc8b072-6615-49c5-a62e-126f8e429568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 91812\n",
      "-rw-r--r-- 1 vzhekova input   199632 Okt 24 16:30 dev.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   223089 Okt 24 16:30 dev.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   418647 Okt 24 16:30 tst.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   473219 Okt 24 16:30 tst.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 18496311 Okt 24 16:30 train.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 16663248 Okt 24 16:30 train.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input 28468890 Nov 12 15:55 spm.train.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 26971608 Nov 12  2022 spm.train.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   343454 Nov 12  2022 spm.dev.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   321479 Nov 12  2022 spm.dev.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   728203 Nov 12  2022 spm.tst.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   678280 Nov 12  2022 spm.tst.de-en.en\n",
      "\n",
      "First lines of tokenized English:\n",
      "\n",
      "▁It ▁can ▁be ▁a ▁very ▁complicated ▁thing , ▁the ▁ocean .\n",
      "▁And ▁it ▁can ▁be ▁a ▁very ▁complicated ▁thing , ▁what ▁human ▁health ▁is .\n",
      "▁And ▁bring ing ▁those ▁two ▁together ▁might ▁seem ▁a ▁very ▁da un ting ▁task , ▁but ▁what ▁I ' m ▁going ▁to ▁try ▁to ▁say ▁is ▁that ▁even ▁in ▁that ▁complexity , ▁there ' s ▁some ▁simple ▁them es ▁that ▁I ▁think , ▁if ▁we ▁understand , ▁we ▁can ▁really ▁move ▁forward .\n",
      "▁And ▁those ▁simple ▁them es ▁aren ' t ▁really ▁them es ▁about ▁the ▁complex ▁science ▁of ▁what ' s ▁going ▁on , ▁but ▁things ▁that ▁we ▁all ▁pretty ▁well ▁know .\n",
      "▁And ▁I ' m ▁going ▁to ▁start ▁with ▁this ▁one : ▁If ▁mom ma ▁a in ' t ▁happy , ▁a in ' t ▁nobody ▁happy .\n",
      "▁We ▁know ▁that , ▁right ? ▁We ' ve ▁experience d ▁that .\n",
      "▁And ▁if ▁we ▁just ▁take ▁that ▁and ▁we ▁build ▁from ▁there , ▁then ▁we ▁can ▁go ▁to ▁the ▁next ▁step , ▁which ▁is ▁that ▁if ▁the ▁ocean ▁a in ' t ▁happy , ▁a in ' t ▁nobody ▁happy .\n",
      "▁That ' s ▁the ▁them e ▁of ▁my ▁talk .\n",
      "▁And ▁we ' re ▁making ▁the ▁ocean ▁pretty ▁unhappy ▁in ▁a ▁lot ▁of ▁different ▁ways .\n",
      "▁This ▁is ▁a ▁shot ▁of ▁Can ner y ▁Ro w ▁in ▁19 3 2 .\n",
      "\n",
      "First lines of tokenized German:\n",
      "\n",
      "▁Das ▁Meer ▁kann ▁ziemlich ▁kompliziert ▁sein .\n",
      "▁Und ▁was ▁menschliche ▁Gesundheit ▁ist , ▁kann ▁auch ▁ziemlich ▁kompliziert ▁sein .\n",
      "▁Und ▁diese ▁zwei ▁zusammen ▁zu ▁bringen , ▁erscheint ▁vielleicht ▁wie ▁eine ▁gewaltige ▁Aufgabe . ▁Aber ▁was ▁ich ▁Ihnen ▁zu ▁sagen ▁versuche ▁ist , ▁dass ▁es ▁trotz ▁dieser ▁Komplexität ▁einige ▁einfache ▁Themen ▁gibt , ▁von ▁denen ▁ich ▁denke , ▁wenn ▁wir ▁diese ▁verstehen , ▁können ▁wir ▁uns ▁wirklich ▁weiter ▁entwickeln .\n",
      "▁Und ▁diese ▁einfachen ▁Themen ▁sind ▁eigentlich ▁keine ▁komplexe n ▁wissen schaftlichen ▁Zusammen hänge , ▁sondern ▁Tatsache n , die ▁wir ▁alle ▁gut ▁kennen .\n",
      "▁Und ▁ich ▁werde ▁mit ▁dieser ▁hier ▁anfangen : ▁Wenn ▁die ▁Mama ▁nicht ▁glücklich ▁ist , ▁ist ▁keiner ▁glücklich .\n",
      "▁Kenn en ▁wir ▁das ▁nicht ▁alle ? ▁Das ▁haben ▁wir ▁alle ▁schon ▁erlebt .\n",
      "▁Wenn ▁wir ▁das ▁nehmen ▁und ▁darauf ▁aufbauen , ▁dann ▁können ▁wir ▁einen ▁Schritt ▁weiter ▁gehen : ▁Wenn ▁das ▁Meer ▁nicht ▁glücklich ▁ist , ▁ist ▁keiner ▁glücklich .\n",
      "▁Da rum ▁geht ▁es ▁in ▁meinem ▁Vortrag .\n",
      "▁Wir ▁machen ▁das ▁Meer ▁auf ▁viele ▁verschiedene ▁Arten ▁ziemlich ▁unglücklich .\n",
      "▁Das ▁ist ▁ein ▁Bild ▁der ▁Can ner y ▁Ro w ▁von ▁19 3 2 .\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr sample_data\n",
    "\n",
    "!echo -e \"\\nFirst lines of tokenized English:\\n\"\n",
    "!head sample_data/spm.train.de-en.en\n",
    "!echo -e \"\\nFirst lines of tokenized German:\\n\"\n",
    "!head sample_data/spm.train.de-en.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f01f44e-bb99-40d8-a23d-64e61c235e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vzhekova/fairseq/examples/translation/sample_data\n",
      "2022-11-12 16:46:42 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iwslt14.de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='/home/vzhekova/fairseq/examples/translation/sample_data/spm.tst.de-en', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/home/vzhekova/fairseq/examples/translation/sample_data/spm.train.de-en', use_plasma_view=False, user_dir=None, validpref='/home/vzhekova/fairseq/examples/translation/sample_data/spm.dev.de-en', wandb_project=None, workers=8)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/bin/fairseq-preprocess\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/preprocess.py\", line 299, in main\n",
      "    raise FileExistsError(_dict_path(args.source_lang, args.destdir))\n",
      "FileExistsError: data-bin/iwslt14.de-en/dict.en.txt\n"
     ]
    }
   ],
   "source": [
    "# Preprocess/binarize the data\n",
    "TEXT=\"/home/vzhekova/fairseq/examples/translation/sample_data\"\n",
    "!echo $TEXT\n",
    "# Binarize the data for training\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en --target-lang de \\\n",
    "    --trainpref $TEXT/spm.train.de-en \\\n",
    "    --validpref $TEXT/spm.dev.de-en \\\n",
    "    --testpref $TEXT/spm.tst.de-en \\\n",
    "    --destdir data-bin/iwslt14.de-en \\\n",
    "    --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0661db82-813a-4928-9d2e-64a453f79d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-12 19:13:02 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-12 19:13:02 | INFO | fairseq.tasks.translation | [en] dictionary: 6120 types\n",
      "2022-11-12 19:13:02 | INFO | fairseq.tasks.translation | [de] dictionary: 7624 types\n",
      "2022-11-12 19:13:03 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(6120, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(7624, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=7624, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-11-12 19:13:03 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-11-12 19:13:03 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-11-12 19:13:03 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-11-12 19:13:03 | INFO | fairseq_cli.train | num. shared model params: 51,175,424 (num. trained: 51,175,424)\n",
      "2022-11-12 19:13:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-12 19:13:03 | INFO | fairseq.data.data_utils | loaded 2,052 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/valid.en-de.en\n",
      "2022-11-12 19:13:03 | INFO | fairseq.data.data_utils | loaded 2,052 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/valid.en-de.de\n",
      "2022-11-12 19:13:03 | INFO | fairseq.tasks.translation | /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en valid en-de 2052 examples\n",
      "2022-11-12 19:13:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-11-12 19:13:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-12 19:13:07 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     \n",
      "2022-11-12 19:13:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-12 19:13:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-12 19:13:07 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-11-12 19:13:07 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "2022-11-12 19:13:08 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/checkpoint_last.pt (epoch 8 @ 0 updates)\n",
      "2022-11-12 19:13:08 | INFO | fairseq.trainer | loading train data for epoch 8\n",
      "2022-11-12 19:13:08 | INFO | fairseq.data.data_utils | loaded 174,443 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/train.en-de.en\n",
      "2022-11-12 19:13:08 | INFO | fairseq.data.data_utils | loaded 174,443 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/train.en-de.de\n",
      "2022-11-12 19:13:08 | INFO | fairseq.tasks.translation | /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en train en-de 174443 examples\n",
      "2022-11-12 19:13:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 008:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-12 19:13:09 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-11-12 19:13:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/vzhekova/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 008: 100%|▉| 1266/1267 [06:00<00:00,  3.58it/s, loss=5.573, nll_loss=4.3542022-11-12 19:19:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:05,  4.76it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02, 10.33it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 12.66it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.81it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.38it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:00<00:01, 14.37it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.53it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.43it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.67it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.99it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.96it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.93it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-12 19:19:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.914 | nll_loss 4.675 | ppl 25.55 | wps 29882.2 | wpb 1919.9 | bsz 73.3 | num_updates 1267\n",
      "2022-11-12 19:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1267 updates\n",
      "2022-11-12 19:19:11 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint8.pt\n",
      "2022-11-12 19:19:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint8.pt\n",
      "2022-11-12 19:19:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 1267 updates, score 5.914) (writing took 28.948590012005297 seconds)\n",
      "2022-11-12 19:19:41 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-11-12 19:19:41 | INFO | train | epoch 008 | loss 5.479 | nll_loss 4.249 | ppl 19.01 | wps 11497.5 | ups 3.24 | wpb 3549.6 | bsz 137.7 | num_updates 1267 | lr 0.000158375 | gnorm 1.042 | train_wall 353 | gb_free 8.8 | wall 394\n",
      "2022-11-12 19:19:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 009:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-12 19:19:41 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-11-12 19:19:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 1266/1267 [05:54<00:00,  3.54it/s, loss=5.47, nll_loss=4.231,2022-11-12 19:25:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:05,  5.14it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02, 11.14it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 13.35it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 14.15it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 15.34it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.95it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:00<00:01, 14.93it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.86it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.73it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.93it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 15.18it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 15.10it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 15.09it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.23it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-12 19:25:38 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.814 | nll_loss 4.544 | ppl 23.33 | wps 30493.7 | wpb 1919.9 | bsz 73.3 | num_updates 2534 | best_loss 5.814\n",
      "2022-11-12 19:25:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 2534 updates\n",
      "2022-11-12 19:25:38 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint9.pt\n",
      "2022-11-12 19:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint9.pt\n",
      "2022-11-12 19:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 2534 updates, score 5.814) (writing took 28.078852343023755 seconds)\n",
      "2022-11-12 19:26:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-11-12 19:26:08 | INFO | train | epoch 009 | loss 5.411 | nll_loss 4.165 | ppl 17.93 | wps 11645.3 | ups 3.28 | wpb 3549.6 | bsz 137.7 | num_updates 2534 | lr 0.00031675 | gnorm 1.233 | train_wall 348 | gb_free 8.7 | wall 780\n",
      "2022-11-12 19:26:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 010:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-12 19:26:08 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-11-12 19:26:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 1266/1267 [05:54<00:00,  3.56it/s, loss=5.345, nll_loss=4.0882022-11-12 19:32:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:05,  4.99it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02, 10.82it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 13.02it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 15.18it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.92it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:00<00:01, 14.82it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.87it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.73it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.93it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 15.19it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 15.08it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 15.04it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-12 19:32:04 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.742 | nll_loss 4.447 | ppl 21.81 | wps 30382.1 | wpb 1919.9 | bsz 73.3 | num_updates 3801 | best_loss 5.742\n",
      "2022-11-12 19:32:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3801 updates\n",
      "2022-11-12 19:32:04 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint10.pt\n",
      "2022-11-12 19:32:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint10.pt\n",
      "2022-11-12 19:32:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 3801 updates, score 5.742) (writing took 28.295546543988166 seconds)\n",
      "2022-11-12 19:32:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-11-12 19:32:33 | INFO | train | epoch 010 | loss 5.366 | nll_loss 4.109 | ppl 17.26 | wps 11664.4 | ups 3.29 | wpb 3549.6 | bsz 137.7 | num_updates 3801 | lr 0.000475125 | gnorm 1.276 | train_wall 348 | gb_free 8.4 | wall 1166\n",
      "2022-11-12 19:32:33 | INFO | fairseq_cli.train | done training in 1164.6 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "    /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --max-epoch 10 \\\n",
    "    --reset-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd62baa2-8d09-47f6-9903-12958649d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:58:01 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-13 12:58:02 | INFO | fairseq.tasks.translation | [en] dictionary: 6120 types\n",
      "2022-11-13 12:58:02 | INFO | fairseq.tasks.translation | [de] dictionary: 7624 types\n",
      "2022-11-13 12:58:02 | INFO | fairseq_cli.generate | loading model(s) from /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint_best.pt\n",
      "2022-11-13 12:58:06 | INFO | fairseq.data.data_utils | loaded 4,698 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/test.en-de.en\n",
      "2022-11-13 12:58:06 | INFO | fairseq.data.data_utils | loaded 4,698 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/test.en-de.de\n",
      "2022-11-13 12:58:06 | INFO | fairseq.tasks.translation | /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en test en-de 4698 examples\n",
      "2022-11-13 13:00:06 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-11-13 13:00:06 | INFO | fairseq_cli.generate | Translated 4,698 sentences (99,443 tokens) in 62.0s (75.81 sentences/s, 1604.71 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "TEST_INPUT=\"/home/vzhekova/fairseq/examples/translation/sample_data/spm.tst.de-en.de\"\n",
    "PRED_LOG=\"/home/vzhekova/fairseq/examples/translation/en-de.decode.log\"\n",
    "\n",
    "!fairseq-generate /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en \\\n",
    "      --task translation \\\n",
    "      --source-lang en \\\n",
    "      --target-lang de \\\n",
    "      --path /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint_best.pt \\\n",
    "      --batch-size 256 \\\n",
    "      --beam 4 \\\n",
    "      --remove-bpe=sentencepiece > $PRED_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "372cc63a-e30a-4fbd-a20d-c2ac3ff2f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hypotheses and references from the decoding log file\n",
    "!grep ^H $PRED_LOG | sed 's/^H-//g' | cut -f 3 | sed 's/ ##//g' > ./hyp.txt\n",
    "!grep ^T $PRED_LOG | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > ./ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcb7daab-4196-48d4-916a-968e9a8c05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Und er sagt.\n",
      "Sie sind wieder wieder wieder.\n",
      "Wir nehmen es aus.\n",
      "Und ich dachte darüber nach.\n",
      "Was wollen diese Menschen sein?\n",
      "Ich wollte sie unterstützen.\n",
      "\"Ja\", sagte er.\n",
      "Wer macht das Beste?\n",
      "Warum stimmt das?\n",
      "Es macht sie gut.\"\n",
      "..........\n",
      "Und er sagt...\n",
      "Bewerte sie wieder.\n",
      "Wir heben ab.\n",
      "Ich dachte darüber nach.\n",
      "Was brauchen diese Menschen?\n",
      "Ich wollte sie unterstützten.\n",
      "\"Ja\", sagte er.\n",
      "Wer ist am erfolgreichsten?\n",
      "Und warum ist das so?\n",
      "So fühlen sie sich gut.\"\n"
     ]
    }
   ],
   "source": [
    "!head ./hyp.txt\n",
    "print(\"..........\")\n",
    "!head ./ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "127b23c5-d00c-45ff-91a0-4bb7d2f7c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 15.0,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
      " \"verbose_score\": \"50.1/21.5/10.8/5.7 (BP = 0.933 ratio = 0.935 hyp_len = 80332 ref_len = 85901)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.3.1\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "!cat ./hyp.txt | sacrebleu ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f27ee0-249c-471b-a665-c9637b17d156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
