{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ace930-4937-4b09-830e-499bb4e5bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if we can connect to the GPU with PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30f384c-c0de-4417-b048-dfe88ef8a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vzhekova/fairseq/examples/translation\n"
     ]
    }
   ],
   "source": [
    "%cd /home/vzhekova/fairseq/examples/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "562e42d5-a18a-4e2c-aa49-018587a9efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48856\n",
      "-rw-r--r-- 1 vzhekova input   199632 Okt 24 16:30 dev.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   223089 Okt 24 16:30 dev.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   418647 Okt 24 16:30 tst.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   473219 Okt 24 16:30 tst.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 18496311 Okt 24 16:30 train.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 16663248 Okt 24 16:30 train.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input 13535860 Okt 24 16:37 data.zip\n",
      "\n",
      "First lines of English:\n",
      "\n",
      "It can be a very complicated thing, the ocean.\n",
      "And it can be a very complicated thing, what human health is.\n",
      "And bringing those two together might seem a very daunting task, but what I'm going to try to say is that even in that complexity, there's some simple themes that I think, if we understand, we can really move forward.\n",
      "And those simple themes aren't really themes about the complex science of what's going on, but things that we all pretty well know.\n",
      "And I'm going to start with this one: If momma ain't happy, ain't nobody happy.\n",
      "We know that, right? We've experienced that.\n",
      "And if we just take that and we build from there, then we can go to the next step, which is that if the ocean ain't happy, ain't nobody happy.\n",
      "That's the theme of my talk.\n",
      "And we're making the ocean pretty unhappy in a lot of different ways.\n",
      "This is a shot of Cannery Row in 1932.\n",
      "\n",
      "First lines of German:\n",
      "\n",
      "Das Meer kann ziemlich kompliziert sein.\n",
      "Und was menschliche Gesundheit ist, kann auch ziemlich kompliziert sein.\n",
      "Und diese zwei zusammen zu bringen, erscheint vielleicht wie eine gewaltige Aufgabe. Aber was ich Ihnen zu sagen versuche ist, dass es trotz dieser Komplexität einige einfache Themen gibt, von denen ich denke, wenn wir diese verstehen, können wir uns wirklich weiter entwickeln.\n",
      "Und diese einfachen Themen sind eigentlich keine komplexen wissenschaftlichen Zusammenhänge, sondern Tatsachen,die wir alle gut kennen.\n",
      "Und ich werde mit dieser hier anfangen: Wenn die Mama nicht glücklich ist, ist keiner glücklich.\n",
      "Kennen wir das nicht alle? Das haben wir alle schon erlebt.\n",
      "Wenn wir das nehmen und darauf aufbauen, dann können wir einen Schritt weiter gehen: Wenn das Meer nicht glücklich ist, ist keiner glücklich.\n",
      "Darum geht es in meinem Vortrag.\n",
      "Wir machen das Meer auf viele verschiedene Arten ziemlich unglücklich.\n",
      "Das ist ein Bild der Cannery Row von 1932.\n"
     ]
    }
   ],
   "source": [
    "# List files in downloaded `sample_data`\n",
    "!ls -ltr sample_data\n",
    "\n",
    "!echo -e \"\\nFirst lines of English:\\n\"\n",
    "!head sample_data/train.de-en.en\n",
    "!echo -e \"\\nFirst lines of German:\\n\"\n",
    "!head sample_data/train.de-en.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8866d30e-685d-4b71-9022-b7d95844f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training sentencepiece model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sample_data/train.de-en.en\n",
      "  input: sample_data/train.de-en.de\n",
      "  input_format: \n",
      "  model_prefix: bpe\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: sample_data/train.de-en.en\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: sample_data/train.de-en.de\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 348862 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=34918444\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9504% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 348862 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 507320 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 348862\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 292588\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 292588 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=139634 obj=11.7092 num_tokens=603926 num_tokens/piece=4.32506\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=118725 obj=9.20969 num_tokens=606833 num_tokens/piece=5.11125\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=89012 obj=9.19675 num_tokens=643408 num_tokens/piece=7.22833\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=88926 obj=9.18331 num_tokens=643549 num_tokens/piece=7.2369\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=66687 obj=9.26725 num_tokens=697181 num_tokens/piece=10.4545\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=66686 obj=9.25067 num_tokens=697168 num_tokens/piece=10.4545\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=50014 obj=9.36363 num_tokens=756842 num_tokens/piece=15.1326\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=50013 obj=9.34353 num_tokens=756760 num_tokens/piece=15.1313\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=37508 obj=9.48497 num_tokens=819410 num_tokens/piece=21.8463\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=37508 obj=9.46172 num_tokens=819314 num_tokens/piece=21.8437\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=28131 obj=9.63262 num_tokens=883095 num_tokens/piece=31.3922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=28131 obj=9.60543 num_tokens=883000 num_tokens/piece=31.3889\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21098 obj=9.80532 num_tokens=946429 num_tokens/piece=44.8587\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21098 obj=9.77326 num_tokens=946409 num_tokens/piece=44.8578\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=15823 obj=10.0061 num_tokens=1011334 num_tokens/piece=63.9154\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=15823 obj=9.96889 num_tokens=1011373 num_tokens/piece=63.9179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11867 obj=10.2377 num_tokens=1074641 num_tokens/piece=90.5571\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11867 obj=10.1939 num_tokens=1074640 num_tokens/piece=90.557\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11000 obj=10.2648 num_tokens=1090898 num_tokens/piece=99.1725\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11000 obj=10.2534 num_tokens=1090946 num_tokens/piece=99.1769\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: bpe.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# segment the subwords\n",
    "spm.SentencePieceTrainer.train(input=\"sample_data/train.de-en.en,sample_data/train.de-en.de\", \n",
    "                               model_prefix=\"bpe\", \n",
    "                               vocab_size=10000)\n",
    "\n",
    "print('Finished training sentencepiece model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a86cbf28-2cae-417e-8026-22c7e1e16fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained sentencepiece model\n",
    "spm_model = spm.SentencePieceProcessor(model_file=\"bpe.model\")\n",
    "\n",
    "# preprocess the sentences from train/dev/test sets\n",
    "for partition in [\"train\", \"dev\", \"tst\"]:\n",
    "    for lang in [\"de\", \"en\"]:\n",
    "        f_out = open(f\"sample_data/spm.{partition}.de-en.{lang}\", \"w\")\n",
    "\n",
    "        with open(f\"sample_data/{partition}.de-en.{lang}\", \"r\") as f_in:\n",
    "            for line_idx, line in enumerate(f_in.readlines()):\n",
    "                # Segmented into subwords\n",
    "                line_segmented = spm_model.encode(line.strip(), out_type=str)\n",
    "                # Join the subwords into a string\n",
    "                line_segmented = \" \".join(line_segmented)\n",
    "                f_out.write(line_segmented + \"\\n\")\n",
    "\n",
    "        f_out.close()\n",
    "        \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cc8b072-6615-49c5-a62e-126f8e429568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 91812\n",
      "-rw-r--r-- 1 vzhekova input   199632 Okt 24 16:30 dev.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   223089 Okt 24 16:30 dev.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   418647 Okt 24 16:30 tst.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   473219 Okt 24 16:30 tst.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 18496311 Okt 24 16:30 train.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 16663248 Okt 24 16:30 train.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input 28468890 Nov 12 15:55 spm.train.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input 26971608 Nov 12  2022 spm.train.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   343454 Nov 12  2022 spm.dev.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   321479 Nov 12  2022 spm.dev.de-en.en\n",
      "-rw-r--r-- 1 vzhekova input   728203 Nov 12  2022 spm.tst.de-en.de\n",
      "-rw-r--r-- 1 vzhekova input   678280 Nov 12  2022 spm.tst.de-en.en\n",
      "\n",
      "First lines of tokenized English:\n",
      "\n",
      "▁It ▁can ▁be ▁a ▁very ▁complicated ▁thing , ▁the ▁ocean .\n",
      "▁And ▁it ▁can ▁be ▁a ▁very ▁complicated ▁thing , ▁what ▁human ▁health ▁is .\n",
      "▁And ▁bring ing ▁those ▁two ▁together ▁might ▁seem ▁a ▁very ▁da un ting ▁task , ▁but ▁what ▁I ' m ▁going ▁to ▁try ▁to ▁say ▁is ▁that ▁even ▁in ▁that ▁complexity , ▁there ' s ▁some ▁simple ▁them es ▁that ▁I ▁think , ▁if ▁we ▁understand , ▁we ▁can ▁really ▁move ▁forward .\n",
      "▁And ▁those ▁simple ▁them es ▁aren ' t ▁really ▁them es ▁about ▁the ▁complex ▁science ▁of ▁what ' s ▁going ▁on , ▁but ▁things ▁that ▁we ▁all ▁pretty ▁well ▁know .\n",
      "▁And ▁I ' m ▁going ▁to ▁start ▁with ▁this ▁one : ▁If ▁mom ma ▁a in ' t ▁happy , ▁a in ' t ▁nobody ▁happy .\n",
      "▁We ▁know ▁that , ▁right ? ▁We ' ve ▁experience d ▁that .\n",
      "▁And ▁if ▁we ▁just ▁take ▁that ▁and ▁we ▁build ▁from ▁there , ▁then ▁we ▁can ▁go ▁to ▁the ▁next ▁step , ▁which ▁is ▁that ▁if ▁the ▁ocean ▁a in ' t ▁happy , ▁a in ' t ▁nobody ▁happy .\n",
      "▁That ' s ▁the ▁them e ▁of ▁my ▁talk .\n",
      "▁And ▁we ' re ▁making ▁the ▁ocean ▁pretty ▁unhappy ▁in ▁a ▁lot ▁of ▁different ▁ways .\n",
      "▁This ▁is ▁a ▁shot ▁of ▁Can ner y ▁Ro w ▁in ▁19 3 2 .\n",
      "\n",
      "First lines of tokenized German:\n",
      "\n",
      "▁Das ▁Meer ▁kann ▁ziemlich ▁kompliziert ▁sein .\n",
      "▁Und ▁was ▁menschliche ▁Gesundheit ▁ist , ▁kann ▁auch ▁ziemlich ▁kompliziert ▁sein .\n",
      "▁Und ▁diese ▁zwei ▁zusammen ▁zu ▁bringen , ▁erscheint ▁vielleicht ▁wie ▁eine ▁gewaltige ▁Aufgabe . ▁Aber ▁was ▁ich ▁Ihnen ▁zu ▁sagen ▁versuche ▁ist , ▁dass ▁es ▁trotz ▁dieser ▁Komplexität ▁einige ▁einfache ▁Themen ▁gibt , ▁von ▁denen ▁ich ▁denke , ▁wenn ▁wir ▁diese ▁verstehen , ▁können ▁wir ▁uns ▁wirklich ▁weiter ▁entwickeln .\n",
      "▁Und ▁diese ▁einfachen ▁Themen ▁sind ▁eigentlich ▁keine ▁komplexe n ▁wissen schaftlichen ▁Zusammen hänge , ▁sondern ▁Tatsache n , die ▁wir ▁alle ▁gut ▁kennen .\n",
      "▁Und ▁ich ▁werde ▁mit ▁dieser ▁hier ▁anfangen : ▁Wenn ▁die ▁Mama ▁nicht ▁glücklich ▁ist , ▁ist ▁keiner ▁glücklich .\n",
      "▁Kenn en ▁wir ▁das ▁nicht ▁alle ? ▁Das ▁haben ▁wir ▁alle ▁schon ▁erlebt .\n",
      "▁Wenn ▁wir ▁das ▁nehmen ▁und ▁darauf ▁aufbauen , ▁dann ▁können ▁wir ▁einen ▁Schritt ▁weiter ▁gehen : ▁Wenn ▁das ▁Meer ▁nicht ▁glücklich ▁ist , ▁ist ▁keiner ▁glücklich .\n",
      "▁Da rum ▁geht ▁es ▁in ▁meinem ▁Vortrag .\n",
      "▁Wir ▁machen ▁das ▁Meer ▁auf ▁viele ▁verschiedene ▁Arten ▁ziemlich ▁unglücklich .\n",
      "▁Das ▁ist ▁ein ▁Bild ▁der ▁Can ner y ▁Ro w ▁von ▁19 3 2 .\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr sample_data\n",
    "\n",
    "!echo -e \"\\nFirst lines of tokenized English:\\n\"\n",
    "!head sample_data/spm.train.de-en.en\n",
    "!echo -e \"\\nFirst lines of tokenized German:\\n\"\n",
    "!head sample_data/spm.train.de-en.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f01f44e-bb99-40d8-a23d-64e61c235e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vzhekova/fairseq/examples/translation/sample_data\n",
      "2022-11-12 16:46:42 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iwslt14.de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='/home/vzhekova/fairseq/examples/translation/sample_data/spm.tst.de-en', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/home/vzhekova/fairseq/examples/translation/sample_data/spm.train.de-en', use_plasma_view=False, user_dir=None, validpref='/home/vzhekova/fairseq/examples/translation/sample_data/spm.dev.de-en', wandb_project=None, workers=8)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/bin/fairseq-preprocess\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/preprocess.py\", line 299, in main\n",
      "    raise FileExistsError(_dict_path(args.source_lang, args.destdir))\n",
      "FileExistsError: data-bin/iwslt14.de-en/dict.en.txt\n"
     ]
    }
   ],
   "source": [
    "# Preprocess/binarize the data\n",
    "TEXT=\"/home/vzhekova/fairseq/examples/translation/sample_data\"\n",
    "!echo $TEXT\n",
    "# Binarize the data for training\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en --target-lang de \\\n",
    "    --trainpref $TEXT/spm.train.de-en \\\n",
    "    --validpref $TEXT/spm.dev.de-en \\\n",
    "    --testpref $TEXT/spm.tst.de-en \\\n",
    "    --destdir data-bin/iwslt14.de-en \\\n",
    "    --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0661db82-813a-4928-9d2e-64a453f79d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-25 11:33:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-25 11:33:56 | INFO | fairseq.tasks.translation | [en] dictionary: 6120 types\n",
      "2022-11-25 11:33:56 | INFO | fairseq.tasks.translation | [de] dictionary: 7624 types\n",
      "2022-11-25 11:33:58 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(6120, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(7624, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=7624, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-11-25 11:33:58 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-11-25 11:33:58 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-11-25 11:33:58 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-11-25 11:33:58 | INFO | fairseq_cli.train | num. shared model params: 51,175,424 (num. trained: 51,175,424)\n",
      "2022-11-25 11:33:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-11-25 11:33:58 | INFO | fairseq.data.data_utils | loaded 2,052 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/valid.en-de.en\n",
      "2022-11-25 11:33:58 | INFO | fairseq.data.data_utils | loaded 2,052 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/valid.en-de.de\n",
      "2022-11-25 11:33:58 | INFO | fairseq.tasks.translation | /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en valid en-de 2052 examples\n",
      "2022-11-25 11:34:02 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-11-25 11:34:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-25 11:34:02 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     \n",
      "2022-11-25 11:34:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-11-25 11:34:02 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-11-25 11:34:02 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-11-25 11:34:02 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "2022-11-25 11:34:02 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
      "2022-11-25 11:34:02 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-11-25 11:34:02 | INFO | fairseq.data.data_utils | loaded 174,443 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/train.en-de.en\n",
      "2022-11-25 11:34:03 | INFO | fairseq.data.data_utils | loaded 174,443 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/train.en-de.de\n",
      "2022-11-25 11:34:03 | INFO | fairseq.tasks.translation | /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en train en-de 174443 examples\n",
      "2022-11-25 11:34:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 001:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 11:34:03 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-11-25 11:34:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/vzhekova/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001: 100%|▉| 1266/1267 [06:04<00:00,  3.52it/s, loss=8.9, nll_loss=8.221, 2022-11-25 11:40:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.88it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 11.32it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.69it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.26it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.41it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.99it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.98it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 15.04it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 11:40:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.564 | nll_loss 7.813 | ppl 224.9 | wps 30122.9 | wpb 1919.9 | bsz 73.3 | num_updates 1267\n",
      "2022-11-25 11:40:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1267 updates\n",
      "2022-11-25 11:40:09 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint1.pt\n",
      "2022-11-25 11:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint1.pt\n",
      "2022-11-25 11:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 1267 updates, score 8.564) (writing took 32.24604367814027 seconds)\n",
      "2022-11-25 11:40:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-11-25 11:40:42 | INFO | train | epoch 001 | loss 9.768 | nll_loss 9.239 | ppl 604.42 | wps 11325.5 | ups 3.19 | wpb 3549.6 | bsz 137.7 | num_updates 1267 | lr 0.000158375 | gnorm 1.695 | train_wall 356 | gb_free 8.7 | wall 400\n",
      "2022-11-25 11:40:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 002:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 11:40:42 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-11-25 11:40:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 1266/1267 [05:56<00:00,  3.56it/s, loss=7.747, nll_loss=6.8792022-11-25 11:46:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:08,  3.25it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:03,  8.26it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.92it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.49it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.93it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.29it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.33it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.46it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.72it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.70it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.74it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 11:46:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.671 | nll_loss 6.75 | ppl 107.66 | wps 29873.5 | wpb 1919.9 | bsz 73.3 | num_updates 2534 | best_loss 7.671\n",
      "2022-11-25 11:46:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2534 updates\n",
      "2022-11-25 11:46:41 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint2.pt\n",
      "2022-11-25 11:46:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint2.pt\n",
      "2022-11-25 11:47:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 2534 updates, score 7.671) (writing took 31.376434087986127 seconds)\n",
      "2022-11-25 11:47:12 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-11-25 11:47:12 | INFO | train | epoch 002 | loss 8.147 | nll_loss 7.344 | ppl 162.45 | wps 11513.7 | ups 3.24 | wpb 3549.6 | bsz 137.7 | num_updates 2534 | lr 0.00031675 | gnorm 1.271 | train_wall 349 | gb_free 8.6 | wall 790\n",
      "2022-11-25 11:47:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 003:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 11:47:12 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-11-25 11:47:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 1266/1267 [06:03<00:00,  3.33it/s, loss=7.234, nll_loss=6.28,2022-11-25 11:53:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:08,  3.14it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:03,  7.88it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.51it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 11.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.21it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.67it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 13.92it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.00it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 13.98it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.22it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.22it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.33it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 11:53:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.207 | nll_loss 6.201 | ppl 73.59 | wps 28784.7 | wpb 1919.9 | bsz 73.3 | num_updates 3801 | best_loss 7.207\n",
      "2022-11-25 11:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3801 updates\n",
      "2022-11-25 11:53:19 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint3.pt\n",
      "2022-11-25 11:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint3.pt\n",
      "2022-11-25 11:53:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 3801 updates, score 7.207) (writing took 30.5963143769186 seconds)\n",
      "2022-11-25 11:53:49 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-11-25 11:53:49 | INFO | train | epoch 003 | loss 7.409 | nll_loss 6.485 | ppl 89.57 | wps 11328 | ups 3.19 | wpb 3549.6 | bsz 137.7 | num_updates 3801 | lr 0.000475125 | gnorm 1.134 | train_wall 355 | gb_free 8.4 | wall 1187\n",
      "2022-11-25 11:53:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 004:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 11:53:49 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-11-25 11:53:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 1266/1267 [06:22<00:00,  3.51it/s, loss=6.724, nll_loss=5.6912022-11-25 12:00:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:09,  2.96it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:03,  7.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.45it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 11.86it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.48it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.87it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.15it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.11it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.35it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.69it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.63it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.75it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:00:15 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.891 | nll_loss 5.815 | ppl 56.28 | wps 29592.5 | wpb 1919.9 | bsz 73.3 | num_updates 5068 | best_loss 6.891\n",
      "2022-11-25 12:00:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5068 updates\n",
      "2022-11-25 12:00:15 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint4.pt\n",
      "2022-11-25 12:00:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint4.pt\n",
      "2022-11-25 12:00:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 5068 updates, score 6.891) (writing took 36.106027045985684 seconds)\n",
      "2022-11-25 12:00:51 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-11-25 12:00:51 | INFO | train | epoch 004 | loss 6.913 | nll_loss 5.909 | ppl 60.09 | wps 10668.4 | ups 3.01 | wpb 3549.6 | bsz 137.7 | num_updates 5068 | lr 0.000444203 | gnorm 1.023 | train_wall 374 | gb_free 8.7 | wall 1609\n",
      "2022-11-25 12:00:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 005:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:00:51 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-11-25 12:00:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 1266/1267 [06:19<00:00,  3.70it/s, loss=6.518, nll_loss=5.45,2022-11-25 12:07:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:06,  4.09it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  9.61it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 12.09it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 13.19it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.58it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.45it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.49it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.57it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.51it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 15.11it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 15.01it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 15.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:07:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.685 | nll_loss 5.565 | ppl 47.35 | wps 30174.9 | wpb 1919.9 | bsz 73.3 | num_updates 6335 | best_loss 6.685\n",
      "2022-11-25 12:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6335 updates\n",
      "2022-11-25 12:07:13 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint5.pt\n",
      "2022-11-25 12:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint5.pt\n",
      "2022-11-25 12:07:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 6335 updates, score 6.685) (writing took 32.369378049857914 seconds)\n",
      "2022-11-25 12:07:46 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-11-25 12:07:46 | INFO | train | epoch 005 | loss 6.503 | nll_loss 5.435 | ppl 43.25 | wps 10840.2 | ups 3.05 | wpb 3549.6 | bsz 137.7 | num_updates 6335 | lr 0.000397307 | gnorm 0.96 | train_wall 370 | gb_free 8.5 | wall 2024\n",
      "2022-11-25 12:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 006:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:07:46 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-11-25 12:07:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 1266/1267 [06:28<00:00,  3.47it/s, loss=6.13, nll_loss=5.001,2022-11-25 12:14:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:10,  2.69it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:03,  6.85it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02,  9.45it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 10.84it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 12.46it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:01<00:01, 12.91it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.10it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 13.22it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 13.51it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.40it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:02<00:00, 14.56it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:14:17 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.553 | nll_loss 5.426 | ppl 42.98 | wps 28500.4 | wpb 1919.9 | bsz 73.3 | num_updates 7602 | best_loss 6.553\n",
      "2022-11-25 12:14:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 7602 updates\n",
      "2022-11-25 12:14:17 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint6.pt\n",
      "2022-11-25 12:14:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint6.pt\n",
      "2022-11-25 12:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 7602 updates, score 6.553) (writing took 31.759232949931175 seconds)\n",
      "2022-11-25 12:14:49 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-11-25 12:14:49 | INFO | train | epoch 006 | loss 6.215 | nll_loss 5.1 | ppl 34.29 | wps 10631.5 | ups 3 | wpb 3549.6 | bsz 137.7 | num_updates 7602 | lr 0.00036269 | gnorm 0.951 | train_wall 378 | gb_free 8.6 | wall 2447\n",
      "2022-11-25 12:14:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 007:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:14:49 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-11-25 12:14:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 1266/1267 [06:13<00:00,  3.37it/s, loss=5.966, nll_loss=4.81,2022-11-25 12:21:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.82it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  9.04it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 11.45it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.56it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.83it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.20it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.15it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.19it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.36it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:21:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.469 | nll_loss 5.324 | ppl 40.06 | wps 28607.7 | wpb 1919.9 | bsz 73.3 | num_updates 8869 | best_loss 6.469\n",
      "2022-11-25 12:21:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 8869 updates\n",
      "2022-11-25 12:21:05 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint7.pt\n",
      "2022-11-25 12:21:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint7.pt\n",
      "2022-11-25 12:21:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 8869 updates, score 6.469) (writing took 34.057510380866006 seconds)\n",
      "2022-11-25 12:21:40 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-11-25 12:21:40 | INFO | train | epoch 007 | loss 6.004 | nll_loss 4.854 | ppl 28.92 | wps 10926.1 | ups 3.08 | wpb 3549.6 | bsz 137.7 | num_updates 8869 | lr 0.000335786 | gnorm 0.972 | train_wall 365 | gb_free 8.5 | wall 2858\n",
      "2022-11-25 12:21:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 008:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:21:41 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-11-25 12:21:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008: 100%|▉| 1266/1267 [06:04<00:00,  3.53it/s, loss=5.917, nll_loss=4.7512022-11-25 12:27:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.44it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 11.03it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.33it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.13it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.37it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.31it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.28it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.60it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.62it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.71it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:27:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.433 | nll_loss 5.266 | ppl 38.49 | wps 29459.6 | wpb 1919.9 | bsz 73.3 | num_updates 10136 | best_loss 6.433\n",
      "2022-11-25 12:27:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10136 updates\n",
      "2022-11-25 12:27:48 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint8.pt\n",
      "2022-11-25 12:27:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint8.pt\n",
      "2022-11-25 12:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 10136 updates, score 6.433) (writing took 31.60954480106011 seconds)\n",
      "2022-11-25 12:28:21 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-11-25 12:28:21 | INFO | train | epoch 008 | loss 5.84 | nll_loss 4.663 | ppl 25.34 | wps 11229.5 | ups 3.16 | wpb 3549.6 | bsz 137.7 | num_updates 10136 | lr 0.000314099 | gnorm 1 | train_wall 358 | gb_free 8.8 | wall 3259\n",
      "2022-11-25 12:28:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 009:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:28:21 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-11-25 12:28:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 1266/1267 [06:09<00:00,  3.33it/s, loss=5.56, nll_loss=4.337,2022-11-25 12:34:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:08,  3.32it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.83it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.37it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.06it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.19it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.23it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.27it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.29it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:34:33 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.092 | nll_loss 4.867 | ppl 29.17 | wps 29195.4 | wpb 1919.9 | bsz 73.3 | num_updates 11403 | best_loss 6.092\n",
      "2022-11-25 12:34:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 11403 updates\n",
      "2022-11-25 12:34:33 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint9.pt\n",
      "2022-11-25 12:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint9.pt\n",
      "2022-11-25 12:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 11403 updates, score 6.092) (writing took 29.977704371791333 seconds)\n",
      "2022-11-25 12:35:03 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-11-25 12:35:03 | INFO | train | epoch 009 | loss 5.611 | nll_loss 4.397 | ppl 21.06 | wps 11184.6 | ups 3.15 | wpb 3549.6 | bsz 137.7 | num_updates 11403 | lr 0.000296135 | gnorm 1.092 | train_wall 361 | gb_free 8.7 | wall 3661\n",
      "2022-11-25 12:35:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 010:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:35:03 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-11-25 12:35:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 1266/1267 [06:07<00:00,  3.42it/s, loss=5.368, nll_loss=4.1132022-11-25 12:41:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.47it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.68it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.19it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.20it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.46it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.40it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.61it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.94it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.91it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.95it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:41:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.885 | nll_loss 4.643 | ppl 24.99 | wps 29898.4 | wpb 1919.9 | bsz 73.3 | num_updates 12670 | best_loss 5.885\n",
      "2022-11-25 12:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 12670 updates\n",
      "2022-11-25 12:41:13 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint10.pt\n",
      "2022-11-25 12:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint10.pt\n",
      "2022-11-25 12:41:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 12670 updates, score 5.885) (writing took 32.05926694790833 seconds)\n",
      "2022-11-25 12:41:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-11-25 12:41:45 | INFO | train | epoch 010 | loss 5.381 | nll_loss 4.129 | ppl 17.5 | wps 11176.3 | ups 3.15 | wpb 3549.6 | bsz 137.7 | num_updates 12670 | lr 0.000280939 | gnorm 1.228 | train_wall 360 | gb_free 8.4 | wall 4063\n",
      "2022-11-25 12:41:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 011:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:41:46 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2022-11-25 12:41:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 011: 100%|▉| 1266/1267 [06:05<00:00,  3.35it/s, loss=5.217, nll_loss=3.94,2022-11-25 12:47:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.40it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.52it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.96it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.39it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.03it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.20it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.18it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.38it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.27it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.30it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.38it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:47:54 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.692 | nll_loss 4.395 | ppl 21.04 | wps 28906.5 | wpb 1919.9 | bsz 73.3 | num_updates 13937 | best_loss 5.692\n",
      "2022-11-25 12:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 13937 updates\n",
      "2022-11-25 12:47:54 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint11.pt\n",
      "2022-11-25 12:48:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint11.pt\n",
      "2022-11-25 12:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint11.pt (epoch 11 @ 13937 updates, score 5.692) (writing took 30.595908483024687 seconds)\n",
      "2022-11-25 12:48:25 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2022-11-25 12:48:25 | INFO | train | epoch 011 | loss 5.183 | nll_loss 3.899 | ppl 14.92 | wps 11259.5 | ups 3.17 | wpb 3549.6 | bsz 137.7 | num_updates 13937 | lr 0.000267865 | gnorm 1.255 | train_wall 359 | gb_free 7.5 | wall 4463\n",
      "2022-11-25 12:48:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 012:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:48:25 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2022-11-25 12:48:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 012: 100%|▉| 1266/1267 [06:11<00:00,  3.63it/s, loss=4.965, nll_loss=3.65,2022-11-25 12:54:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.63it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.79it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 11.29it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.56it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.02it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.09it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.17it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.42it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.36it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.53it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.89it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.79it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.87it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 12:54:39 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.425 | nll_loss 4.059 | ppl 16.67 | wps 29768.6 | wpb 1919.9 | bsz 73.3 | num_updates 15204 | best_loss 5.425\n",
      "2022-11-25 12:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 15204 updates\n",
      "2022-11-25 12:54:39 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint12.pt\n",
      "2022-11-25 12:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint12.pt\n",
      "2022-11-25 12:55:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint12.pt (epoch 12 @ 15204 updates, score 5.425) (writing took 31.683864257996902 seconds)\n",
      "2022-11-25 12:55:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2022-11-25 12:55:11 | INFO | train | epoch 012 | loss 4.992 | nll_loss 3.68 | ppl 12.81 | wps 11065.1 | ups 3.12 | wpb 3549.6 | bsz 137.7 | num_updates 15204 | lr 0.000256461 | gnorm 1.223 | train_wall 364 | gb_free 8.4 | wall 4869\n",
      "2022-11-25 12:55:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 013:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 12:55:11 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2022-11-25 12:55:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 013: 100%|▉| 1266/1267 [06:26<00:00,  3.66it/s, loss=4.762, nll_loss=3.4212022-11-25 13:01:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:06,  4.16it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  9.78it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 12.16it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 13.38it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.66it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.42it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.40it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.58it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.48it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.74it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.98it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.90it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.92it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 14.67it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:01:40 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.243 | nll_loss 3.838 | ppl 14.3 | wps 29761.7 | wpb 1919.9 | bsz 73.3 | num_updates 16471 | best_loss 5.243\n",
      "2022-11-25 13:01:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 16471 updates\n",
      "2022-11-25 13:01:40 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint13.pt\n",
      "2022-11-25 13:01:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint13.pt\n",
      "2022-11-25 13:02:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint13.pt (epoch 13 @ 16471 updates, score 5.243) (writing took 31.112821544054896 seconds)\n",
      "2022-11-25 13:02:12 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2022-11-25 13:02:12 | INFO | train | epoch 013 | loss 4.82 | nll_loss 3.484 | ppl 11.19 | wps 10703.2 | ups 3.02 | wpb 3549.6 | bsz 137.7 | num_updates 16471 | lr 0.0002464 | gnorm 1.235 | train_wall 376 | gb_free 8.6 | wall 5289\n",
      "2022-11-25 13:02:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 014:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:02:12 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2022-11-25 13:02:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 014: 100%|▉| 1266/1267 [06:05<00:00,  3.49it/s, loss=4.59, nll_loss=3.226,2022-11-25 13:08:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.41it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 11.05it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.40it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.08it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.13it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.09it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.22it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.59it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.52it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.57it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:08:20 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.144 | nll_loss 3.73 | ppl 13.27 | wps 29312.2 | wpb 1919.9 | bsz 73.3 | num_updates 17738 | best_loss 5.144\n",
      "2022-11-25 13:08:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 17738 updates\n",
      "2022-11-25 13:08:20 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint14.pt\n",
      "2022-11-25 13:08:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint14.pt\n",
      "2022-11-25 13:08:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint14.pt (epoch 14 @ 17738 updates, score 5.144) (writing took 29.360353376017883 seconds)\n",
      "2022-11-25 13:08:49 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2022-11-25 13:08:49 | INFO | train | epoch 014 | loss 4.668 | nll_loss 3.313 | ppl 9.94 | wps 11305.7 | ups 3.19 | wpb 3549.6 | bsz 137.7 | num_updates 17738 | lr 0.000237437 | gnorm 1.236 | train_wall 358 | gb_free 8.7 | wall 5687\n",
      "2022-11-25 13:08:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 015:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:08:49 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2022-11-25 13:08:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 015: 100%|▉| 1266/1267 [06:06<00:00,  3.30it/s, loss=4.509, nll_loss=3.1342022-11-25 13:14:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.56it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  8.70it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 11.36it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.59it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.06it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.23it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.44it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.80it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.74it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.83it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 14.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:14:58 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.055 | nll_loss 3.626 | ppl 12.34 | wps 29646.9 | wpb 1919.9 | bsz 73.3 | num_updates 19005 | best_loss 5.055\n",
      "2022-11-25 13:14:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 19005 updates\n",
      "2022-11-25 13:14:58 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint15.pt\n",
      "2022-11-25 13:15:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint15.pt\n",
      "2022-11-25 13:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint15.pt (epoch 15 @ 19005 updates, score 5.055) (writing took 29.511132375802845 seconds)\n",
      "2022-11-25 13:15:28 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2022-11-25 13:15:28 | INFO | train | epoch 015 | loss 4.539 | nll_loss 3.166 | ppl 8.98 | wps 11271.8 | ups 3.18 | wpb 3549.6 | bsz 137.7 | num_updates 19005 | lr 0.000229386 | gnorm 1.22 | train_wall 359 | gb_free 8.5 | wall 6086\n",
      "2022-11-25 13:15:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 016:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:15:28 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2022-11-25 13:15:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 016: 100%|▉| 1266/1267 [06:12<00:00,  3.15it/s, loss=4.445, nll_loss=3.06,2022-11-25 13:21:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.38it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:03,  8.29it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.91it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.33it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.97it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.14it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.15it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.23it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.56it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.63it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.70it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:21:44 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.974 | nll_loss 3.532 | ppl 11.57 | wps 29325.9 | wpb 1919.9 | bsz 73.3 | num_updates 20272 | best_loss 4.974\n",
      "2022-11-25 13:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 20272 updates\n",
      "2022-11-25 13:21:44 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint16.pt\n",
      "2022-11-25 13:21:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint16.pt\n",
      "2022-11-25 13:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint16.pt (epoch 16 @ 20272 updates, score 4.974) (writing took 30.512970611918718 seconds)\n",
      "2022-11-25 13:22:15 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2022-11-25 13:22:15 | INFO | train | epoch 016 | loss 4.431 | nll_loss 3.044 | ppl 8.25 | wps 11066.5 | ups 3.12 | wpb 3549.6 | bsz 137.7 | num_updates 20272 | lr 0.000222102 | gnorm 1.207 | train_wall 365 | gb_free 8.6 | wall 6493\n",
      "2022-11-25 13:22:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 017:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:22:15 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2022-11-25 13:22:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 017: 100%|▉| 1266/1267 [06:11<00:00,  3.46it/s, loss=4.386, nll_loss=2.9952022-11-25 13:28:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:08,  3.22it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:03,  8.08it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:02, 10.64it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.11it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.21it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.68it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.12it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.54it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.57it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.55it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:02<00:00, 14.53it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:28:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.927 | nll_loss 3.492 | ppl 11.25 | wps 28904.8 | wpb 1919.9 | bsz 73.3 | num_updates 21539 | best_loss 4.927\n",
      "2022-11-25 13:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 21539 updates\n",
      "2022-11-25 13:28:29 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint17.pt\n",
      "2022-11-25 13:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint17.pt\n",
      "2022-11-25 13:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint17.pt (epoch 17 @ 21539 updates, score 4.927) (writing took 33.44776231306605 seconds)\n",
      "2022-11-25 13:29:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2022-11-25 13:29:03 | INFO | train | epoch 017 | loss 4.338 | nll_loss 2.938 | ppl 7.67 | wps 11023.9 | ups 3.11 | wpb 3549.6 | bsz 137.7 | num_updates 21539 | lr 0.00021547 | gnorm 1.199 | train_wall 363 | gb_free 8.5 | wall 6900\n",
      "2022-11-25 13:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 018:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:29:03 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2022-11-25 13:29:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 018: 100%|▉| 1266/1267 [06:15<00:00,  3.52it/s, loss=4.223, nll_loss=2.81,2022-11-25 13:35:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.83it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  9.23it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 11.85it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 13.01it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.34it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.36it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.54it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.50it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.63it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.90it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.86it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.89it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:35:21 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.874 | nll_loss 3.426 | ppl 10.75 | wps 29973.1 | wpb 1919.9 | bsz 73.3 | num_updates 22806 | best_loss 4.874\n",
      "2022-11-25 13:35:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 22806 updates\n",
      "2022-11-25 13:35:21 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint18.pt\n",
      "2022-11-25 13:35:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint18.pt\n",
      "2022-11-25 13:35:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint18.pt (epoch 18 @ 22806 updates, score 4.874) (writing took 35.765060767997056 seconds)\n",
      "2022-11-25 13:35:57 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2022-11-25 13:35:57 | INFO | train | epoch 018 | loss 4.253 | nll_loss 2.842 | ppl 7.17 | wps 10854.6 | ups 3.06 | wpb 3549.6 | bsz 137.7 | num_updates 22806 | lr 0.000209399 | gnorm 1.179 | train_wall 366 | gb_free 8.7 | wall 7315\n",
      "2022-11-25 13:35:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 019:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:35:57 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2022-11-25 13:35:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 019: 100%|▉| 1266/1267 [06:32<00:00,  3.55it/s, loss=4.186, nll_loss=2.7692022-11-25 13:42:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:07,  3.81it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  9.39it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 11.83it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 12.76it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 13.86it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.15it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.44it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 14.71it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 14.69it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 14.68it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 14.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:42:32 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.836 | nll_loss 3.379 | ppl 10.41 | wps 29335.4 | wpb 1919.9 | bsz 73.3 | num_updates 24073 | best_loss 4.836\n",
      "2022-11-25 13:42:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 24073 updates\n",
      "2022-11-25 13:42:32 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint19.pt\n",
      "2022-11-25 13:42:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint19.pt\n",
      "2022-11-25 13:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint19.pt (epoch 19 @ 24073 updates, score 4.836) (writing took 28.202673449879512 seconds)\n",
      "2022-11-25 13:43:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2022-11-25 13:43:01 | INFO | train | epoch 019 | loss 4.183 | nll_loss 2.763 | ppl 6.79 | wps 10610.8 | ups 2.99 | wpb 3549.6 | bsz 137.7 | num_updates 24073 | lr 0.000203814 | gnorm 1.18 | train_wall 379 | gb_free 8.5 | wall 7739\n",
      "2022-11-25 13:43:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1267\n",
      "epoch 020:   0%|                                       | 0/1267 [00:00<?, ?it/s]2022-11-25 13:43:01 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2022-11-25 13:43:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 020: 100%|▉| 1266/1267 [06:03<00:00,  3.67it/s, loss=4.147, nll_loss=2.7242022-11-25 13:49:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:   4%|▎      | 1/28 [00:00<00:06,  4.18it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  11%|▊      | 3/28 [00:00<00:02,  9.59it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  18%|█▎     | 5/28 [00:00<00:01, 12.14it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  25%|█▊     | 7/28 [00:00<00:01, 13.19it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  32%|██▎    | 9/28 [00:00<00:01, 14.55it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  39%|██▎   | 11/28 [00:00<00:01, 14.47it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  46%|██▊   | 13/28 [00:01<00:01, 14.49it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  54%|███▏  | 15/28 [00:01<00:00, 14.63it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  61%|███▋  | 17/28 [00:01<00:00, 14.55it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  68%|████  | 19/28 [00:01<00:00, 14.75it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  75%|████▌ | 21/28 [00:01<00:00, 15.03it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  82%|████▉ | 23/28 [00:01<00:00, 15.00it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  89%|█████▎| 25/28 [00:01<00:00, 15.02it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  96%|█████▊| 27/28 [00:01<00:00, 15.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-11-25 13:49:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.797 | nll_loss 3.336 | ppl 10.1 | wps 30034.3 | wpb 1919.9 | bsz 73.3 | num_updates 25340 | best_loss 4.797\n",
      "2022-11-25 13:49:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 25340 updates\n",
      "2022-11-25 13:49:07 | INFO | fairseq.trainer | Saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint20.pt\n",
      "2022-11-25 13:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/vzhekova/Biases_NMT/Notebooks/checkpoints/checkpoint20.pt\n",
      "2022-11-25 13:49:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint20.pt (epoch 20 @ 25340 updates, score 4.797) (writing took 29.867120309965685 seconds)\n",
      "2022-11-25 13:49:37 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2022-11-25 13:49:37 | INFO | train | epoch 020 | loss 4.118 | nll_loss 2.689 | ppl 6.45 | wps 11344.2 | ups 3.2 | wpb 3549.6 | bsz 137.7 | num_updates 25340 | lr 0.000198654 | gnorm 1.179 | train_wall 357 | gb_free 8.5 | wall 8135\n",
      "2022-11-25 13:49:37 | INFO | fairseq_cli.train | done training in 8134.5 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "    /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --max-epoch 20 \\\n",
    "    --reset-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd62baa2-8d09-47f6-9903-12958649d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-25 14:24:08 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-11-25 14:24:10 | INFO | fairseq.tasks.translation | [en] dictionary: 6120 types\n",
      "2022-11-25 14:24:10 | INFO | fairseq.tasks.translation | [de] dictionary: 7624 types\n",
      "2022-11-25 14:24:10 | INFO | fairseq_cli.generate | loading model(s) from /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint_best.pt\n",
      "2022-11-25 14:24:23 | INFO | fairseq.data.data_utils | loaded 4,698 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/test.en-de.en\n",
      "2022-11-25 14:24:23 | INFO | fairseq.data.data_utils | loaded 4,698 examples from: /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en/test.en-de.de\n",
      "2022-11-25 14:24:23 | INFO | fairseq.tasks.translation | /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en test en-de 4698 examples\n",
      "2022-11-25 14:25:58 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-11-25 14:25:58 | INFO | fairseq_cli.generate | Translated 4,698 sentences (112,228 tokens) in 30.4s (154.58 sentences/s, 3692.66 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "TEST_INPUT=\"/home/vzhekova/fairseq/examples/translation/sample_data/spm.tst.de-en.de\"\n",
    "PRED_LOG=\"/home/vzhekova/fairseq/examples/translation/en-de.decode.log\"\n",
    "\n",
    "!fairseq-generate /home/vzhekova/fairseq/examples/translation/data-bin/iwslt14.de-en \\\n",
    "      --task translation \\\n",
    "      --source-lang en \\\n",
    "      --target-lang de \\\n",
    "      --path /home/vzhekova/fairseq/examples/translation/checkpoints/checkpoint_best.pt \\\n",
    "      --batch-size 256 \\\n",
    "      --beam 1 \\\n",
    "      --remove-bpe=sentencepiece > $PRED_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372cc63a-e30a-4fbd-a20d-c2ac3ff2f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hypotheses and references from the decoding log file\n",
    "!grep ^H $PRED_LOG | sed 's/^H-//g' | cut -f 3 | sed 's/ ##//g' > ./hyp.txt\n",
    "!grep ^T $PRED_LOG | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > ./ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb7daab-4196-48d4-916a-968e9a8c05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Und er sagt:\n",
      "Sie wieder zu erfassen.\n",
      "Wir gehen ab.\n",
      "Und ich dachte darüber nach.\n",
      "Was wollen diese Menschen wollen?\n",
      "Ich wollte sie unterstützen.\n",
      "\"Ja\", sagte er.\n",
      "Wer hat das Beste?\n",
      "Und warum ist das so?\n",
      "Es macht sie gut.\"\n",
      "..........\n",
      "Und er sagt...\n",
      "Bewerte sie wieder.\n",
      "Wir heben ab.\n",
      "Ich dachte darüber nach.\n",
      "Was brauchen diese Menschen?\n",
      "Ich wollte sie unterstützten.\n",
      "\"Ja\", sagte er.\n",
      "Wer ist am erfolgreichsten?\n",
      "Und warum ist das so?\n",
      "So fühlen sie sich gut.\"\n"
     ]
    }
   ],
   "source": [
    "!head ./hyp.txt\n",
    "print(\"..........\")\n",
    "!head ./ref.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127b23c5-d00c-45ff-91a0-4bb7d2f7c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 26.1,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
      " \"verbose_score\": \"59.3/32.1/19.6/12.4 (BP = 1.000 ratio = 1.007 hyp_len = 86517 ref_len = 85901)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.3.1\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "!cat ./hyp.txt | sacrebleu ref.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
