{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb9c1b6-7f4d-468a-984f-e7b61fa4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# SEED = 1234\n",
    "\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.enabled = False \n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "PATH=\"/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity/Sampling\"\n",
    "FASTBPE=\"/home/vzhekova/fastBPE/fast\" # path to the fastBPE tool\n",
    "FAST_ALIGN=\"/home/vzhekova/fast_align/build/fast_align\" # path to the fast_align tool\n",
    "TERCOM = \"/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_male/Perturbation-basedQE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48aa5a31-2b6f-4744-bd0e-cd820e61740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# check if we can connect to the GPU with PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c199b4a0-7cab-4d4c-b270-e55ef77d010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity/Sampling\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ec0de-a7d8-4c97-a71f-48fb82d1ac2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Translation English-German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0534e037-1503-462b-aacc-bed90ec0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46951a34-7917-47b4-84dc-f454251e4d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from bpecodes.en ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from tok.en_original.en ...\n",
      "Read 6 words (6 unique) from text file.\n",
      "Applying BPE to tok.en_original.en ...\n",
      "Modified 6 words from text file.\n",
      "2023-09-19 17:24:12 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_en-de', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict='/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.en.txt', suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='bpe.en_original', tgtdict='/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.de.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-09-19 17:24:12 | INFO | fairseq_cli.preprocess | [en] Dictionary: 42024 types\n",
      "2023-09-19 17:24:12 | INFO | fairseq_cli.preprocess | [en] bpe.en_original.en: 1 sents, 8 tokens, 0.0% replaced (by <unk>)\n",
      "2023-09-19 17:24:12 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_en-de\n",
      "2023-09-19 17:24:17 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': True, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-19 17:24:17 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-19 17:24:17 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-19 17:24:17 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-19 17:24:54 | INFO | fairseq.data.data_utils | loaded 1 examples from: data-bin_original_en-de/test.en-de.en\n",
      "2023-09-19 17:24:54 | INFO | fairseq.tasks.translation | data-bin_original_en-de test en-de 1 examples\n",
      "2023-09-19 17:24:58 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-19 17:24:58 | INFO | fairseq_cli.generate | Translated 1 sentences (7 tokens) in 0.7s (1.52 sentences/s, 10.63 tokens/s)\n",
      "['Dieser argumentierte mit John .']\n",
      "2023-09-19 17:25:03 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': True, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-19 17:25:03 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-19 17:25:03 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-19 17:25:03 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-19 17:25:38 | INFO | fairseq.data.data_utils | loaded 1 examples from: data-bin_original_en-de/test.en-de.en\n",
      "2023-09-19 17:25:38 | INFO | fairseq.tasks.translation | data-bin_original_en-de test en-de 1 examples\n",
      "2023-09-19 17:25:43 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-19 17:25:43 | INFO | fairseq_cli.generate | Translated 1 sentences (7 tokens) in 0.7s (1.47 sentences/s, 10.28 tokens/s)\n",
      "['Dieser argumentierte mit John .']\n",
      "2023-09-19 17:25:48 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': True, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-19 17:25:49 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-19 17:25:49 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-19 17:25:49 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-19 17:26:24 | INFO | fairseq.data.data_utils | loaded 1 examples from: data-bin_original_en-de/test.en-de.en\n",
      "2023-09-19 17:26:24 | INFO | fairseq.tasks.translation | data-bin_original_en-de test en-de 1 examples\n",
      "2023-09-19 17:26:28 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-19 17:26:28 | INFO | fairseq_cli.generate | Translated 1 sentences (7 tokens) in 0.8s (1.33 sentences/s, 9.28 tokens/s)\n",
      "['Dieser argumentierte mit John .']\n",
      "2023-09-19 17:26:34 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': True, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-19 17:26:34 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-19 17:26:34 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-19 17:26:34 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-19 17:27:11 | INFO | fairseq.data.data_utils | loaded 1 examples from: data-bin_original_en-de/test.en-de.en\n",
      "2023-09-19 17:27:11 | INFO | fairseq.tasks.translation | data-bin_original_en-de test en-de 1 examples\n",
      "2023-09-19 17:27:15 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-19 17:27:15 | INFO | fairseq_cli.generate | Translated 1 sentences (7 tokens) in 0.8s (1.24 sentences/s, 8.70 tokens/s)\n",
      "['Dieser argumentierte mit John .']\n",
      "2023-09-19 17:27:21 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': True, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-19 17:27:22 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-19 17:27:22 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-19 17:27:22 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n"
     ]
    }
   ],
   "source": [
    "with open('en_original.en') as fin, open('hyp_original.txt','w') as hyp_out:\n",
    "    all_translations = []\n",
    "    \n",
    "    for line in fin:\n",
    "        with open('tok.en_original.en','w') as fout:\n",
    "            print(line, end='', file=fout)\n",
    "            fout.close()\n",
    "        \n",
    "        # Dividing text into subword units\n",
    "        filename_in = 'tok.en_original.en'\n",
    "        filename_out = 'bpe.en_original.en'\n",
    "        !$FASTBPE applybpe $filename_out $filename_in bpecodes.en\n",
    "        \n",
    "        # Binarize text\n",
    "        src = 'bpe.en_original'\n",
    "        destDir = 'data-bin_original_en-de'\n",
    "        !fairseq-preprocess \\\n",
    "            --source-lang en \\\n",
    "            --target-lang de \\\n",
    "            --testpref $src \\\n",
    "            --only-source \\\n",
    "            --srcdict /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.en.txt \\\n",
    "            --tgtdict /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.de.txt \\\n",
    "            --destdir $destDir \\\n",
    "            --workers 8\n",
    "\n",
    "        \n",
    "        translations = set()\n",
    "        \n",
    "        # Sample until 10 unqiue translations\n",
    "        while len(translations)  < 2:\n",
    "            # Translate\n",
    "            srcDir = 'data-bin_original_en-de'\n",
    "            filename_out = 'original_en-de.decode_Beam_10.log'\n",
    "            !fairseq-generate $srcDir  \\\n",
    "                --task translation \\\n",
    "                --source-lang en \\\n",
    "                --target-lang de \\\n",
    "                --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "                --sampling \\\n",
    "                --beam 1 \\\n",
    "                --nbest 1 \\\n",
    "                --batch-size 1 \\\n",
    "                --memory-efficient-fp16 \\\n",
    "                --remove-bpe > $filename_out\n",
    "            \n",
    "            \n",
    "            filename_in = 'original_en-de.decode_Beam_10.log'\n",
    "            translation = !grep ^H $filename_in | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g'\n",
    "            print(translation)\n",
    "            translations.add(translation.n)\n",
    "        \n",
    "        for tran in translations:\n",
    "            all_translations.append(tran)\n",
    "        \n",
    "    for tran in all_translations:\n",
    "        print(tran, end='\\n', file=hyp_out)\n",
    "        \n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5db6b3b-c60b-40e1-b101-868baafc4fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-18 14:57:45 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': True, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-18 14:57:45 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-18 14:57:45 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-18 14:57:45 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-18 14:58:20 | INFO | fairseq.data.data_utils | loaded 335 examples from: data-bin_original_en-de/test.en-de.en\n",
      "2023-09-18 14:58:20 | INFO | fairseq.tasks.translation | data-bin_original_en-de test en-de 335 examples\n",
      "2023-09-18 15:01:49 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-18 15:01:49 | INFO | fairseq_cli.generate | Translated 335 sentences (4,693 tokens) in 203.4s (1.65 sentences/s, 23.07 tokens/s)\n",
      "Finished translation.\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "# Top-K-Sampling --sampling-topk 10 \\\n",
    "# Top-p-Sampling --sampling-topp 0.8 \\\n",
    "# Temperature --temperature 0.9 \\\n",
    "!fairseq-generate data-bin_original_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --sampling \\\n",
    "    --beam 1 \\\n",
    "    --nbest 1 \\\n",
    "    --batch-size 1 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_en-de.decode_P_0.8.log\n",
    "\n",
    "# !fairseq-generate data-bin_disambiguated_male_en-de  \\\n",
    "#     --task translation \\\n",
    "#     --source-lang en \\\n",
    "#     --target-lang de \\\n",
    "#     --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "#     --sampling \\\n",
    "#     --sampling-topp 0.995 \\\n",
    "#     --beam 100 \\\n",
    "#     --nbest 100 \\\n",
    "#     --batch-size 2 \\\n",
    "#     --memory-efficient-fp16 \\\n",
    "#     --remove-bpe > disambiguated_male_en-de.decode_P_0.8.log\n",
    "\n",
    "# !fairseq-generate data-bin_disambiguated_female_en-de  \\\n",
    "#     --task translation \\\n",
    "#     --source-lang en \\\n",
    "#     --target-lang de \\\n",
    "#     --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "#     --sampling \\\n",
    "#     --sampling-topp 0.995 \\\n",
    "#     --beam 100 \\\n",
    "#     --nbest 100 \\\n",
    "#     --batch-size 2 \\\n",
    "#     --memory-efficient-fp16 \\\n",
    "#     --remove-bpe > disambiguated_female_en-de.decode_P_0.8.log\n",
    "\n",
    "print('Finished translation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6efb5c4a-0c37-4a6f-bd00-0aaea4da69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LC_ALL=C sort -V' sorts the results in natural order \n",
    "!grep ^H original_en-de.decode_P_0.8.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original.txt\n",
    "!grep ^H disambiguated_male_en-de.decode_P_0.8.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_disambiguated_male.txt\n",
    "!grep ^H disambiguated_female_en-de.decode_P_0.8.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_disambiguated_female.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425c7dc-f10a-4b6f-81e5-f7d89f14d91f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Backtranslation German-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "1b245127-a93d-45e3-b3a7-b9791d8b4189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original.txt ...\n",
      "Read 8088 words (244 unique) from text file.\n",
      "Applying BPE to hyp_original.txt ...\n",
      "Modified 8088 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_disambiguated_male.txt ...\n",
      "Read 8868 words (216 unique) from text file.\n",
      "Applying BPE to hyp_disambiguated_male.txt ...\n",
      "Modified 8868 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_disambiguated_female.txt ...\n",
      "Read 8146 words (206 unique) from text file.\n",
      "Applying BPE to hyp_disambiguated_female.txt ...\n",
      "Modified 8146 words from text file.\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Dividing tokenized text into subword units\n",
    "\n",
    "!$FASTBPE applybpe bpe.hyp_original.de hyp_original.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_disambiguated_male.de hyp_disambiguated_male.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_disambiguated_female.de hyp_disambiguated_female.txt bpecodes.de\n",
    "\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "9dd3cf73-9c96-49df-b749-f2bbe337df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-26 12:23:10 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-07-26 12:23:10 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-07-26 12:23:11 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original.de: 1030 sents, 10142 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-26 12:23:11 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_de-en\n",
      "2023-07-26 12:23:15 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_disambiguated_male_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_disambiguated_male', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-07-26 12:23:15 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-07-26 12:23:15 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_disambiguated_male.de: 1030 sents, 11558 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-26 12:23:15 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_disambiguated_male_de-en\n",
      "2023-07-26 12:23:19 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_disambiguated_female_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_disambiguated_female', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-07-26 12:23:20 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-07-26 12:23:20 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_disambiguated_female.de: 1030 sents, 10838 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-26 12:23:20 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_disambiguated_female_de-en\n",
      "Finished preprocessing.\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_disambiguated_male \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_disambiguated_male_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_disambiguated_female \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_disambiguated_female_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "print('Finished preprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "852891a2-e6c1-4300-9000-cd8f2a4ad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=\"/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble\"\n",
    "NBEST = 10\n",
    "BEAM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "07103d7f-cbc3-456a-acaa-1618cdcf70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-26 12:24:28 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'beam_mt': 0, 'nbest': 10, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-26 12:24:28 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-26 12:24:28 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-26 12:24:28 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-07-26 12:26:57 | INFO | fairseq.data.data_utils | loaded 1,030 examples from: data-bin_original_de-en/test.de-en.de\n",
      "2023-07-26 12:26:57 | INFO | fairseq.tasks.translation | data-bin_original_de-en test de-en 1030 examples\n",
      "2023-07-26 12:27:47 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-26 12:27:47 | INFO | fairseq_cli.generate | Translated 1,030 sentences (8,920 tokens) in 28.7s (35.90 sentences/s, 310.91 tokens/s)\n",
      "2023-07-26 12:27:58 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'beam_mt': 0, 'nbest': 10, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_disambiguated_male_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-26 12:27:58 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-26 12:27:58 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-26 12:27:58 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-07-26 12:28:44 | INFO | fairseq.data.data_utils | loaded 1,030 examples from: data-bin_disambiguated_male_de-en/test.de-en.de\n",
      "2023-07-26 12:28:44 | INFO | fairseq.tasks.translation | data-bin_disambiguated_male_de-en test de-en 1030 examples\n",
      "2023-07-26 12:29:35 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-26 12:29:35 | INFO | fairseq_cli.generate | Translated 1,030 sentences (9,740 tokens) in 30.6s (33.69 sentences/s, 318.55 tokens/s)\n",
      "2023-07-26 12:29:42 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'beam_mt': 0, 'nbest': 10, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_disambiguated_female_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-26 12:29:43 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-26 12:29:43 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-26 12:29:43 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-07-26 12:30:25 | INFO | fairseq.data.data_utils | loaded 1,030 examples from: data-bin_disambiguated_female_de-en/test.de-en.de\n",
      "2023-07-26 12:30:25 | INFO | fairseq.tasks.translation | data-bin_disambiguated_female_de-en test de-en 1030 examples\n",
      "2023-07-26 12:31:15 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-26 12:31:15 | INFO | fairseq_cli.generate | Translated 1,030 sentences (8,943 tokens) in 29.4s (35.08 sentences/s, 304.54 tokens/s)\n",
      "Finished translation.\n"
     ]
    }
   ],
   "source": [
    "# Generate backtranslations\n",
    "!fairseq-generate data-bin_original_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 64 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "!fairseq-generate data-bin_disambiguated_male_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 64 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > disambiguated_male_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "!fairseq-generate data-bin_disambiguated_female_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 64 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > disambiguated_female_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "print('Finished translation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "cd64bb5b-f01d-43e6-9b52-ca4a000040c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LC_ALL=C sort -V' sorts the results in natural order \n",
    "!grep ^H original_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_back.txt\n",
    "!grep ^H disambiguated_male_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_disambiguated_male_back.txt\n",
    "!grep ^H disambiguated_female_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_disambiguated_female_back.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "d1cc9b77-86fc-4604-9d6c-6c6a3eea94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished detokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize text        \n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "md_en = MosesDetokenizer(lang='en')\n",
    "\n",
    "with open('hyp_original_back.txt', encoding='utf8') as fin, open('original_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_disambiguated_male_back.txt', encoding='utf8') as fin, open('disambiguated_male_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_disambiguated_female_back.txt', encoding='utf8') as fin, open('disambiguated_female_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished detokenizing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a4d60-80fb-4ce1-b305-4dcaa5ddbbb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics on translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dab7a65c-57d3-4bd9-883c-b6166d8dc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "2060\n",
      "1030\n",
      "1030\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source = []\n",
    "with open('en_original.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "# List with disambiguated source sentences male\n",
    "source_disambiguated_male = []\n",
    "with open('en_disambiguated_male.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_disambiguated_male.append(line.strip())\n",
    "        \n",
    "# List with disambiguated source sentences female\n",
    "source_disambiguated_female = []\n",
    "with open('en_disambiguated_female.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_disambiguated_female.append(line.strip())        \n",
    "    \n",
    "# List with nbest sentences for every source in original\n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "# List with nbest sentences for every source in disambiguated male        \n",
    "nbest_disambiguated_male = []\n",
    "with open('hyp_disambiguated_male.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10):\n",
    "            nbest_disambiguated_male.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "# List with nbest sentences for every source in disambiguated female        \n",
    "nbest_disambiguated_female = []\n",
    "with open('hyp_disambiguated_female.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10):\n",
    "            nbest_disambiguated_female.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "print(len(source))\n",
    "print(len(nbest_original))\n",
    "print(len(nbest_disambiguated_male))\n",
    "print(len(nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce374900-43dd-484f-bd18-8f8c05e524d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b14270ea-45c3-41ba-a00a-8e3eba27ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique sentences in source nbest list for each source sentence\n",
    "def count_unique_sentences(nbest_sentences):\n",
    "    unique_sent = []\n",
    "    for source_nbest in nbest_sentences:\n",
    "        num_values = len(set(source_nbest))\n",
    "        #print(num_values)\n",
    "        unique_sent.append(num_values)\n",
    "\n",
    "    #print(unique_sent)\n",
    "    return sum(unique_sent)/len(nbest_sentences) # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e76e4e7a-74ad-405a-8fac-1bfd927999ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.183009708737863\n",
      "9.61747572815534\n",
      "9.332038834951456\n"
     ]
    }
   ],
   "source": [
    "# Value should be 10, because beam search generates 10 unique sentences\n",
    "print(count_unique_sentences(nbest_original))\n",
    "\n",
    "print(count_unique_sentences(nbest_disambiguated_male))\n",
    "\n",
    "print(count_unique_sentences(nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c0c73-c1d3-4ca9-9f3f-151c0e86f17f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "8fc503a0-e64c-479f-9366-86e8a53c0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words in source nbest list for each source sentence of original\n",
    "# !!! Method is slow\n",
    "import spacy\n",
    "\n",
    "def count_unique_words(nbest_sentences):\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    unique_words = []\n",
    "    normalizer = 0 # should normalize based on total number of words, because disambiguated sentences have more words overall\n",
    "    counter = 0\n",
    "    for source_nbest in nbest_sentences:\n",
    "        words = set()\n",
    "        for sent in source_nbest:\n",
    "            tokens = sp(sent)\n",
    "            normalizer += len(tokens)\n",
    "            for token in tokens:\n",
    "                if token.text not in stopwords:    # checking whether the word is a stop word\n",
    "                    words.add(token.text)\n",
    "        num_values = len(words)\n",
    "        unique_words.append(num_values)\n",
    "\n",
    "        counter += 1\n",
    "        #print(counter)\n",
    "\n",
    "    #print(unique_words)\n",
    "    print('Normalizer: ' + str(normalizer))\n",
    "    return sum(unique_words)/len(nbest_sentences) # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ef4217fa-a2ad-464c-b6a6-7ad3b2dabf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: 8088\n",
      "14.077669902912621\n",
      "Normalizer: 8868\n",
      "13.951456310679612\n",
      "Normalizer: 8146\n",
      "13.660194174757281\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words(nbest_original))\n",
    "\n",
    "print(count_unique_words(nbest_disambiguated_male))\n",
    "\n",
    "print(count_unique_words(nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95394217-96d3-4ea1-aa9b-65cec09c3be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Statistics on backtranslations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "8b8b4177-38a5-4294-8c2a-619c63fab031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "103\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source = []\n",
    "with open('en_original.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "# List with disambiguated source sentences male\n",
    "source_disambiguated_male = []\n",
    "with open('en_disambiguated_male.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_disambiguated_male.append(line.strip())\n",
    "        \n",
    "# List with disambiguated source sentences female\n",
    "source_disambiguated_female = []\n",
    "with open('en_disambiguated_female.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_disambiguated_female.append(line.strip())        \n",
    "    \n",
    "# List with nbest sentences for every source in original \n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "# List with nbest sentences for every source in disambiguated male\n",
    "nbest_disambiguated_male = []\n",
    "with open('disambiguated_male_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_disambiguated_male.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "# List with nbest sentences for every source in disambiguated female\n",
    "nbest_disambiguated_female = []\n",
    "with open('disambiguated_female_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_disambiguated_female.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "print(len(nbest_original))\n",
    "print(len(nbest_disambiguated_male))\n",
    "print(len(nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f66cfb-d1bc-4b83-8f19-8ab4b21303db",
   "metadata": {},
   "source": [
    "## Source sentence reoccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "618bb74e-385d-443a-b376-e949e99e2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many of the source sentences reoccur in the backtranslation\n",
    "def count_sentence_reoccurrence(source_sentences, nbest_sentences):\n",
    "    results = []\n",
    "    counter = 0\n",
    "    for sent in source_sentences:\n",
    "        matches = 0\n",
    "        for target in nbest_sentences[counter]: \n",
    "            if (sent == target):\n",
    "                matches += 1\n",
    "        results.append(matches)  \n",
    "        counter += 1\n",
    "\n",
    "    return sum(x > 0 for x in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "5b61cd08-a1ec-475a-a2d7-694d11b0dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "99\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(count_sentence_reoccurrence(source, nbest_original))\n",
    "\n",
    "print(count_sentence_reoccurrence(source_disambiguated_male, nbest_disambiguated_male))\n",
    "\n",
    "print(count_sentence_reoccurrence(source_disambiguated_female, nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecb949-c44c-42bd-8076-31893262c777",
   "metadata": {},
   "source": [
    "## Ambiguous source words reoccurrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "76e85e14-a11e-4eb6-95d5-aa76aedc593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accountant', 'administrator', 'advisee', 'advisor', 'analyst', 'appraiser', 'architect', 'assistant', 'attendant', 'auditor', 'baker', 'bartender', 'broker', 'buyer', 'bystander', 'carpenter', 'cashier', 'CEO', 'chef', 'chemist', 'chief', 'child', 'cleaner', 'clerk', 'client', 'construction', 'cook', 'counselor', 'customer', 'designer', 'developer', 'dietitian', 'dispatcher', 'doctor', 'driver', 'editor', 'educator', 'electrician', 'employee', 'engineer', 'examiner', 'farmer', 'firefighter', 'guard', 'guest', 'hairdresser', 'homeowner', 'housekeeper', 'hygienist', 'inspector', 'instructor', 'investigator', 'janitor', 'laborer', 'lawyer', 'librarian', 'machinist', 'manager', 'mechanic', 'mover', 'nurse', 'nutritionist', 'officer', 'onlooker', 'owner', 'painter', 'paralegal', 'paramedic', 'passenger', 'pathologist', 'patient', 'pedestrian', 'pharmacist', 'physician', 'planner', 'plumber', 'practitioner', 'programmer', 'protester', 'psychologist', 'receptionist', 'resident', 'salesperson', 'scientist', 'secretary', 'sheriff', 'specialist', 'student', 'supervisor', 'surgeon', 'tailor', 'taxpayer', 'teacher', 'technician', 'teenager', 'therapist', 'undergraduate', 'veterinarian', 'victim', 'visitor', 'witness', 'worker', 'writer']\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "# Extract ambiguous words from source sentences\n",
    "ambiguous_words = [] \n",
    "with open('en_original.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(\" \")\n",
    "        ambiguous_words.append(tokens[1].replace('\\n', ''))\n",
    "        \n",
    "print(ambiguous_words)\n",
    "print(len(ambiguous_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "ba9666d2-f3b2-4017-8214-3113ff7cff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many of the ambiguous words reoccur in the backtranslation\n",
    "def count_words_reoccurrence(ambiguous_words, nbest_sentences):\n",
    "    results = []\n",
    "    counter = 0\n",
    "    for word in ambiguous_words:\n",
    "        matches = 0\n",
    "        for target in nbest_sentences[counter]: \n",
    "            if (word in target.split(\" \")):\n",
    "                matches += 1\n",
    "        results.append(matches)  \n",
    "        counter += 1\n",
    "\n",
    "    return sum(x > 0 for x in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "93567f32-0885-419c-baf3-b4eb772c4e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "print(count_words_reoccurrence(ambiguous_words, nbest_original))\n",
    "\n",
    "print(count_words_reoccurrence(ambiguous_words, nbest_disambiguated_male))\n",
    "\n",
    "print(count_words_reoccurrence(ambiguous_words, nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628352f6-7bc2-4f3e-b4c9-72eddb35be6b",
   "metadata": {},
   "source": [
    "## Count unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "fd2f4488-4207-430b-8e53-00ba33560ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.54368932038835\n",
      "53.8252427184466\n",
      "50.04854368932039\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_sentences(nbest_original))\n",
    "\n",
    "print(count_unique_sentences(nbest_disambiguated_male))\n",
    "\n",
    "print(count_unique_sentences(nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265064a1-379a-4b66-ad78-61dcd8735726",
   "metadata": {},
   "source": [
    "## Count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "fe3fa560-d25d-401f-a776-c3080761d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: 75134\n",
      "17.78640776699029\n",
      "Normalizer: 83146\n",
      "17.864077669902912\n",
      "Normalizer: 75934\n",
      "18.37864077669903\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words(nbest_original))\n",
    "\n",
    "print(count_unique_words(nbest_disambiguated_male))\n",
    "\n",
    "print(count_unique_words(nbest_disambiguated_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a31ec1-6057-4efc-9cb5-0958d64c2bc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (source-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f2bbb-6aa4-47fa-89bd-3855b164c388",
   "metadata": {},
   "source": [
    "- Input to fast_align must be tokenized and aligned into parallel sentences. \n",
    "- Line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space (|||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "b3cba97b-037d-4e23-8f09-e1470cf6c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alignment_input(sentencesN, sourceIn, targetIn, output):\n",
    "    # List with original source sentences\n",
    "    source = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    # List with nbest sentences for every source in original \n",
    "    target = []\n",
    "    counter = 0\n",
    "    temp = []\n",
    "    with open(targetIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            temp.append(line.strip())\n",
    "            counter += 1\n",
    "            if (counter == 10):\n",
    "                target.append(temp)\n",
    "                counter = 0\n",
    "                temp = []\n",
    "\n",
    "    #print(len(source))\n",
    "    #print(len(target))           \n",
    "\n",
    "    count = 0\n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            for hyp in target[count]:\n",
    "                print(source[count] + ' ||| ' + hyp, end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "b13acbaf-db12-49d2-a25b-1ce4d3615989",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_alignment_input(103, 'tok.en_original.en', 'hyp_original.txt', 'original_source-target_en-de.txt')\n",
    "build_alignment_input(103, 'tok.en_disambiguated_male.en', 'hyp_disambiguated_male.txt', 'disambiguated_male_source-target_en-de.txt')\n",
    "build_alignment_input(103, 'tok.en_disambiguated_female.en', 'hyp_disambiguated_female.txt', 'disambiguated_female_source-target_en-de.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e0a48-e74f-468a-b949-a9cc13a2a519",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fast_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1477d44-1efd-4dda-8549-8b29c438e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$FAST_ALIGN -i original_source-target_en-de.txt -d -o -v > original_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i disambiguated_male_source-target_en-de.txt -d -o -v > disambiguated_male_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i disambiguated_female_source-target_en-de.txt -d -o -v > disambiguated_female_source-target_en-de_fast-aligned.txt\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "2de69fff-155f-44b2-a456-134fce70042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Count unique translated words to the ambiguous words in translations per source sentence\n",
    "def count_unique_words_alignment_translations(position, sentencesN, sourceIn, translationsIn, alignmentsIn, output):\n",
    "    \n",
    "    # Get positions of ambigous words\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "        \n",
    "    # List with translations\n",
    "    translations = []\n",
    "    with open(translationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            translations.append(line.strip())\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Extract alginments of ambiguous words\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                indices.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                indices.append([999])\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = set() # set forces uniqueness\n",
    "    for translation in translations:\n",
    "        tokens = translation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                #print(lineNumber)\n",
    "                #print(tokens[ind])\n",
    "                #print(ind)\n",
    "                translated_ambiguous_words.add(tokens[ind])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 10 == 0):\n",
    "                translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "                translated_ambiguous_words = set()\n",
    "\n",
    "    #print(translations_ambiguous_words)\n",
    "    #print(len(translations_ambiguous_words))\n",
    "    \n",
    "    # Add results to file\n",
    "    ambiguous_words = []\n",
    "    source = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position])\n",
    "\n",
    "    count = 0                \n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(translations_ambiguous_words[count]), end='\\n', file=fout)\n",
    "            count += 1\n",
    "\n",
    "    unique_translations = 0\n",
    "    for set_words in translations_ambiguous_words:\n",
    "        \n",
    "        ############################################################\n",
    "        # remove gender info; removing \"in\" and \"e\" endings in words\n",
    "        set_words_new = set()\n",
    "        for word in set_words:\n",
    "            word_new = re.sub(\"in$|e$\", \"\", word)\n",
    "            #print(word_new)\n",
    "            set_words_new.add(word_new)\n",
    "        #print(set_words_new)\n",
    "        ############################################################\n",
    "        \n",
    "        unique_translations += len(set_words_new)\n",
    "        \n",
    "    #print(unique_translations)\n",
    "    return unique_translations/sentencesN # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "57d33e67-ceeb-4660-ad40-94619a21f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2524271844660193\n",
      "======\n",
      "2.1359223300970873\n",
      "======\n",
      "2.145631067961165\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_translations(1, 103, 'tok.en_original.en', 'hyp_original.txt', 'original_source-target_en-de_fast-aligned.txt', 'unique-words_translations_original.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(2, 103, 'tok.en_disambiguated_male.en', 'hyp_disambiguated_male.txt', 'disambiguated_male_source-target_en-de_fast-aligned.txt', 'unique-words_translations_disambiguated_male.txt')) # positions is 2 because of gender word\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(2, 103, 'tok.en_disambiguated_female.en', 'hyp_disambiguated_female.txt', 'disambiguated_female_source-target_en-de_fast-aligned.txt', 'unique-words_translations_disambiguated_female.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547be75-dfc4-46c3-ba3f-dfe56c4a3037",
   "metadata": {},
   "source": [
    "## awesome_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "7e5d0cfa-eb33-4a71-af0c-9fa94a473c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 1030it [00:02, 440.29it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 1030it [00:02, 500.44it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 1030it [00:01, 524.49it/s]\n",
      "Finished alignment\n"
     ]
    }
   ],
   "source": [
    "# ??? How to set model correctly\n",
    "# MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "!awesome-align \\\n",
    "    --output_file \"original_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"disambiguated_male_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"disambiguated_male_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"disambiguated_female_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"disambiguated_female_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "e72d55fd-918f-4b24-af93-df1cf3b09577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9320388349514563\n",
      "======\n",
      "1.9320388349514563\n",
      "======\n",
      "1.9320388349514563\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_translations(1, 103, 'tok.en_original.en', 'hyp_original.txt', 'original_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_original.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(2, 103, 'tok.en_disambiguated_male.en', 'hyp_disambiguated_male.txt', 'disambiguated_male_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_disambiguated_male.txt')) # positions is 2 because of gender word\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(2, 103, 'tok.en_disambiguated_female.en', 'hyp_disambiguated_female.txt', 'disambiguated_female_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_disambiguated_female.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab63c-5750-42b2-b80a-ea767942f356",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (translation-backtranslation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ac057-9ea8-4bff-a824-8f4498ed95a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fast_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f6e57-70df-4156-99b5-859e397f52b7",
   "metadata": {},
   "source": [
    "- Input to fast_align must be tokenized and aligned into parallel sentences. \n",
    "- Line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space (|||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "66afe2b0-c7ce-441b-8071-5a19daf8f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_alignment_input(1030, 'hyp_original.txt', 'hyp_original_back.txt', 'original_translation-back_en-de.txt')\n",
    "build_alignment_input(1030, 'hyp_disambiguated_male.txt', 'hyp_disambiguated_male_back.txt', 'disambiguated_male_translation-back_en-de.txt')\n",
    "build_alignment_input(1030, 'hyp_disambiguated_female.txt', 'hyp_disambiguated_female_back.txt', 'disambiguated_female_translation-back_en-de.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd83be3-c2a7-4245-844c-52f51e416a4d",
   "metadata": {},
   "source": [
    "- Word alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6552ff-349f-4048-b65b-4c4959901ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$FAST_ALIGN -i original_translation-back_en-de.txt -d -o -v > original_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i disambiguated_male_translation-back_en-de.txt -d -o -v > disambiguated_male_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i disambiguated_female_translation-back_en-de.txt -d -o -v > disambiguated_female_translation-back_en-de_fast-aligned.txt\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d06b-82f7-4341-be31-eef67f4a415a",
   "metadata": {},
   "source": [
    "- Extract target backtranslated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "1cb16ca6-8c3b-4f4c-b547-2a72f013ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Count unique translated words to the ambiguous words in backtranslations per source sentence\n",
    "def count_unique_words_alignment_backtranslations(position, sentencesN, sourceIn, backtranslationsIn, alignmentsIn_translation, alignmentsIn_backtranslation, output):\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "       \n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    positions_ambiguous_words_translations = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn_translation, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                positions_ambiguous_words_translations.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                positions_ambiguous_words_translations.append([999])\n",
    "            lineNumber += 1\n",
    "    \n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(backtranslationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            backtranslations.append(line.strip())\n",
    "\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = []\n",
    "    with open(alignmentsIn_backtranslation, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            positions = positions_ambiguous_words_translations[counter] # exact positions of ambiguous words\n",
    "            list_indices = []\n",
    "            for position in positions:\n",
    "                regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    list_indices.extend([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    list_indices.extend([999])\n",
    "            indices.append(list_indices)\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    backtranslations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "    for backtranslation in backtranslations:\n",
    "        tokens = backtranslation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                #print(lineNumber)\n",
    "                #print(tokens[ind])\n",
    "                #print(ind)\n",
    "                backtranslated_ambiguous_words.add(tokens[ind])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 10 == 0):\n",
    "                backtranslations_ambiguous_words.append(backtranslated_ambiguous_words)\n",
    "                backtranslated_ambiguous_words = set()\n",
    "\n",
    "\n",
    "\n",
    "    #print(backtranslations_ambiguous_words)\n",
    "    print(len(backtranslations_ambiguous_words))\n",
    "\n",
    "    # Here we need to merge the sets for every 10 sets, because we want to see unique words in the nbest 100 backtranslation\n",
    "    backtranslations_ambiguous_words_reduced = []\n",
    "    backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "    counter = 0\n",
    "    for set_words in backtranslations_ambiguous_words:\n",
    "        backtranslated_ambiguous_words.update(set_words)\n",
    "        counter += 1\n",
    "        if (counter % 10 == 0):\n",
    "            backtranslations_ambiguous_words_reduced.append(backtranslated_ambiguous_words)\n",
    "            backtranslated_ambiguous_words = set()\n",
    "\n",
    "    print(len(backtranslations_ambiguous_words_reduced)) \n",
    "    \n",
    "    # Add results to file\n",
    "\n",
    "    ambiguous_words = []\n",
    "    source = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position])\n",
    "\n",
    "    count = 0                \n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(backtranslations_ambiguous_words_reduced[count]), end='\\n', file=fout)\n",
    "            count += 1\n",
    "\n",
    "    unique_backtranslations = 0\n",
    "    for set_words in backtranslations_ambiguous_words_reduced:\n",
    "        unique_backtranslations += len(set_words)\n",
    "        \n",
    "    return unique_backtranslations/sentencesN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "d20f678b-e0ad-43b9-8374-fc67c7535c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030\n",
      "103\n",
      "7.009708737864078\n",
      "1030\n",
      "103\n",
      "6.378640776699029\n",
      "1030\n",
      "103\n",
      "7.135922330097087\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_backtranslations(1, 103, 'tok.en_original.en', 'hyp_original_back.txt', 'original_source-target_en-de_fast-aligned.txt', 'original_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_original.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(2, 103, 'tok.en_disambiguated_male.en', 'hyp_disambiguated_male_back.txt', 'disambiguated_male_source-target_en-de_fast-aligned.txt', 'disambiguated_male_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_disambiguated_male.txt')) # positions is 2 because of gender word\n",
    "print(count_unique_words_alignment_backtranslations(2, 103, 'tok.en_disambiguated_female.en', 'hyp_disambiguated_female_back.txt', 'disambiguated_female_source-target_en-de_fast-aligned.txt', 'disambiguated_female_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_disambiguated_female.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed81f30-6996-4a42-a7bf-945dd7e74c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## awesome_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d423a-639b-4f9e-b9a7-a191578edc4e",
   "metadata": {},
   "source": [
    "- Extract the position of the translated ambiguous word from each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b6d21-3ddb-4904-ab00-b7c41e3a9016",
   "metadata": {},
   "source": [
    "- Word alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "7344a012-1da5-4328-a801-29a8a45826a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 10300it [00:12, 799.09it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 10300it [00:13, 742.12it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 10300it [00:13, 787.48it/s]\n",
      "Finished alignment.\n"
     ]
    }
   ],
   "source": [
    "# ??? How to set model correctly\n",
    "# MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "!awesome-align \\\n",
    "    --output_file \"original_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"disambiguated_male_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"disambiguated_male_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"disambiguated_female_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"disambiguated_female_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f0e8-a5b2-44d2-8732-5b68f5be0a6b",
   "metadata": {},
   "source": [
    "- Extract target backtranslated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "c0a78096-b560-4860-ab0a-e0c30d3b9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030\n",
      "103\n",
      "5.339805825242719\n",
      "1030\n",
      "103\n",
      "5.213592233009709\n",
      "1030\n",
      "103\n",
      "5.854368932038835\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_backtranslations(1, 103, 'tok.en_original.en', 'hyp_original_back.txt', 'original_source-target_en-de_awesome-aligned.txt', 'original_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_original.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(2, 103, 'tok.en_disambiguated_male.en', 'hyp_disambiguated_male_back.txt', 'disambiguated_male_source-target_en-de_awesome-aligned.txt', 'disambiguated_male_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_disambiguated_male.txt')) # positions is 2 because of gender word\n",
    "print(count_unique_words_alignment_backtranslations(2, 103, 'tok.en_disambiguated_female.en', 'hyp_disambiguated_female_back.txt', 'disambiguated_female_source-target_en-de_awesome-aligned.txt', 'disambiguated_female_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_disambiguated_female.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7aa2d8-87dd-40fb-a52b-b0859436258f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (translation-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef887b3-6413-4801-92de-f3f0948d02b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tercom alignement (borrowed from Tu)\n",
    "- https://github.com/TuAnh23/Perturbation-basedQE/blob/master/align_and_analyse_ambiguous_trans.py#L54-L92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab774e7-fd99-417a-90c5-a88658d65ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TuAnh23/Perturbation-basedQE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "9bc7a300-37fa-49b3-b1d0-2a65451a5440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_male/Perturbation-basedQE\n"
     ]
    }
   ],
   "source": [
    "%cd $TERCOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "39a6f345-4e59-45e4-98f7-1ff906fce45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import align_and_analyse_ambiguous_trans as tercom\n",
    "import pandas as pd\n",
    "\n",
    "def count_unique_words_tercom_alignment(position, sentencesN, sourceIn, backtranslationsIn):\n",
    "    # List with source sentences; output 100 times to match backtranslation size\n",
    "    source = []\n",
    "    with open(PATH + \"/\" + sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            for i in range(100): # append the source sentence 100 times to match backtranslations later\n",
    "                source.append(line.strip().split()) # split() tokenizes the sentence, because tercom expects tokens     \n",
    "\n",
    "    print(len(source))\n",
    "\n",
    "    # List with original backtranslations\n",
    "    backtranslations = []\n",
    "    with open(PATH + \"/\" + backtranslationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            backtranslations.append(line.strip().split())\n",
    "\n",
    "    print(len(backtranslations))\n",
    "    \n",
    "    # Generate alignments\n",
    "    alignments = tercom.tercom_alignment(source, backtranslations)\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "        \n",
    "    # Extract target translated words to source words\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = []\n",
    "    for align in alignments:\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        indices.append([item[1] for item in (item for item in align if not(pd.isna(item[0]))) if item[0] == position][0])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "            counter += 1\n",
    "\n",
    "    print(len(indices))\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = set() # set forces uniqueness\n",
    "    for backtranslation in backtranslations:\n",
    "        backtranslation_index = backtranslations.index(backtranslation)\n",
    "        if not(pd.isna(indices[backtranslation_index])):\n",
    "            translated_ambiguous_words.add(backtranslation[indices[backtranslation_index]])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "\n",
    "    #print(translations_ambiguous_words)\n",
    "    #print(len(translations_ambiguous_words))\n",
    "\n",
    "    unique_translations = 0\n",
    "    for set_words in translations_ambiguous_words:\n",
    "        unique_translations += len(set_words)\n",
    "        \n",
    "    return unique_translations/sentencesN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "dc5013c1-cf50-428b-be6f-db240fcd69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10300\n",
      "10300\n",
      "10300\n",
      "[{'consultant', 'bookkeeper', 'accountant', 'taxman', 'advisor', 'adviser'}, {'supervisor', 'caretaker', 'administrator', 'steward', 'Administrator', 'admin', 'caseworker', 'came', 'arrived', 'was', 'manager', 'turned', 'custodian', 'had', 'clerk'}, {'consultant', 'showed', 'counselor', 'counsellor', 'came', 'arrived', 'aide', 'turned', 'advisor', 'had', 'adviser'}, {'consultant', 'showed', 'counselor', 'counsellor', 'came', 'arrived', 'aide', 'turned', 'advisor', 'had', 'adviser'}, {'was', 'He', 'analyst', 'Analysts'}, {'consultant', 'had', 'surveyor', 'expert', 'appraiser', 'came', 'arrived', 'evaluator', 'turned', 'up', 'reviewer', 'assessor'}, {'architect', 'Architect', 'was', 'He', 'It'}, {'auxiliary', 'Assistant', 'paramedic', 'employee', 'volunteer', 'came', 'aide', 'helper', 'arrived', 'wizard', 'had', 'member', 'assistant'}, {'supervisor', 'train', 'caretaker', 'minder', 'conductor', 'carer', 'attendant', 'came', 'arrived', 'companion', 'turned', 'He', 'escort', 'guide', 'had', 'caregiver', 'chaperone'}, {'showed', 'chartered', 'auditor', 'accountant', 'came', 'arrived', 'auditors', 'turned', 'was', 'cashier', 'inspector', 'Auditor', 'had', 'examiner'}, {'baker', 'Baker', 'was', 'worker', 'bakery', 'bakers'}, {'barkeeper', 'barman', 'was', 'had', 'bartender'}, {'mediator', 'real', 'intermediary', 'middleman', 'realtor', 'Realtor', 'estate', 'brokerage', 'broker', 'turned', 'agent', 'had'}, {'had', 'Shoppers', 'Buyer', 'shopper', 'buyer', 'was', 'purchaser', 'turned', 'buyers', 'Buyers', 'customer'}, {'spectator', 'Spectators', 'audience', 'viewer', 'bystander', 'man', 'came', 'arrived', 'stranger', 'He', 'onlooker', 'was', 'turned', 'had', 'Viewers'}, {'maker', 'Carpenter', 'carpenter', 'cabinetmaker', 'He', 'carpentry', 'joiner'}, {'cashier'}, {'executive', 'CEO', 'chief', 'director', 'came', 'arrived', 'chairman', 'manager', 'company', 'of', 'boss'}, {'chief', 'cook', 'came', 'arrived', 'He', 'chef', 'Cook', 'had', 'boss'}, {'She', 'chemists', 'came', 'chemist', 'was', 'He', 'arrived', 'pharmacist'}, {'showed', 'chief', 'came', 'arrived', 'turned', 'chef', 'manager', 'had', 'boss'}, {'kid', 'He', 'baby', 'child'}, {'lady', 'woman', 'cleaner', 'cleaning', 'Cleaner', 'had', 'cleaners'}, {'She', 'employee', 'caseworker', 'came', 'arrived', 'cashier', 'turned', 'member', 'had', 'clerk'}, {'She', 'came', 'arrived', 'Customers', 'turned', 'was', 'client', 'had', 'customer'}, {'It', 'builder', 'building', 'construction'}, {'She', 'cook', 'came', 'arrived', 'was', 'He', 'chef', 'Cook', 'had'}, {'consultant', 'showed', 'supervisor', 'counselor', 'caretaker', 'minder', 'counsellor', 'carer', 'attendant', 'came', 'arrived', 'aide', 'turned', 'advisor', 'had', 'caregiver', 'adviser'}, {'Customer', 'came', 'arrived', 'Customers', 'He', 'turned', 'client', 'had', 'customer'}, {'constructor', 'designer', 'was', 'He', 'arrived', 'engineer'}, {'builder', 'developer', 'had', 'Developer', 'came', 'arrived', 'turned', 'It', 'contractor'}, {'dietician', 'dietitian', 'came', 'arrived', 'turned', 'had', 'assistant', 'nutritionist'}, {'instructor', 'conductor', 'attendant', 'came', 'arrived', 'driver', 'turned', 'dispatcher', 'manager', 'scheduler', 'had'}, {'doctors', 'physician', 'was', 'He', 'Doctor', 'doctor'}, {'Driver', 'came', 'arrived', 'driver', 'turned', 'motorist', 'had'}, {'Editor', 'editor', 'publisher', 'He'}, {'educator', 'tutor', 'came', 'pedagogue', 'teacher', 'arrived', 'had'}, {'was', 'electrician'}, {'employee', 'staff', 'came', 'arrived', 'turned', 'worker', 'member', 'had', 'clerk'}, {'train', 'arrived', 'driver', 'turned', 'had', 'engineer'}, {'showed', 'She', 'Inspector', 'reviewer', 'Examiner', 'auditor', 'came', 'arrived', 'was', 'turned', 'investigator', 'inspector', 'had', 'examiner'}, {'Farmer', 'was', 'arrived', 'farmer', 'He', 'had', 'peasant'}, {'fighter', 'Firefighters', 'firefighter', 'fireman', 'fire', 'service', 'firefighters', 'crews', 'came', 'arrived', 'all', 'turned', 'department', 'any', 'were', 'had', 'brigade'}, {'guards', 'man', 'were', 'guard', 'sentry'}, {'visitor', 'guest', 'Guest', 'Guests', 'He', 'guests', 'client'}, {'hairdresser', 'stylist', 'salon', 'hairdressers', 'hairstylist', 'dresser', 'barbershop', 'had', 'barber'}, {'house', 'homeowner', 'landlord', 'owner', 'landlady', 'the', 'had'}, {'was', 'householder', 'housekeeper', 'housemaid'}, {'specialist', 'expert', 'arrived', 'technician', 'sanitarian', 'professional', 'Hygienist', 'worker', 'hygienist'}, {'supervisor', 'Inspector', 'controller', 'came', 'arrived', 'inspector', 'had', 'examiner'}, {'instructor', 'Teacher', 'came', 'arrived', 'teacher', 'was', 'turned', 'had', 'trainer'}, {'Investigators', 'detective', 'investigators', 'came', 'arrived', 'was', 'He', 'officer', 'investigator', 'examiner'}, {'caretaker', 'housekeeper', 'concierge', 'janitor', 'Caretaker', 'arrived', 'turned', 'had'}, {'employee', 'labourer', 'had', 'Workers', 'came', 'arrived', 'laborer', 'turned', 'He', 'workers', 'worker', 'workman'}, {'barrister', 'solicitor', 'He', 'lawyer', 'attorney'}, {'She', 'Librarian', 'librarian'}, {'mechanist', 'machinist', 'man', 'arrived', 'operator', 'mechanic', 'machine', 'technician', 'engineer'}, {'executive', 'CEO', 'director', 'came', 'arrived', 'was', 'manager', 'turned', 'had'}, {'was', 'mechanic', 'engineer', 'mechanics'}, {'train', 'mower', 'maker', 'creator', 'doer', 'movers', 'came', 'arrived', 'driver', 'turned', 'attendee', 'participant', 'the', 'had', 'mover'}, {'paramedic', 'carer', 'nurse', 'arrived', 'was', 'nurses', 'were'}, {'dietician', 'dietitian', 'came', 'arrived', 'turned', 'He', 'had', 'nutritionist'}, {'civil', 'Officer', 'servant', 'arrived', 'officer', 'turned', 'He', 'official', 'had'}, {'spectator', 'Spectators', 'onlookers', 'audience', 'viewer', 'onlooker', 'came', 'arrived', 'looker', 'turned', 'It', 'had', 'Viewers'}, {'owner', 'Owner', 'landlord'}, {'was', 'artist', 'He', 'painter', 'It', 'Painter'}, {'parallelist', 'entry', 'paralegal', 'newcomer', 'came', 'arrived', 'lateral', 'He', 'paralegger', 'latecomer', 'harrow', 'para', 'Paralegal', 'was', 'entrant'}, {'paramedic', 'paramedics', 'came', 'arrived', 'physician', 'any', 'medic', 'doctor', 'Paramedics', 'emergency'}, {'@-@', 'Passenger', 'passenger', 'driver', 'passengers', 'were', 'Passengers', 'had'}, {'was', 'pathologist'}, {'She', 'patient', 'Patient', 'was', 'He'}, {'She', 'walker', 'woman', 'Pedestrians', 'came', 'He', 'pedestrian', 'Pedestrian'}, {'She', 'came', 'chemist', 'arrived', 'worker', 'He', 'pharmacy', 'apothecary', 'pharmacist'}, {'had', 'doctors', 'physician', 'was', 'He', 'Doctor', 'doctor'}, {'Planner', 'planners', 'was', 'designer', 'He', 'Planners', 'planner', 'It', 'scheduler'}, {'was', 'plumber', 'Plumber'}, {'She', 'Practitioner', 'practitioner', 'was', 'came', 'He', 'arrived'}, {'programmers', 'developer', 'was', 'He', 'programmer', 'coder'}, {'demonstrator', 'protester', 'protesters', 'had', 'protestor'}, {'was', 'She', 'He', 'psychologist'}, {'desk', 'receptionist', 'Receptionist'}, {'She', 'woman', 'occupant', 'inhabitant', 'resident', 'He', 'Residents'}, {'seller', 'salesman', 'salesperson', 'vendor', 'arrived', 'saleswoman', 'shop', 'turned', 'had', 'clerk', 'assistant'}, {'academic', 'scientist', 'was', 'researcher', 'He', 'scientists', 'It'}, {'Secretary', 'clerk', 'secretary'}, {'was', 'Sheriff', 'sheriff'}, {'specialist', 'expert', 'was', 'He', 'professional', 'had'}, {'She', 'pupil', 'was', 'He', 'student', 'schoolboy'}, {'supervisor', 'caretaker', 'minder', 'warden', 'carer', 'attendant', 'manager', 'superior', 'guard', 'had', 'caregiver', 'boss'}, {'was', 'He', 'surgeon'}, {'seamstress', 'tailor', 'was', 'He', 'dressmaker', 'It', 'tailors', 'Tailor'}, {'taxpayer', 'was', 'have', 'Taxpayers', 'Taxpayer', 'were', 'taxpayers', 'taxman', 'came', 'arrived', 'turned', 'had', 'payer'}, {'She', 'instructor', 'Teacher', 'came', 'was', 'teacher', 'arrived', 'turned', 'had'}, {'technicians', 'technician', 'was', 'mechanic', 'He', 'engineer'}, {'teen', 'boy', 'man', 'youngster', 'teenager', 'had', 'youth'}, {'was', 'She', 'therapist', 'He'}, {'Bachelorette', 'bachelor', 'Bachelor', 'students', 'came', 'arrived', 'was', 'He', 'student', 'has'}, {'showed', 'vet', 'vets', 'surgeon', 'came', 'arrived', 'was', 'turned', 'veterinarian', 'had'}, {'victim'}, {'visitor', 'Visitors', 'He', 'were', 'visitors'}, {'witness', 'She', 'He'}, {'employee', 'labourer', 'had', 'came', 'arrived', 'laborer', 'turned', 'He', 'workers', 'worker', 'workman'}, {'writer', 'came', 'arrived', 'author', 'He', 'was', 'novelist'}]\n",
      "103\n",
      "7.922330097087379\n",
      "10300\n",
      "10300\n",
      "10300\n",
      "[{'bookkeeper', 'accountant', 'came', 'arrived', 'was', 'turned', 'had'}, {'caretaker', 'administrator', 'steward', 'employee', 'admin', 'Administrator', 'official', 'arrived', 'administrative', 'manager', 'turned', 'worker', 'officer', 'member', 'had', 'clerk', 'assistant'}, {'consultant', 'counselor', 'counsellor', 'counsel', 'came', 'aide', 'arrived', 'turned', 'advisor', 'had', 'adviser'}, {'consultant', 'showed', 'counselor', 'counsellor', 'counsel', 'aide', 'arrived', 'turned', 'advisor', 'had', 'adviser'}, {'analyst'}, {'consultant', 'had', 'assessor', 'pundit', 'surveyor', 'expert', 'evaluator', 'arrived', 'came', 'turned', 'officer', 'reviewer', 'appraiser'}, {'was', 'arrived', 'architect', 'came'}, {'auxiliary', 'employee', 'staff', 'attendant', 'aide', 'helper', 'arrived', 'assistants', 'member', 'worker', 'assistant'}, {'conductor', 'attendant', 'companion', 'arrived', 'came', 'turned', 'up', 'escort', 'of', 'had', 'chaperone'}, {'showed', 'treasurer', 'chartered', 'auditor', 'accountant', 'came', 'arrived', 'cashier', 'turned', 'investigator', 'inspector', 'had', 'examiner'}, {'had', 'baker', 'Baker', 'came', 'arrived', 'was', 'worker', 'turned', 'master', 'bakery', 'bakers'}, {'barkeeper', 'came', 'arrived', 'barman', 'was', 'turned', 'had', 'bartender'}, {'mediator', 'real', 'intermediary', 'realtor', 'middleman', 'facilitator', 'estate', 'brokerage', 'broker', 'agent', 'had', 'stockbroker'}, {'showed', 'shopkeeper', 'shoppers', 'shopper', 'buyer', 'arrived', 'purchaser', 'turned', 'had', 'customer'}, {'spectator', 'bystanders', 'bystander', 'viewer', 'stranger', 'onlooker', 'turned', 'member', 'had'}, {'maker', 'carpenter', 'cabinetmaker', 'came', 'arrived', 'was', 'had', 'joiner'}, {'cashiers', 'came', 'arrived', 'cashier', 'turned', 'up', 'had'}, {'executive', 'CEO', 'chief', 'director', 'arrived', 'turned', 'manager', 'had'}, {'chief', 'cook', 'was', 'arrived', 'manager', 'chef', 'turned', 'Cook', 'He', 'had', 'boss'}, {'chemists', 'came', 'chemist', 'arrived', 'turned', 'was', 'He', 'had', 'engineer'}, {'Chief', 'chief', 'leader', 'came', 'arrived', 'chiefs', 'turned', 'chef', 'manager', 'had', 'boss', 'chieftain'}, {'husband', 'baby', 'man', 'came', 'kid', 'arrived', 'was', 'He', 'come', 'guy', 'had', 'child'}, {'lady', 'woman', 'cleaner', 'man', 'came', 'arrived', 'cleaners'}, {'employee', 'had', 'caseworker', 'staff', 'staffer', 'came', 'arrived', 'turned', 'member', 'worker', 'clerk'}, {'came', 'arrived', 'turned', 'client', 'had', 'customer'}, {'builder', 'building', 'construction', 'worker'}, {'cook', 'came', 'arrived', 'was', 'He', 'chef', 'turned', 'Cook', 'had'}, {'consultant', 'supervisor', 'counselor', 'caretaker', 'minder', 'counsellor', 'carer', 'attendant', 'came', 'arrived', 'aide', 'turned', 'advisor', 'had', 'caregiver', 'adviser'}, {'customers', 'clientele', 'came', 'arrived', 'turned', 'client', 'had', 'customer'}, {'designer', 'was'}, {'builder', 'developer', 'came', 'arrived', 'designer', 'turned', 'developers', 'contractor', 'engineer'}, {'dietician', 'dietitian', 'came', 'arrived', 'turned', 'had', 'assistant', 'nutritionist'}, {'instructor', 'came', 'arrived', 'driver', 'turned', 'was', 'come', 'dispatcher', 'manager', 'car', 'chauffeur', 'vehicle', 'had'}, {'had', 'husband', 'GP', 'man', 'came', 'arrived', 'was', 'physician', 'He', 'turned', 'guy', 'Doctor', 'doctor'}, {'man', 'came', 'arrived', 'driver', 'was', 'turned', 'come', 'motorist', 'had'}, {'editor', 'reporter', 'lecturer', 'was', 'teacher', 'reader', 'editors'}, {'instructor', 'educator', 'tutor', 'carer', 'pedagogue', 'arrived', 'teacher', 'guardian', 'came', 'turned', 'had', 'caregiver'}, {'electrician', 'came', 'arrived', 'was', 'turned', 'had'}, {'employee', 'staff', 'came', 'arrived', 'turned', 'worker', 'member', 'had', 'clerk'}, {'came', 'arrived', 'was', 'turned', 'had', 'engineer'}, {'showed', 'auditor', 'came', 'arrived', 'was', 'turned', 'investigator', 'inspector', 'had', 'examiner'}, {'Farmer', 'came', 'arrived', 'was', 'farmer', 'turned', 'He', 'had', 'peasant'}, {'fighter', 'firefighter', 'fireman', 'firefighters', 'came', 'was', 'arrived', 'turned', 'all', 'any', 'were', 'the', 'had', 'brigade'}, {'guards', 'man', 'were', 'guard', 'sentry', 'Guard'}, {'visitor', 'guest', 'customer'}, {'hairdresser', 'stylist', 'was', 'arrived', 'hairstylist', 'dresser', 'barbershop', 'had', 'barber'}, {'house', 'homeowner', 'landlord', 'came', 'owner', 'arrived', 'householder', 'turned', 'was', 'the', 'had'}, {'housekeeper', 'steward', 'housemaid', 'housemaster', 'came', 'arrived', 'householder', 'had'}, {'specialist', 'technician', 'professional', 'Hygienist', 'worker', 'hygienist'}, {'supervisor', 'showed', 'Inspector', 'Commissioner', 'controller', 'came', 'arrived', 'commissar', 'turned', 'inspector', 'had', 'commissioner', 'examiner'}, {'instructor', 'tutor', 'came', 'arrived', 'teacher', 'turned', 'was', 'had', 'trainer'}, {'detective', 'investigators', 'arrived', 'was', 'investigator', 'examiner'}, {'caretaker', 'housekeeper', 'janitor', 'concierge', 'attendant', 'arrived', 'was', 'turned', 'had'}, {'employee', 'labourer', 'had', 'came', 'arrived', 'was', 'laborer', 'turned', 'come', 'workers', 'worker', 'workman'}, {'showed', 'barrister', 'advocate', 'came', 'arrived', 'solicitor', 'turned', 'was', 'lawyer', 'attorney', 'had'}, {'arrived', 'had', 'came', 'librarian'}, {'machinist', 'locksmith', 'came', 'arrived', 'operator', 'mechanic', 'was', 'turned', 'up', 'man', 'fitter', 'had', 'engineer'}, {'executive', 'CEO', 'director', 'came', 'arrived', 'manager', 'turned', 'had'}, {'came', 'was', 'arrived', 'mechanic', 'turned', 'mechanics', 'had', 'engineer'}, {'train', 'husband', 'man', 'came', 'arrived', 'was', 'He', 'driver', 'turned', 'come', 'guy', 'had', 'mover'}, {'caretaker', 'paramedic', 'carer', 'keeper', 'came', 'arrived', 'nurse', 'nurses', 'turned', 'were', 'was', 'come', 'had'}, {'dietician', 'dietitian', 'came', 'arrived', 'turned', 'had', 'nutritionist'}, {'civil', 'employee', 'Officer', 'man', 'servant', 'arrived', 'come', 'officer', 'turned', 'was', 'came', 'He', 'official', 'had'}, {'observer', 'spectator', 'bystander', 'viewer', 'onlooker', 'came', 'arrived', 'turned', 'member', 'had'}, {'husband', 'landlord', 'man', 'owner', 'was', 'He', 'guy', 'proprietor'}, {'came', 'arrived', 'was', 'artist', 'He', 'turned', 'painter', 'up', 'had', 'Painter'}, {'commuter', 'freshman', 'husband', 'newcomer', 'man', 'came', 'arrived', 'was', 'He', 'paralegal', 'latecomer', 'Paralegal', 'driver', 'turned', 'come', 'guy', 'had', 'entrant'}, {'paramedic', 'paramedics', 'came', 'arrived', 'was', 'turned', 'medic', 'had', 'Paramedics'}, {'passenger', 'driver', 'had'}, {'had', 'came', 'arrived', 'turned', 'pathologist'}, {'patient', 'man', 'came', 'arrived', 'was', 'turned', 'He', 'had'}, {'She', 'walker', 'woman', 'passenger', 'Pedestrians', 'came', 'arrived', 'turned', 'He', 'pedestrian', 'had', 'Pedestrian'}, {'She', 'had', 'came', 'arrived', 'chemist', 'turned', 'worker', 'He', 'apothecary', 'pharmacist'}, {'had', 'husband', 'GP', 'man', 'came', 'arrived', 'was', 'He', 'turned', 'physician', 'Doctor', 'guy', 'doctor'}, {'planners', 'came', 'designer', 'was', 'arrived', 'planner', 'scheduler'}, {'plumber', 'came', 'arrived', 'was', 'installer', 'fitter', 'had', 'Plumber'}, {'showed', 'husband', 'man', 'Practitioner', 'arrived', 'practitioner', 'He', 'came', 'was', 'come', 'turned', 'guy', 'had'}, {'programmers', 'came', 'arrived', 'was', 'He', 'programmer', 'had', 'coder'}, {'demonstrator', 'protester', 'arrived', 'protesters', 'had', 'protestor', 'demonstrators'}, {'arrived', 'was', 'psychologist', 'came'}, {'receptionist', 'concierge', 'Receptionist', 'member', 'worker'}, {'husband', 'man', 'occupant', 'arrived', 'come', 'inhabitant', 'resident', 'was', 'came', 'He', 'guy', 'had'}, {'seller', 'salesman', 'salesperson', 'vendor', 'came', 'arrived', 'was', 'turned', 'shop', 'saleswoman', 'had', 'clerk', 'assistant'}, {'academic', 'scientist', 'came', 'arrived', 'was', 'turned', 'He', 'researcher', 'scientists', 'had', 'scholar'}, {'Secretary', 'came', 'arrived', 'was', 'secretary', 'clerk'}, {'showed', 'Sheriff', 'came', 'arrived', 'turned', 'sheriff', 'had'}, {'consultant', 'Specialist', 'employee', 'specialist', 'had', 'expert', 'came', 'arrived', 'turned', 'professional', 'member', 'worker', 'specialists'}, {'disciple', 'pupil', 'came', 'arrived', 'was', 'He', 'student', 'had'}, {'supervisor', 'caretaker', 'minder', 'warden', 'overseer', 'attendant', 'carer', 'manager', 'superior', 'guard', 'caregiver', 'boss'}, {'surgeon', 'came', 'arrived', 'turned', 'He', 'had'}, {'seamstress', 'tailor', 'came', 'arrived', 'was', 'He', 'come', 'turned', 'dressmaker', 'had'}, {'came', 'taxpayer', 'taxpayers', 'have', 'taxman', 'were', 'payer'}, {'instructor', 'Teacher', 'tutor', 'came', 'arrived', 'teacher', 'was', 'turned', 'teachers', 'had'}, {'came', 'arrived', 'technician', 'turned', 'mechanic', 'had', 'engineer'}, {'teen', 'boy', 'man', 'arrived', 'male', 'turned', 'teenager', 'had', 'youth', 'juvenile'}, {'arrived', 'therapist', 'was', 'came'}, {'Bachelorette', 'bachelor', 'Bachelor', 'undergraduate', 'arrived', 'turned', 'student', 'had'}, {'showed', 'vet', 'vets', 'surgeon', 'came', 'arrived', 'was', 'turned', 'veterinarian', 'had'}, {'victim'}, {'visitors', 'visitor', 'guest', 'attendee'}, {'witness', 'came', 'arrived', 'turned', 'up', 'had'}, {'employee', 'had', 'labourer', 'man', 'came', 'arrived', 'was', 'laborer', 'turned', 'He', 'come', 'worker', 'workman'}, {'writer', 'came', 'arrived', 'author', 'was', 'He', 'novelist', 'had', 'writers'}]\n",
      "103\n",
      "9.048543689320388\n",
      "10300\n",
      "10300\n",
      "10300\n",
      "[{'showed', 'bookkeeper', 'accountant', 'came', 'arrived', 'was', 'turned', 'had', 'bookie'}, {'caretaker', 'administrator', 'employee', 'had', 'staff', 'came', 'arrived', 'administrative', 'turned', 'manager', 'was', 'member', 'worker', 'clerk', 'assistant'}, {'consultant', 'She', 'counselor', 'counsellor', 'came', 'arrived', 'was', 'turned', 'aide', 'up', 'advisor', 'had', 'adviser'}, {'consultant', 'She', 'counselor', 'counsellor', 'came', 'arrived', 'was', 'turned', 'aide', 'advisor', 'had', 'adviser'}, {'She', 'Analysts', 'analyst', 'analysts', 'came', 'arrived', 'was', 'turned', 'had'}, {'consultant', 'had', 'assessor', 'surveyor', 'expert', 'evaluator', 'was', 'arrived', 'turned', 'came', 'reviewer', 'appraiser'}, {'showed', 'She', 'did', 'architect', 'Architect', 'was', 'arrived', 'turned', 'came', 'It', 'had'}, {'She', 'employee', 'staff', 'attendant', 'came', 'arrived', 'helper', 'turned', 'was', 'aide', 'worker', 'member', 'had', 'assistant'}, {'supervisor', 'waitress', 'minder', 'conductor', 'carer', 'attendant', 'companion', 'arrived', 'was', 'turned', 'came', 'escort', 'had', 'caregiver', 'childminder'}, {'showed', 'She', 'chartered', 'auditor', 'accountant', 'came', 'arrived', 'cashier', 'turned', 'come', 'was', 'investigator', 'inspector', 'had', 'examiner'}, {'She', 'lady', 'had', 'woman', 'baker', 'wife', 'came', 'arrived', 'was', 'worker', 'turned', 'bakery', 'bakers', 'to'}, {'barkeeper', 'barman', 'arrived', 'came', 'was', 'turned', 'barmaid', 'had', 'bartender'}, {'She', 'realtor', 'Realtor', 'estate', 'brokerage', 'was', 'broker', 'turned', 'agent', 'had'}, {'shopkeeper', 'shopper', 'buyer', 'arrived', 'was', 'turned', 'purchaser', 'had', 'customer'}, {'She', 'spectator', 'waitress', 'lady', 'woman', 'bystanders', 'bystander', 'viewer', 'wife', 'onlooker', 'came', 'arrived', 'was', 'turned', 'come', 'up', 'member', 'had'}, {'She', 'housewife', 'housekeeper', 'maker', 'chambermaid', 'Carpenter', 'carpenter', 'cabinetmaker', 'came', 'arrived', 'was', 'had', 'joiner'}, {'showed', 'lady', 'came', 'arrived', 'cashier', 'turned', 'up', 'was', 'had'}, {'executive', 'CEO', 'chief', 'director', 'chairwoman', 'came', 'arrived', 'was', 'manager', 'turned', 'come', 'chairman', 'had', 'boss'}, {'She', 'cook', 'came', 'arrived', 'was', 'come', 'chef', 'turned', 'up', 'had'}, {'She', 'chemists', 'pharmacist', 'came', 'chemist', 'arrived', 'was', 'turned', 'worker', 'It', 'had'}, {'showed', 'She', 'chief', 'director', 'leader', 'headmistress', 'came', 'was', 'teacher', 'manager', 'head', 'arrived', 'come', 'turned', 'had', 'boss'}, {'She', 'girl', 'lady', 'woman', 'baby', 'wife', 'came', 'arrived', 'come', 'kid', 'was', 'had', 'child'}, {'maid', 'lady', 'woman', 'cleaner', 'came', 'arrived', 'turned', 'had', 'cleaners'}, {'She', 'employee', 'caseworker', 'came', 'arrived', 'was', 'turned', 'saleswoman', 'cashier', 'worker', 'member', 'had', 'clerk', 'assistant'}, {'She', 'came', 'arrived', 'was', 'turned', 'come', 'client', 'had', 'customer'}, {'builder', 'came', 'building', 'too', 'construction'}, {'She', 'cook', 'came', 'was', 'arrived', 'come', 'chef', 'turned', 'up', 'had'}, {'consultant', 'supervisor', 'She', 'counselor', 'minder', 'counsellor', 'carer', 'attendant', 'came', 'arrived', 'was', 'turned', 'aide', 'advisor', 'had', 'caregiver', 'childminder', 'adviser'}, {'She', 'came', 'arrived', 'was', 'come', 'turned', 'client', 'had', 'customer'}, {'showed', 'She', 'came', 'designer', 'was', 'arrived', 'turned', 'has', 'had'}, {'She', 'builder', 'developer', 'had', 'came', 'arrived', 'was', 'designer', 'contractor', 'construction'}, {'dietician', 'dietitian', 'expert', 'came', 'arrived', 'was', 'turned', 'had', 'assistant', 'nutritionist'}, {'instructor', 'came', 'arrived', 'driver', 'was', 'come', 'manager', 'dispatcher', 'turned', 'had'}, {'She', 'had', 'came', 'physician', 'was', 'arrived', 'turned', 'Doctor', 'doctor'}, {'She', 'all', 'arrived', 'driver', 'was', 'turned', 'up', 'came', 'any', 'the', 'motorist', 'had'}, {'She', 'editor', 'publisher', 'proofreader', 'lecturer', 'came', 'was', 'arrived', 'turned', 'teacher', 'reader', 'had', 'Editor'}, {'instructor', 'She', 'educator', 'came', 'arrived', 'teacher', 'turned', 'up', 'pedagogue', 'come', 'was', 'had', 'school', 'educationalist'}, {'She', 'electrician', 'came', 'arrived', 'was', 'turned', 'up', 'had'}, {'She', 'employee', 'staff', 'came', 'arrived', 'was', 'turned', 'worker', 'member', 'had', 'clerk'}, {'showed', 'She', 'came', 'arrived', 'was', 'turned', 'had', 'engineer'}, {'showed', 'She', 'auditor', 'came', 'arrived', 'come', 'turned', 'was', 'tester', 'investigator', 'inspector', 'had', 'examinee', 'examiner'}, {'&apos;s', 'lady', 'woman', 'wife', 'came', 'arrived', 'farmer', 'turned', 'had', 'peasant'}, {'fighter', 'firefighter', 'fireman', 'fire', 'firewoman', 'all', 'arrived', 'came', 'turned', 'up', 'any', 'the', 'had', 'brigade'}, {'guardswoman', 'woman', 'watchwoman', 'Guardswoman', 'guards', 'came', 'arrived', 'were', 'guard', 'sentry', 'Guard', 'had'}, {'visitor', 'guest', 'hostess', 'lady', 'woman', 'She', 'wife', 'attendant', 'came', 'arrived', 'was', 'come', 'host', 'visitors', 'had'}, {'hairdresser', 'came', 'stylist', 'arrived', 'turned', 'hairdressers', 'was', 'hairstylist', 'dresser', 'barbershop', 'had', 'barber'}, {'house', 'homeowner', 'housekeeper', 'landlord', 'came', 'owner', 'landlady', 'arrived', 'was', 'turned', 'the', 'landowner', 'had'}, {'housekeeper', 'caretaker', 'hostess', 'housemaid', 'landlord', 'came', 'arrived', 'landlady', 'householder', 'turned', 'was', 'had'}, {'lady', 'specialist', 'had', 'expert', 'came', 'arrived', 'technician', 'turned', 'professional', 'worker', 'hygienist'}, {'supervisor', 'She', 'Inspector', 'Commissioner', 'controller', 'came', 'arrived', 'was', 'turned', 'inspector', 'has', 'had', 'commissioner'}, {'instructor', 'She', 'tutor', 'Teacher', 'leader', 'came', 'arrived', 'teacher', 'was', 'turned', 'come', 'manager', 'had', 'trainer'}, {'showed', 'She', 'Investigators', 'detective', 'investigators', 'came', 'arrived', 'was', 'turned', 'officer', 'investigation', 'investigator', 'had'}, {'caretaker', 'housekeeper', 'concierge', 'janitor', 'hostess', 'cleaner', 'lady', 'woman', 'landlord', 'was', 'arrived', 'turned', 'landlady', 'had', 'cleaners'}, {'She', 'employee', 'labourer', 'had', 'Workers', 'came', 'was', 'arrived', 'come', 'turned', 'workers', 'worker', 'workwoman'}, {'showed', 'She', 'barrister', 'came', 'arrived', 'solicitor', 'turned', 'was', 'lawyer', 'attorney', 'had', 'jurist'}, {'She', 'came', 'arrived', 'librarian', 'turned', 'was', 'Librarian', 'had'}, {'She', 'builder', 'machinist', 'woman', 'came', 'arrived', 'was', 'mechanic', 'operator', 'worker', 'turned', 'fitter', 'had', 'engineer'}, {'showed', 'came', 'arrived', 'come', 'manager', 'was', 'turned', 'had'}, {'came', 'was', 'arrived', 'mechanic', 'turned', 'mechanics', 'had', 'engineer'}, {'She', 'woman', 'wife', 'came', 'arrived', 'driver', 'was', 'turned', 'come', 'motorist', 'had'}, {'carer', 'nurse', 'arrived', 'came', 'turned', 'up', 'nurses', 'had'}, {'dietician', 'dietitian', 'came', 'arrived', 'was', 'turned', 'had', 'nutritionist', 'Nutritionist'}, {'civil', 'She', 'came', 'arrived', 'servant', 'officer', 'turned', 'was', 'official', 'had'}, {'She', 'spectator', 'onlookers', 'bystander', 'viewer', 'audience', 'onlooker', 'came', 'arrived', 'was', 'turned', 'member', 'had'}, {'showed', 'She', 'landlord', 'came', 'owner', 'arrived', 'turned', 'landlady', 'was', 'Owner', 'proprietor', 'had'}, {'She', 'did', 'came', 'arrived', 'was', 'artist', 'turned', 'painter', 'painters', 'had', 'Painter'}, {'lady', 'entry', 'came', 'arrived', 'come', 'She', 'translator', 'paralegaless', 'entrant', 'woman', 'wife', 'had', 'starter', 'paralegal', 'was', 'turned', 'up', 'latecomer', 'dresser'}, {'paramedic', 'paramedics', 'attendant', 'came', 'arrived', 'was', 'turned', 'nurse', 'medic', 'had', 'Paramedics', 'assistant'}, {'She', 'co', 'passenger', 'came', 'arrived', 'driver', 'turned', 'up', 'was', 'had'}, {'showed', 'had', 'came', 'was', 'arrived', 'turned', 'pathologist'}, {'She', 'patient', 'came', 'was', 'arrived', 'turned', 'up', 'had'}, {'She', 'woman', 'came', 'arrived', 'all', 'turned', 'up', 'any', 'drove', 'the', 'pedestrian', 'had', 'Pedestrian'}, {'She', 'had', 'came', 'chemist', 'pharmacy', 'worker', 'turned', 'was', 'arrived', 'apothecary', 'pharmacist'}, {'She', 'had', 'came', 'arrived', 'was', 'physician', 'turned', 'Doctor', 'doctor'}, {'showed', 'She', 'Planner', 'planners', 'came', 'designer', 'was', 'officer', 'turned', 'arrived', 'planner', 'scheduler', 'had'}, {'plumber', 'came', 'arrived', 'was', 'turned', 'had', 'Plumber'}, {'showed', 'She', 'woman', 'wife', 'Practitioner', 'practitioner', 'was', 'came', 'arrived', 'turned', 'come', 'had'}, {'showed', 'She', 'came', 'was', 'arrived', 'turned', 'programmer', 'had', 'coder'}, {'showed', 'demonstrator', 'protester', 'She', 'arrived', 'was', 'turned', 'had', 'protestor'}, {'showed', 'She', 'psychologist', 'came', 'was', 'arrived', 'turned', 'had'}, {'desk', 'receptionist', 'lady', 'came', 'arrived', 'turned', 'Receptionist', 'had'}, {'She', 'house', 'housekeeper', 'woman', 'occupant', 'came', 'arrived', 'was', 'turned', 'resident', 'up', 'inhabitant', 'householder', 'had'}, {'lady', 'came', 'arrived', 'was', 'saleswoman', 'turned', 'had', 'clerk', 'assistant'}, {'showed', 'academic', 'She', 'scientist', 'came', 'arrived', 'researcher', 'scientists', 'turned', 'up', 'was', 'It', 'had'}, {'showed', 'Secretary', 'came', 'arrived', 'turned', 'secretary', 'had', 'clerk'}, {'&apos;s', 'sheriffin', 'deputy', 'woman', 'Sheriff', 'wife', 'came', 'arrived', 'turned', 'sheriff', 'office', 'had'}, {'consultant', 'showed', 'She', 'doctor', 'specialist', 'expert', 'came', 'arrived', 'was', 'turned', 'professional', 'up', 'It', 'had'}, {'She', 'schoolgirl', 'pupil', 'came', 'arrived', 'was', 'turned', 'up', 'student', 'had'}, {'supervisor', 'minder', 'carer', 'attendant', 'came', 'arrived', 'was', 'turned', 'superior', 'manager', 'guardian', 'guard', 'had', 'caregiver', 'childminder', 'boss'}, {'She', 'doctor', 'surgeon', 'came', 'was', 'arrived', 'turned', 'physician', 'surgery', 'had'}, {'seamstress', 'She', 'tailor', 'came', 'tailoress', 'was', 'tailress', 'turned', 'dressmaker', 'had'}, {'was', 'had', 'helmswoman', 'came', 'taxpayer', 'arrived', 'Taxpayers', 'turned', 'were', 'taxpayers', 'taxman', 'taxwoman', 'Taxpayer', 'to', 'payer', 'driver'}, {'She', 'instructor', 'Teacher', 'came', 'arrived', 'teacher', 'was', 'come', 'turned', 'had'}, {'She', 'technicians', 'came', 'technician', 'arrived', 'was', 'turned', 'mechanic', 'had', 'engineer'}, {'girl', 'teen', 'woman', 'came', 'arrived', 'was', 'turned', 'up', 'youngster', 'teenager', 'had', 'youth', 'juvenile'}, {'showed', 'She', 'therapist', 'came', 'arrived', 'turned', 'had'}, {'She', 'came', 'arrived', 'was', 'turned', 'up', 'student', 'had'}, {'showed', 'She', 'vet', 'vets', 'surgeon', 'came', 'arrived', 'was', 'turned', 'nurse', 'veterinarian', 'had'}, {'woman', 'victim', 'wife', 'came', 'arrived', 'turned', 'had'}, {'visitor', 'She', 'guest', 'attendant', 'came', 'was', 'arrived', 'turned', 'visitors', 'had'}, {'showed', 'She', 'witness', 'came', 'arrived', 'turned', 'up', 'appeared', 'Witness', 'had'}, {'She', 'employee', 'had', 'labourer', 'woman', 'staff', 'wife', 'came', 'arrived', 'was', 'come', 'turned', 'up', 'member', 'worker'}, {'She', 'writer', 'came', 'was', 'author', 'arrived', 'turned', 'novelist', 'had'}]\n",
      "103\n",
      "11.281553398058252\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_tercom_alignment(1, 103, 'tok.en_original.en', 'hyp_original_back.txt'))\n",
    "print(count_unique_words_tercom_alignment(2, 103, 'tok.en_disambiguated_male.en', 'hyp_disambiguated_male_back.txt'))\n",
    "print(count_unique_words_tercom_alignment(2, 103, 'tok.en_disambiguated_female.en', 'hyp_disambiguated_female_back.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "f8fe19b8-d41f-4058-a373-048e35d1bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_words\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125d1e1-22bd-401c-aa6e-de29fc5f39a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b475d6e-6cf6-4d0a-acd8-92bdc22c1557",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "ba32e36e-44c1-4d7f-9602-a4583aa5525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_translations(filename_tokenized, filename_translations, filename_out, filename_alignments):\n",
    "    \"\"\"\n",
    "    Match alignment indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_alignments, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    translations = []\n",
    "    with open(filename_translations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                translations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 103\n",
    "    counter = 0\n",
    "    for i in range(0, 103): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 9):\n",
    "                alignments = indices_translation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(translations[counter + f])):\n",
    "                                 words_set.add(translations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 10\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 103:\n",
    "            print(source[count] + ' | ' + str(target_words[count]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "721f40e3-99fa-4f8f-bfa1-a9125ed91fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_translations(filename_tokenized, filename_translations, filename_out, filename_alignments):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_alignments, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    translations = []\n",
    "    with open(filename_translations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                translations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 103\n",
    "    counter = 0\n",
    "    for i in range(0, 103): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 9):\n",
    "                alignments = indices_translation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(translations[counter + f])):\n",
    "                                 words_set.add(translations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 10\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 103:\n",
    "            print(source[count] + ' | ' + str([len(target_set) for target_set in target_words[count]]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "dadbd1b8-92da-4661-bbf7-357a63515b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_word_translations('tok.en_original.en', 'hyp_original.txt', 'translations_words_original.txt', 'original_source-target_en-de_awesome-aligned.txt')\n",
    "count_word_translations('tok.en_original.en', 'hyp_original.txt', 'translations_words_original_occurrence.txt', 'original_source-target_en-de_awesome-aligned.txt')\n",
    "\n",
    "extract_word_translations('tok.en_disambiguated_male.en', 'hyp_disambiguated_male.txt', 'translations_words_disambiguated_male.txt', 'disambiguated_male_source-target_en-de_awesome-aligned.txt')\n",
    "count_word_translations('tok.en_disambiguated_male.en', 'hyp_disambiguated_male.txt', 'translations_words_disambiguated_male_occurrence.txt', 'disambiguated_male_source-target_en-de_awesome-aligned.txt')\n",
    "\n",
    "extract_word_translations('tok.en_disambiguated_female.en', 'hyp_disambiguated_female.txt', 'translations_words_disambiguated_female.txt', 'disambiguated_female_source-target_en-de_awesome-aligned.txt')\n",
    "count_word_translations('tok.en_disambiguated_female.en', 'hyp_disambiguated_female.txt', 'translations_words_disambiguated_female_occurrence.txt', 'disambiguated_female_source-target_en-de_awesome-aligned.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14aa275-bbed-46e9-8fbd-da11f424ce1a",
   "metadata": {},
   "source": [
    "## Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "efbbd367-55cd-4cbc-95c1-9f52966ec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alignment_indices_backtranslation(filename_translations, filename_backtranslations):\n",
    "    \"\"\"\n",
    "    Extract alignment indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_translations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "       \n",
    "    # Match alignement indices from translation to backtranslation\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices_backtranslation = []\n",
    "    with open(filename_backtranslations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for index_list in indices_translation[counter]:\n",
    "                index_matches = []\n",
    "                for index in index_list:\n",
    "                    regex = r\"\" + str(index) + r\"-(\\d)\"\n",
    "                    if re.findall(regex, line): \n",
    "                        index_matches.extend([int(i) for i in re.findall(regex, line)])\n",
    "                    else:\n",
    "                        index_matches.extend([999])\n",
    "                indices_line.append(index_matches)\n",
    "            indices_backtranslation.append(indices_line)\n",
    "            lineNumber += 1 \n",
    "    return indices_backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "c8811a5e-caac-4ca9-ab1f-df546bf83c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_backtranslations(filename_tokenized, filename_backtranslations, filename_out, indices_backtranslation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(filename_backtranslations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                backtranslations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 103\n",
    "    counter = 0\n",
    "    for i in range(0, 103): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 99):\n",
    "                alignments = indices_backtranslation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(backtranslations[counter + f])):\n",
    "                                 words_set.add(backtranslations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 100\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 103:\n",
    "            print(source[count] + ' | ' + str(target_words[count]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "68636e2c-4d46-42de-8e44-5d061468fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_backtranslations(filename_tokenized, filename_backtranslations, filename_out, indices_backtranslation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(filename_backtranslations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                backtranslations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 103\n",
    "    counter = 0\n",
    "    for i in range(0, 103): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 99):\n",
    "                alignments = indices_backtranslation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(backtranslations[counter + f])):\n",
    "                                 words_set.add(backtranslations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 100\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 103:\n",
    "            print(source[count] + ' | ' + str([len(target_set) for target_set in target_words[count]]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "98fd42d2-828f-4757-9267-e5983003ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_original = extract_alignment_indices_backtranslation('original_source-target_en-de_awesome-aligned.txt', 'original_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_original.en', 'hyp_original_back.txt', 'backtranslations_words_original.txt', indices_original)\n",
    "count_word_backtranslations('tok.en_original.en', 'hyp_original_back.txt', 'backtranslations_words_original_occurrence.txt', indices_original)\n",
    "\n",
    "indices_disambiguated_male = extract_alignment_indices_backtranslation('disambiguated_male_source-target_en-de_awesome-aligned.txt', 'disambiguated_male_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_disambiguated_male.en', 'hyp_disambiguated_male_back.txt', 'backtranslations_words_disambiguated_male.txt', indices_disambiguated_male)\n",
    "count_word_backtranslations('tok.en_disambiguated_male.en', 'hyp_disambiguated_male_back.txt', 'backtranslations_words_disambiguated_male_occurrence.txt', indices_disambiguated_male)\n",
    "\n",
    "indices_disambiguated_female = extract_alignment_indices_backtranslation('disambiguated_female_source-target_en-de_awesome-aligned.txt', 'disambiguated_female_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_disambiguated_female.en', 'hyp_disambiguated_female_back.txt', 'backtranslations_words_disambiguated_female.txt', indices_disambiguated_female)\n",
    "count_word_backtranslations('tok.en_disambiguated_female.en', 'hyp_disambiguated_female_back.txt', 'backtranslations_words_disambiguated_female_occurrence.txt', indices_disambiguated_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa9d8d-3fd6-4a95-9591-8af3b42f4b37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gender statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "56c85b2b-de5f-4973-b35f-05e366073d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class GENDER(Enum):\n",
    "    \"\"\"\n",
    "    Enumerate possible genders.\n",
    "    Ignore option resolves to words that should be ignored in particular language\n",
    "    \"\"\"\n",
    "    male = 0\n",
    "    female = 1\n",
    "    neutral = 2\n",
    "    unknown = 3\n",
    "    ignore = 4\n",
    "    \n",
    "# ??? These are not always correct; 'der' could be Dativ or Genitiv for female, 'die' could be plural\n",
    "# !!! There isn't always an article\n",
    "DE_DETERMINERS = {\"der\": GENDER.male, \"ein\": GENDER.male, \"dem\": GENDER.male, \"den\": GENDER.male, \n",
    "                  \"einen\": GENDER.male, \"des\": GENDER.male, \"er\": GENDER.male, \"seiner\": GENDER.male,\n",
    "                  \"ihn\": GENDER.male, \"seinen\": GENDER.male, \"ihm\": GENDER.male, \"ihren\": GENDER.male,\n",
    "                  \"die\": GENDER.female, \"eine\": GENDER.female, \"einer\": GENDER.female, \"seinem\": GENDER.male,\n",
    "                  \"ihrem\": GENDER.male, \"sein\": GENDER.male,\n",
    "                  \"sie\": GENDER.female, \"seine\": GENDER.female, \"ihrer\": GENDER.female, \n",
    "                  \"ihr\": GENDER.neutral, \"ihre\": GENDER.neutral, \"das\": GENDER.neutral,\n",
    "                  \"jemanden\": GENDER.neutral}\n",
    "\n",
    "def get_german_determiners(words):\n",
    "    \"\"\"\n",
    "    Get a list of (gender)\n",
    "    given a list of words.\n",
    "    \"\"\"\n",
    "    determiners = []\n",
    "    for (word_ind, word) in enumerate(words):\n",
    "        word = word.lower()\n",
    "        if word in DE_DETERMINERS:\n",
    "            determiners.append((DE_DETERMINERS[word].name))\n",
    "    return determiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "ebd8cf96-02b7-4594-8b3a-f06047f00ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    }
   ],
   "source": [
    "dets = get_german_determiners([\"dem\"])\n",
    "print(dets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9d63e-efae-414e-95a6-767582b657a3",
   "metadata": {},
   "source": [
    "- Calculate gender based on the articles of unique words: how many of the sentences produce both genders, female and male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "20718016-7517-4085-bf89-45e457eaa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract articles of target tranlsated words\n",
    "def extract_articles(position, sentencesN, translationsIn, alignmentsIn, sourceIn, output):\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "    \n",
    "    # List with original translations\n",
    "    translations_original = []\n",
    "    with open(translationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            translations_original.append(line.strip())\n",
    "\n",
    "\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                indices.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                indices.append([999])\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = set() # set forces uniqueness\n",
    "    for translation in translations_original:\n",
    "        tokens = translation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                translated_ambiguous_words.add(tokens[0]) # extract articles; currently assume index 0 for article position, TODO\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 10 == 0):\n",
    "                translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "                translated_ambiguous_words = set()\n",
    "\n",
    "\n",
    "    #print(len(translations_ambiguous_words))\n",
    "    \n",
    "    # Add results to file\n",
    "\n",
    "    # List with original source sentences\n",
    "    source = []\n",
    "    ambiguous_words = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position])\n",
    "\n",
    "    count = 0  \n",
    "    genders = []\n",
    "    male = []\n",
    "    female = []\n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            #print(translations_ambiguous_words[count])\n",
    "            genders.append(set(get_german_determiners(translations_ambiguous_words[count])))\n",
    "            male.append(\"male\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            female.append(\"female\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(get_german_determiners(translations_ambiguous_words[count])), end='\\n', file=fout)\n",
    "            count += 1\n",
    "            \n",
    "    return (sum(1 for i in genders if ('male' in i and 'female' in i)), \n",
    "            male.count(True), female.count(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "c2dcc149-f38f-419d-b61e-97aa7b657426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 100, 51)\n",
      "(34, 102, 34)\n",
      "(6, 6, 103)\n"
     ]
    }
   ],
   "source": [
    "print(extract_articles(1, 103, 'hyp_original.txt', 'original_source-target_en-de_awesome-aligned.txt', 'tok.en_original.en', 'unique-words_translations_original_articles.txt'))\n",
    "print(extract_articles(2, 103, 'hyp_disambiguated_male.txt', 'disambiguated_male_source-target_en-de_awesome-aligned.txt', 'tok.en_disambiguated_male.en', 'unique-words_translations_disambiguated_male_articles.txt'))\n",
    "print(extract_articles(2, 103, 'hyp_disambiguated_female.txt', 'disambiguated_female_source-target_en-de_awesome-aligned.txt', 'tok.en_disambiguated_female.en', 'unique-words_translations_disambiguated_female_articles.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a57d8-d077-489d-b209-2d0b5147e7de",
   "metadata": {},
   "source": [
    "- Calculate gender in percentage for each sentence: percent of \"male\" vs. female in translations for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "697e9d85-f8ee-443e-b423-00882b38d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract articles of target tranlsated words\n",
    "def extract_articles_percent(position, sentencesN, translationsIn, alignmentsIn, sourceIn, output):\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "    \n",
    "    # List with original translations\n",
    "    translations_original = []\n",
    "    with open(translationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            translations_original.append(line.strip())\n",
    "\n",
    "\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                indices.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                indices.append([999])\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = [] \n",
    "    for translation in translations_original:\n",
    "        tokens = translation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                translated_ambiguous_words.append(tokens[0]) # extract articles; currently assume index 0 for article position, TODO\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 10 == 0):\n",
    "                translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "                translated_ambiguous_words = []\n",
    "\n",
    "\n",
    "    #print(len(translations_ambiguous_words))\n",
    "    \n",
    "    # Add results to file\n",
    "\n",
    "    # List with original source sentences\n",
    "    source = []\n",
    "    ambiguous_words = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position])\n",
    "\n",
    "    count = 0  \n",
    "    genders = []\n",
    "    male = []\n",
    "    female = []\n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            #print(translations_ambiguous_words[count])\n",
    "            genders.append(get_german_determiners(translations_ambiguous_words[count]))\n",
    "            male.append(\"male\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            female.append(\"female\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(get_german_determiners(translations_ambiguous_words[count])), end='\\n', file=fout)\n",
    "            count += 1\n",
    "     \n",
    "    #print(genders)\n",
    "    return (sum([i.count('male')/10 for i in genders])/sentencesN*100, \n",
    "            sum([i.count('female')/10 for i in genders])/sentencesN*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "1d9e30d9-b142-4917-9809-dc649469e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83.20388349514562, 14.368932038834945)\n",
      "(88.83495145631069, 8.349514563106792)\n",
      "(1.1650485436893205, 95.63106796116504)\n"
     ]
    }
   ],
   "source": [
    "print(extract_articles_percent(1, 103, 'hyp_original.txt', 'original_source-target_en-de_awesome-aligned.txt', 'tok.en_original.en', 'unique-words_translations_original_articles.txt'))\n",
    "print(extract_articles_percent(2, 103, 'hyp_disambiguated_male.txt', 'disambiguated_male_source-target_en-de_awesome-aligned.txt', 'tok.en_disambiguated_male.en', 'unique-words_translations_disambiguated_male_articles.txt'))\n",
    "print(extract_articles_percent(2, 103, 'hyp_disambiguated_female.txt', 'disambiguated_female_source-target_en-de_awesome-aligned.txt', 'tok.en_disambiguated_female.en', 'unique-words_translations_disambiguated_female_articles.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd6997-6b45-4a8b-b471-98ddeb058aba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "802b46b4-5cdc-4cc1-83d2-006c7f39bd23",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [642], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntwicklerin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/spacy/__init__.py:30\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depr_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     deprecation_warning(Warnings\u001b[38;5;241m.\u001b[39mW001\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdepr_path))\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/spacy/util.py:159\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    157\u001b[0m data_path \u001b[38;5;241m=\u001b[39m get_data_path()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE049\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath2str(data_path)))\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, basestring_):  \u001b[38;5;66;03m# in data dir / shortcut\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m([d\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data_path\u001b[38;5;241m.\u001b[39miterdir()]):\n",
      "\u001b[0;31mOSError\u001b[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('de')\n",
    "doc = nlp('Entwicklerin')\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)\n",
    "    \n",
    "doc = nlp('Entwickler')\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "3d6a45f8-af8e-4082-8f4d-710c3c9929f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entwickleri\n",
      "entwickl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.cistem import Cistem\n",
    " \n",
    "stemmer = Cistem(case_insensitive=True)\n",
    "s1 = \"Entwicklerin\"\n",
    "print(stemmer.stem(s1))\n",
    "s2 = \"Entwickler\"\n",
    "print(stemmer.stem(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "506bc47f-9628-4de2-8a1a-1ac07ecac2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting german-lemmatizer\n",
      "  Downloading german_lemmatizer-0.1.1-py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: joblib in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from german-lemmatizer) (1.2.0)\n",
      "Collecting docker\n",
      "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from german-lemmatizer) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from docker->german-lemmatizer) (2.28.1)\n",
      "Requirement already satisfied: packaging>=14.0 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from docker->german-lemmatizer) (21.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from docker->german-lemmatizer) (1.26.12)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from docker->german-lemmatizer) (0.58.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from packaging>=14.0->docker->german-lemmatizer) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from requests>=2.26.0->docker->german-lemmatizer) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from requests>=2.26.0->docker->german-lemmatizer) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from requests>=2.26.0->docker->german-lemmatizer) (3.4)\n",
      "Requirement already satisfied: six in /home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages (from websocket-client>=0.32.0->docker->german-lemmatizer) (1.16.0)\n",
      "Installing collected packages: docker, german-lemmatizer\n",
      "Successfully installed docker-6.1.3 german-lemmatizer-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install german-lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "8cf0a2c5-e6c3-4b1e-bbbc-a10ae2046bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object lemmatize at 0x7f229d89be40>"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from german_lemmatizer import lemmatize\n",
    "\n",
    "lemmatize(\n",
    "    ['Johannes war ein guter Schler', 'Sabiene sang zahlreiche Lieder'],\n",
    "    working_dir='*',\n",
    "    chunk_size=10000,\n",
    "    n_jobs=1,\n",
    "    escape=False,\n",
    "    remove_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "cfa8ecf8-a56c-49ef-a38b-8e38557b6571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting HanTa\n",
      "  Downloading HanTa-1.1.1-py3-none-any.whl (15.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: HanTa\n",
      "Successfully installed HanTa-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install HanTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "9944e49a-7ed9-4442-b8d8-fd2edc63776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rztin', 'NN')\n",
      "('Arzt', 'NN')\n"
     ]
    }
   ],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "print(tagger.analyze('rztin'))\n",
    "print(tagger.analyze('Arzt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "5ea9b234-9966-4ab9-89f2-4fde1b24f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob-de\n",
      "  Downloading textblob_de-0.4.3-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting textblob>=0.9.0\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in ./miniconda3/envs/nmt/lib/python3.8/site-packages (from textblob>=0.9.0->textblob-de) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./miniconda3/envs/nmt/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (2022.10.31)\n",
      "Requirement already satisfied: click in ./miniconda3/envs/nmt/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (8.1.3)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/envs/nmt/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (4.64.1)\n",
      "Requirement already satisfied: joblib in ./miniconda3/envs/nmt/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (1.2.0)\n",
      "Installing collected packages: textblob, textblob-de\n",
      "Successfully installed textblob-0.17.1 textblob-de-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "059ef94f-2875-4dff-b0a5-42ccbf395d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/vzhekova/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /home/vzhekova/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/vzhekova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vzhekova/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/vzhekova/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/vzhekova/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "0dc0030d-3341-406b-ab1e-8c39b4048cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rztin']\n",
      "['Arzt']\n"
     ]
    }
   ],
   "source": [
    "from textblob_de import TextBlobDE\n",
    "\n",
    "word = 'rztin'\n",
    "w = TextBlobDE(word)\n",
    "print(w.words.lemmatize())\n",
    "\n",
    "word = 'Arzt'\n",
    "w = TextBlobDE(word)\n",
    "print(w.words.lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc37b18-8d7e-41bb-a1fd-31b3758afec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "b5372913-41cf-440d-8eac-73878487f6a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Language' from 'spacy' (/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/spacy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [638], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy_udpipe\u001b[39;00m\n\u001b[1;32m      3\u001b[0m spacy_udpipe\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# download English model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntwicklerin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/spacy_udpipe/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_from_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUDPipeTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUDPipeModel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, load, load_from_path\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UDPipeTokenizer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mudpipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UDPipeModel\n",
      "File \u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/spacy_udpipe/utils.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Dict\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m blank, Language\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lang_class\n\u001b[1;32m     10\u001b[0m BASE_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3131\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Language' from 'spacy' (/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/spacy/__init__.py)"
     ]
    }
   ],
   "source": [
    "import spacy_udpipe\n",
    "\n",
    "spacy_udpipe.download(\"de\") # download English model\n",
    "\n",
    "text = \"Entwicklerin\"\n",
    "nlp = spacy_udpipe.load(\"de\")\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa3d3c-5905-4fa2-bcd7-7f9425f829ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
