{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7eb9c1b6-7f4d-468a-984f-e7b61fa4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "PATH=\"/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_female\"\n",
    "FASTBPE=\"/home/vzhekova/fastBPE/fast\" # path to the fastBPE tool\n",
    "FAST_ALIGN=\"/home/vzhekova/fast_align/build/fast_align\" # path to the fast_align tool\n",
    "TERCOM = \"/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_male/Perturbation-basedQE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "48aa5a31-2b6f-4744-bd0e-cd820e61740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# check if we can connect to the GPU with PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c199b4a0-7cab-4d4c-b270-e55ef77d010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_female\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf0636bc-6478-4b07-8a97-bd6e694ac4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences\n",
    "!cut -f3 -d'\t' en.txt > en_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c28ebdb9-66ef-4876-a057-1a48e75b10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences containing 'because' and remove the second part of the clause\n",
    "# 330 unique sentences in total\n",
    "with open('en_sentences.txt', 'r') as fin, open('en_original.txt', 'w') as fout:\n",
    "    for line in fin:\n",
    "        sentence = ''\n",
    "        tokens = line.split(\" \")\n",
    "        for token in tokens:\n",
    "            if token == 'because':\n",
    "                print(sentence + '.', end='\\n', file=fout)\n",
    "            sentence = sentence + token.replace(',', '') + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919d7d23-9867-4311-9da8-f64bf8f2e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify gender ambiguous words with gender\n",
    "\n",
    "# List with source words\n",
    "words = set() # set forces uniqueness\n",
    "with open('words.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        words.add(line.strip())\n",
    "        \n",
    "with open('en_original.txt') as in_file, open('en_disambiguated.txt', 'w') as out_file: \n",
    "    for line in in_file:\n",
    "        sentence = line.split(' ')\n",
    "        for token in sentence:\n",
    "            if (token.replace(',', '') in words): # tokens often contain \",\"\n",
    "                token_pos = sentence.index(token)\n",
    "                sentence[token_pos] = \"male \" + token # could also replace with \"female\"\n",
    "        print(' '.join(sentence), end='', file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ec0de-a7d8-4c97-a71f-48fb82d1ac2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Translation English-German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d4f3657-9c41-4db4-b7a7-c9b69e6d5f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "mpn = MosesPunctNormalizer()\n",
    "mt_en = MosesTokenizer(lang='en')\n",
    "md_en = MosesDetokenizer(lang='en')\n",
    "\n",
    "with open('en_original.txt') as fin, open('tok.en_original.en','w') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt_en.tokenize(mpn.normalize(line), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout) \n",
    "        \n",
    "with open('en_disambiguated.txt') as fin, open('tok.en_disambiguated.en','w') as fout:\n",
    "    for line in fin:\n",
    "        tokens = mt_en.tokenize(mpn.normalize(line), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished tokenizing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c50fa559-689b-4781-aad0-d25b6de74274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from bpecodes.en ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from tok.en_original.en ...\n",
      "Read 2733 words (470 unique) from text file.\n",
      "Applying BPE to tok.en_original.en ...\n",
      "Modified 2733 words from text file.\n",
      "Loading codes from bpecodes.en ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from tok.en_disambiguated.en ...\n",
      "Read 3388 words (471 unique) from text file.\n",
      "Applying BPE to tok.en_disambiguated.en ...\n",
      "Modified 3388 words from text file.\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Dividing text into subword units\n",
    "\n",
    "!$FASTBPE applybpe bpe.en_original.en tok.en_original.en bpecodes.en\n",
    "!$FASTBPE applybpe bpe.en_disambiguated.en tok.en_disambiguated.en bpecodes.en\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1ec72fd-81a4-4cd2-93a6-22c47288284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 11:28:37 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_en-de', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict='/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.en.txt', suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='bpe.en_original', tgtdict='/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.de.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-05-09 11:28:37 | INFO | fairseq_cli.preprocess | [en] Dictionary: 42024 types\n",
      "2023-05-09 11:28:38 | INFO | fairseq_cli.preprocess | [en] bpe.en_original.en: 330 sents, 3720 tokens, 0.0% replaced (by <unk>)\n",
      "2023-05-09 11:28:38 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_en-de\n"
     ]
    }
   ],
   "source": [
    "# Binarize text\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --testpref bpe.en_original \\\n",
    "    --only-source \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.en.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.de.txt \\\n",
    "    --destdir data-bin_original_en-de \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1615792f-24aa-40b5-958a-10790d5481b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 11:29:02 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_disambiguated_en-de', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict='/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.en.txt', suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='bpe.en_disambiguated', tgtdict='/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.de.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-05-09 11:29:02 | INFO | fairseq_cli.preprocess | [en] Dictionary: 42024 types\n",
      "2023-05-09 11:29:02 | INFO | fairseq_cli.preprocess | [en] bpe.en_disambiguated.en: 330 sents, 4375 tokens, 0.0% replaced (by <unk>)\n",
      "2023-05-09 11:29:02 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_disambiguated_en-de\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --testpref bpe.en_disambiguated \\\n",
    "    --only-source \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.en.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/dict.de.txt \\\n",
    "    --destdir data-bin_disambiguated_en-de \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0534e037-1503-462b-aacc-bed90ec0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "NBEST = 100\n",
    "BEAM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1df524a1-a957-45a8-ad21-a362d7abd060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 15:19:46 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-18 15:19:46 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-18 15:19:46 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-18 15:19:46 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-07-18 15:20:33 | INFO | fairseq.data.data_utils | loaded 330 examples from: data-bin_original_en-de/test.en-de.en\n",
      "2023-07-18 15:20:33 | INFO | fairseq.tasks.translation | data-bin_original_en-de test en-de 330 examples\n",
      "2023-07-18 15:23:29 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-18 15:23:29 | INFO | fairseq_cli.generate | Translated 330 sentences (4,122 tokens) in 119.5s (2.76 sentences/s, 34.48 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# Generate N hypothesis\n",
    "!fairseq-generate data-bin_original_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_en-de.decode_Beam_100.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "13e4522e-8427-4fa0-904c-6b7edc65c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 19:54:42 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_disambiguated_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-18 19:54:42 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-18 19:54:42 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-18 19:54:42 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-07-18 19:55:21 | INFO | fairseq.data.data_utils | loaded 330 examples from: data-bin_disambiguated_en-de/test.en-de.en\n",
      "2023-07-18 19:55:21 | INFO | fairseq.tasks.translation | data-bin_disambiguated_en-de test en-de 330 examples\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/bin/fairseq-generate\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 413, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 50, in main\n",
      "    return _main(cfg, sys.stdout)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 201, in _main\n",
      "    hypos = task.inference_step(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/tasks/fairseq_task.py\", line 542, in inference_step\n",
      "    return generator.generate(\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/vzhekova/fairseq/fairseq/sequence_generator.py\", line 204, in generate\n",
      "    return self._generate(sample, **kwargs)\n",
      "  File \"/home/vzhekova/fairseq/fairseq/sequence_generator.py\", line 360, in _generate\n",
      "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/sequence_generator.py\", line 868, in forward_decoder\n",
      "    avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 10.92 GiB total capacity; 6.61 GiB already allocated; 82.38 MiB free; 7.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Generate N hypothesis\n",
    "!fairseq-generate data-bin_disambiguated_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > disambiguated_en-de.decode_Beam_100.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425c7dc-f10a-4b6f-81e5-f7d89f14d91f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Backtranslation German-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6efb5c4a-0c37-4a6f-bd00-0aaea4da69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LC_ALL=C sort -V' sorts the results in natural order \n",
    "!grep ^H original_en-de.decode_Beam_100.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_100.txt\n",
    "!grep ^H disambiguated_en-de.decode_Beam_100.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_disambiguated_100.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1b245127-a93d-45e3-b3a7-b9791d8b4189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original_100.txt ...\n",
      "Read 277261 words (3716 unique) from text file.\n",
      "Applying BPE to hyp_original_100.txt ...\n",
      "Modified 277261 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_disambiguated_100.txt ...\n",
      "Read 281702 words (3780 unique) from text file.\n",
      "Applying BPE to hyp_disambiguated_100.txt ...\n",
      "Modified 281702 words from text file.\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Dividing tokenized text into subword units\n",
    "\n",
    "!$FASTBPE applybpe bpe.hyp_original_100.de hyp_original_100.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_disambiguated_100.de hyp_disambiguated_100.txt bpecodes.de\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9dd3cf73-9c96-49df-b749-f2bbe337df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 15:45:33 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_de-en_100', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original_100', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-07-18 15:45:33 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-07-18 15:45:35 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original_100.de: 33000 sents, 405031 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-18 15:45:35 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_de-en_100\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original_100 \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_de-en_100 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "702ebc86-b0d5-4d1d-9436-41934e67ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 15:45:39 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_disambiguated_de-en_100', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_disambiguated_100', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-07-18 15:45:39 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-07-18 15:45:40 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_disambiguated_100.de: 33000 sents, 438532 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-18 15:45:40 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_disambiguated_de-en_100\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_disambiguated_100 \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_disambiguated_de-en_100 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "852891a2-e6c1-4300-9000-cd8f2a4ad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=\"/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble\"\n",
    "NBEST = 100\n",
    "BEAM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "07103d7f-cbc3-456a-acaa-1618cdcf70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 15:46:37 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_de-en_100', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-18 15:46:38 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-18 15:46:38 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-18 15:46:38 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-07-18 15:47:22 | INFO | fairseq.data.data_utils | loaded 33,000 examples from: data-bin_original_de-en_100/test.de-en.de\n",
      "2023-07-18 15:47:22 | INFO | fairseq.tasks.translation | data-bin_original_de-en_100 test de-en 33000 examples\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/bin/fairseq-generate\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 413, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 50, in main\n",
      "    return _main(cfg, sys.stdout)\n",
      "  File \"/home/vzhekova/fairseq/fairseq_cli/generate.py\", line 201, in _main\n",
      "    hypos = task.inference_step(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/tasks/fairseq_task.py\", line 542, in inference_step\n",
      "    return generator.generate(\n",
      "  File \"/home/vzhekova/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/vzhekova/fairseq/fairseq/sequence_generator.py\", line 204, in generate\n",
      "    return self._generate(sample, **kwargs)\n",
      "  File \"/home/vzhekova/fairseq/fairseq/sequence_generator.py\", line 360, in _generate\n",
      "    lprobs, avg_attn_scores = self.model.forward_decoder(\n",
      "  File \"/home/vzhekova/fairseq/fairseq/sequence_generator.py\", line 868, in forward_decoder\n",
      "    avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 10.92 GiB total capacity; 6.65 GiB already allocated; 60.38 MiB free; 7.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Generate N hypothesis\n",
    "!fairseq-generate data-bin_original_de-en_100  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_de-en.decode_Beam_100_backtranslation.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c08bb193-d831-474f-9ae3-7f3956f65574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-18 20:00:15 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 4, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 4, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_disambiguated_de-en_100', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-18 20:00:15 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-07-18 20:00:15 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-07-18 20:00:15 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-07-18 20:00:53 | INFO | fairseq.data.data_utils | loaded 33,000 examples from: data-bin_disambiguated_de-en_100/test.de-en.de\n",
      "2023-07-18 20:00:53 | INFO | fairseq.tasks.translation | data-bin_disambiguated_de-en_100 test de-en 33000 examples\n",
      "2023-07-19 00:17:17 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-19 00:17:17 | INFO | fairseq_cli.generate | Translated 33,000 sentences (373,201 tokens) in 11458.5s (2.88 sentences/s, 32.57 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# Generate N hypothesis\n",
    "!fairseq-generate data-bin_disambiguated_de-en_100  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 4 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > disambiguated_de-en.decode_Beam_100_backtranslation.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cd64bb5b-f01d-43e6-9b52-ca4a000040c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LC_ALL=C sort -V' sorts the results in natural order \n",
    "!grep ^H original_de-en.decode_Beam_100_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_back_100.txt\n",
    "!grep ^H disambiguated_de-en.decode_Beam_100_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_disambiguated_back_100.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d1cc9b77-86fc-4604-9d6c-6c6a3eea94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished detokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize text        \n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "md_en = MosesDetokenizer(lang='en')\n",
    "\n",
    "with open('hyp_original_back_100.txt', encoding='utf8') as fin, open('original_back_100.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_disambiguated_back_100.txt', encoding='utf8') as fin, open('disambiguated_back_100.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished detokenizing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a4d60-80fb-4ce1-b305-4dcaa5ddbbb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics on translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dab7a65c-57d3-4bd9-883c-b6166d8dc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "330\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source = []\n",
    "with open('en_original.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "# List with disambiguated source sentences\n",
    "source_disambiguated = []\n",
    "with open('en_disambiguated.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_disambiguated.append(line.strip())\n",
    "    \n",
    "# List with nbest sentences for every source in original\n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "# List with nbest sentences for every source in disambiguated            \n",
    "nbest_disambiguated = []\n",
    "with open('hyp_disambiguated_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_disambiguated.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "print(len(source))\n",
    "print(len(nbest_original))\n",
    "print(len(nbest_disambiguated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce374900-43dd-484f-bd18-8f8c05e524d4",
   "metadata": {},
   "source": [
    "## Count unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e76e4e7a-74ad-405a-8fac-1bfd927999ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.7090909090909\n"
     ]
    }
   ],
   "source": [
    "# Count unique sentences in source nbest list for each source sentence of original; 9.945454545454545\n",
    "# Value should be 10, because beam search generates 10 unique sentences\n",
    "unique_sent = []\n",
    "for source_nbest in nbest_original:\n",
    "    num_values = len(set(source_nbest))\n",
    "    #print(num_values)\n",
    "    unique_sent.append(num_values)\n",
    "    \n",
    "#print(unique_sent)\n",
    "print(sum(unique_sent)/330) # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8c5cf0e5-b2e7-4ae6-9d2c-6bc0aa82fcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0060606060606\n"
     ]
    }
   ],
   "source": [
    "# Count unique sentences in source nbest list for each source sentence of modified; 9.954545454545455\n",
    "unique_sent = []\n",
    "for source_nbest in nbest_disambiguated:\n",
    "    num_values = len(set(source_nbest))\n",
    "    #print(num_values)\n",
    "    unique_sent.append(num_values)\n",
    "    \n",
    "#print(unique_sent)\n",
    "print(sum(unique_sent)/330) # average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c0c73-c1d3-4ca9-9f3f-151c0e86f17f",
   "metadata": {},
   "source": [
    "## Count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63153491-34b4-454f-84d0-786aef4793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words in source nbest list for each source sentence of original; 16.836363636363636\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "unique_words = []\n",
    "counter = 0\n",
    "for source_nbest in nbest_original:\n",
    "    words = set()\n",
    "    for sent in source_nbest:\n",
    "        tokens = sp(sent)\n",
    "        for token in tokens:\n",
    "            if token.text not in stopwords:    # checking whether the word is a stop word\n",
    "                words.add(token.text)\n",
    "    num_values = len(words)\n",
    "    unique_words.append(num_values)\n",
    "    \n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "#print(unique_words)\n",
    "print(sum(unique_words)/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3cf13-db7e-4ab4-8476-381617f88df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words in source nbest list for each source sentence of modified; 17.64848484848485\n",
    "# !!! This is normal to generate more unique words, because the disambiguated sentences have more words in total\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "unique_words = []\n",
    "counter = 0\n",
    "for source_nbest in nbest_disambiguated:\n",
    "    words = set()\n",
    "    for sent in source_nbest:\n",
    "        tokens = sp(sent)\n",
    "        for token in tokens:\n",
    "            if token.text not in stopwords:    # checking whether the word is a stop word\n",
    "                words.add(token.text)\n",
    "    num_values = len(words)\n",
    "    unique_words.append(num_values)\n",
    "    \n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "#print(unique_words)\n",
    "print(sum(unique_words)/330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95394217-96d3-4ea1-aa9b-65cec09c3be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics on backtranslations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8b8b4177-38a5-4294-8c2a-619c63fab031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source_original = []\n",
    "with open('en_original.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_original.append(line.strip())\n",
    "        \n",
    "# List with disambiguated source sentences\n",
    "source_disambiguated = []\n",
    "with open('en_disambiguated.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_disambiguated.append(line.strip())\n",
    "    \n",
    "# List with nbest sentences for every source in original \n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "# List with nbest sentences for every source in disambiguated\n",
    "nbest_disambiguated = []\n",
    "with open('disambiguated_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_disambiguated.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "print(len(nbest_original))\n",
    "print(len(nbest_disambiguated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f66cfb-d1bc-4b83-8f19-8ab4b21303db",
   "metadata": {},
   "source": [
    "## Source sentence reoccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ae8bfa24-2883-4d8c-9ce4-d13eb7ef02a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.53333333333333\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "# Count how many times the source sentence occurs in the nbest list of original; 258\n",
    "results = []\n",
    "counter = 0\n",
    "for sent in source_original:\n",
    "    matches = 0\n",
    "    for target in nbest_original[counter]: \n",
    "        if (sent == target):\n",
    "            matches += 1\n",
    "    results.append(matches)  \n",
    "    counter += 1\n",
    "    \n",
    "print(sum(results)/330)\n",
    "print(sum(x > 0 for x in results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b11b20be-122f-4a42-9c4f-e38242f831fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.163636363636364\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "# Count how many times the source sentence occurs in the nbest list of disambiguated; 230\n",
    "results = []\n",
    "counter = 0\n",
    "for sent in source_disambiguated:\n",
    "    matches = 0\n",
    "    for target in nbest_disambiguated[counter]: \n",
    "        if (sent == target):\n",
    "            matches += 1\n",
    "    results.append(matches)  \n",
    "    counter += 1\n",
    "    \n",
    "print(sum(results)/330)\n",
    "print(sum(x > 0 for x in results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecb949-c44c-42bd-8076-31893262c777",
   "metadata": {},
   "source": [
    "## Ambiguous source words reoccurrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "76e85e14-a11e-4eb6-95d5-aa76aedc593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['developer', 'mechanic', 'mover', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'mover', 'farmer', 'CEO', 'hairdresser', 'developer', 'driver', 'auditor', 'CEO', 'guard', 'assistant', 'assistant', 'auditor', 'salesperson', 'manager', 'physician', 'laborer', 'physician', 'hairdresser', 'developer', 'farmer', 'receptionist', 'manager', 'cleaner', 'mechanic', 'writer', 'worker', 'editor', 'analyst', 'carpenter', 'cook', 'carpenter', 'cleaner', 'laborer', 'mechanic', 'mechanic', 'cook', 'farmer', 'CEO', 'librarian', 'chief', 'developer', 'nurse', 'lawyer', 'developer', 'mover', 'mover', 'worker', 'secretary', 'CEO', 'carpenter', 'sheriff', 'mechanic', 'analyst', 'assistant', 'chief', 'janitor', 'manager', 'supervisor', 'chief', 'worker', 'salesperson', 'lawyer', 'developer', 'sheriff', 'janitor', 'laborer', 'driver', 'mover', 'developer', 'janitor', 'salesperson', 'chief', 'laborer', 'guard', 'nurse', 'worker', 'laborer', 'lawyer', 'CEO', 'laborer', 'laborer', 'nurse', 'manager', 'guard', 'developer', 'driver', 'manager', 'farmer', 'analyst', 'supervisor', 'laborer', 'carpenter', 'worker', 'manager', 'farmer', 'CEO', 'salesperson', 'driver', 'worker', 'supervisor', 'lawyer', 'analyst', 'supervisor', 'worker', 'CEO', 'CEO', 'salesperson', 'driver', 'farmer', 'guard', 'CEO', 'physician', 'manager', 'analyst', 'mover', 'CEO', 'laborer', 'farmer', 'sheriff', 'analyst', 'guard', 'mechanic', 'carpenter', 'sheriff', 'manager', 'sheriff', 'cook', 'mover', 'analyst', 'sheriff', 'salesperson', 'CEO', 'janitor', 'supervisor', 'developer', 'guard', 'driver', 'driver', 'chief', 'physician', 'sheriff', 'mechanic', 'developer', 'physician', 'mechanic', 'mover', 'chief', 'physician', 'farmer', 'salesperson', 'janitor', 'chief', 'lawyer', 'laborer', 'cook', 'analyst', 'guard', 'lawyer', 'CEO', 'carpenter', 'lawyer', 'manager', 'mover', 'farmer', 'farmer', 'guard', 'mover', 'guard', 'analyst', 'cook', 'driver', 'CEO', 'driver', 'salesperson', 'cook', 'lawyer', 'CEO', 'mechanic', 'physician', 'carpenter', 'supervisor', 'janitor', 'lawyer', 'worker', 'attendant', 'carpenter', 'carpenter', 'physician', 'sheriff', 'janitor', 'janitor', 'salesperson', 'counselor', 'secretary', 'supervisor', 'chief', 'guard', 'sheriff', 'mechanic', 'cleaner', 'sheriff', 'cleaner', 'baker', 'developer', 'clerk', 'worker', 'receptionist', 'salesperson', 'receptionist', 'accountant', 'manager', 'cook', 'supervisor', 'chief', 'supervisor', 'secretary', 'lawyer', 'auditor', 'analyst', 'guard', 'laborer', 'analyst', 'receptionist', 'receptionist', 'supervisor', 'clerk', 'chief', 'analyst', 'worker', 'guard', 'guard', 'developer', 'manager', 'mechanic', 'supervisor', 'housekeeper', 'supervisor', 'carpenter', 'mechanic', 'farmer', 'farmer', 'janitor', 'cook', 'driver', 'mechanic', 'mechanic', 'mover', 'carpenter', 'worker', 'supervisor', 'chief', 'janitor', 'analyst', 'sheriff', 'janitor', 'mover', 'developer', 'mechanic', 'editor', 'counselor', 'tailor', 'farmer', 'supervisor', 'manager', 'mover', 'laborer', 'housekeeper', 'baker', 'attendant', 'writer', 'cook', 'analyst', 'physician', 'driver', 'mover', 'driver', 'developer', 'chief', 'physician', 'janitor', 'chief', 'laborer', 'sheriff', 'chief', 'cook', 'salesperson', 'lawyer', 'farmer', 'cleaner', 'cook', 'laborer', 'cleaner', 'customer', 'manager', 'teenager', 'therapist', 'librarian', 'librarian', 'advisor', 'advisor', 'customer', 'bartender', 'patient', 'specialist', 'practitioner', 'practitioner', 'examiner', 'examiner', 'hairdresser', 'hairdresser', 'programmer', 'programmer', 'undergraduate', 'scientist', 'dietitian', 'dietitian', 'painter', 'painter', 'broker', 'broker', 'firefighter', 'firefighter']\n",
      "330\n",
      "330\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with source words\n",
    "words = set() # set forces uniqueness\n",
    "with open('words.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        words.add(line.strip())\n",
    "\n",
    "# Extract ambiguous words from source sentences\n",
    "ambiguous_words = [] \n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "        \n",
    "print(ambiguous_words)\n",
    "print(len(ambiguous_words))\n",
    "        \n",
    "# List with nbest sentences for every source\n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "\n",
    "nbest_disambiguated = []\n",
    "with open('disambiguated_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_disambiguated.append(temp)\n",
    "            counter = 0\n",
    "            temp = []  \n",
    "\n",
    "print(len(nbest_original))\n",
    "print(len(nbest_disambiguated))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3346913e-ed3c-4444-b0a9-2cc0a4a24817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5754, 9274, 1157, 6472, 687, 297, 4629, 7914, 4147, 7704, 1648, 4469, 3891, 9307, 4415, 4144, 5480, 6244, 6885, 4117, 781, 4571, 425, 5, 313, 3668, 7336, 6224, 6585, 7174, 4606, 8534, 3760, 6492, 6158, 8681, 6224, 7980, 6937, 6243, 45, 8425, 7240, 6431, 7917, 1958, 9635, 1267, 7354, 7277, 6396, 4080, 246, 953, 6324, 8810, 2097, 7655, 9023, 8321, 8888, 4752, 2331, 3312, 7746, 2278, 1378, 3770, 754, 5904, 7815, 9161, 2477, 53, 9594, 5643, 3788, 3886, 444, 1308, 252, 5038, 6127, 5720, 69, 6898, 893, 169, 202, 5801, 8957, 7345, 3235, 9515, 9999, 9327, 9472, 7716, 40, 7410, 4621, 6812, 9442, 3483, 425, 9543, 7882, 4848, 5778, 9880, 3923, 4189, 3219, 1729, 427, 9319, 6572, 3148, 1673, 776, 6685, 7577, 2528, 1869, 193, 9250, 8603, 8233, 7823, 9160, 6485, 8978, 8924, 8480, 4215, 1225, 8402, 8907, 922, 3573, 4243, 4543, 7195, 5955, 9597, 9964, 294, 845, 8920, 6952, 5207, 530, 9294, 446, 1376, 703, 7880, 511, 3106, 928, 5219, 145, 6053, 7529, 3423, 5417, 1397, 7649, 5588, 6328, 1018, 6595, 6382, 8147, 1234, 4126, 9451, 5066, 9650, 2009, 9111, 835, 6308, 4695, 3030, 8219, 477, 7497, 2745, 2555, 6832, 5520, 1325, 7149, 7389, 395, 8656, 2849, 2462, 413, 912, 6138, 3776, 1895, 6019, 9550, 6688, 5911, 8280, 6722, 8641, 2509, 1986, 6259, 8153, 523, 8966, 6601, 9294, 7091, 2047, 891, 2831, 7914, 6410, 4198, 7737, 6831, 133, 9158, 8730, 9392, 6355, 1538, 1239, 8616, 4823, 2998, 3585, 3507, 6619, 8787, 3449, 5641, 3394, 6611, 7335, 7227, 7035, 2304, 6748, 9229, 9558, 8318, 2789, 6459, 5163, 2571, 552, 2556, 8419, 9606, 3337, 1589, 4626, 6979, 4693, 184, 6337, 7590, 3920, 4779, 1030, 304, 8168, 5480, 829, 3516, 6190, 8258, 679, 9576, 1281, 9813, 4366, 5179, 326, 3815, 1527, 29, 9412, 2264, 6045, 535, 6395, 6952, 4458, 5765, 153, 5526, 5748, 1641, 4493, 1332, 9560, 9769, 797, 610, 7293, 3042, 8635, 932, 4072, 9203, 2624, 7161, 6057, 6064, 8626, 8845, 256, 116, 3318, 3663, 7047, 7120, 4772, 2962, 472, 201]\n",
      "4984.557575757576\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# Count how many times the source words occurs in the nbest list of original\n",
    "results = []\n",
    "counter = 0\n",
    "for word in ambiguous_words:\n",
    "    matches = 0\n",
    "    for target in nbest_original[counter]: \n",
    "        if (word in target.split(\" \")):\n",
    "            matches += 1\n",
    "    results.append(matches)  \n",
    "    counter += 1\n",
    "    \n",
    "print(results)\n",
    "print(sum(results)/330)\n",
    "print(sum(x > 0 for x in results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "94d3d860-88c4-4a08-a0ca-f8d5cca165eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4488, 9337, 145, 8322, 524, 401, 4177, 8910, 2478, 3798, 778, 5041, 3286, 6942, 3088, 2325, 6125, 5664, 6328, 2586, 402, 4754, 356, 2, 322, 4485, 4527, 2575, 7164, 6352, 5223, 8708, 3369, 6842, 6325, 7855, 5658, 8753, 5585, 6070, 16, 8351, 6934, 8081, 4689, 1180, 9614, 898, 6202, 8039, 5696, 2725, 22, 0, 6759, 8994, 1011, 6470, 6094, 8278, 8209, 5734, 1527, 1936, 7414, 2079, 519, 4528, 585, 5639, 6425, 7583, 1227, 13, 8935, 72, 4350, 1790, 325, 831, 177, 5773, 6540, 7012, 8, 6460, 338, 96, 133, 6395, 7955, 5646, 5201, 8229, 9876, 4515, 9360, 7112, 5, 5528, 4972, 7676, 6175, 2569, 278, 8267, 7433, 4551, 5586, 9322, 2994, 4273, 2449, 423, 290, 7429, 3148, 3314, 723, 789, 5483, 6360, 52, 465, 127, 4770, 5000, 7523, 7285, 9104, 5574, 5492, 8362, 5290, 6810, 29, 7264, 7067, 791, 1343, 3146, 6366, 6419, 5776, 8604, 9208, 198, 768, 5091, 7503, 5262, 565, 9483, 0, 1005, 761, 4625, 442, 1608, 740, 5020, 93, 7981, 6868, 3370, 5016, 1059, 5281, 5322, 5874, 7, 3832, 3413, 6625, 56, 4553, 8843, 7100, 8167, 1136, 7581, 739, 8086, 4530, 1552, 8351, 341, 5663, 2403, 1972, 6426, 5979, 1435, 5691, 6011, 376, 6331, 1782, 1194, 272, 892, 5847, 3255, 1173, 6958, 8802, 6712, 5555, 4492, 6553, 7424, 2279, 2247, 6652, 8440, 458, 9134, 7398, 8498, 8449, 1548, 648, 2609, 8061, 6297, 3638, 6603, 6728, 65, 9086, 8284, 8919, 5924, 1313, 1078, 7995, 5319, 3700, 3719, 3075, 8458, 9131, 2981, 8674, 2676, 4363, 7110, 3710, 3712, 1411, 8472, 8613, 9917, 8148, 659, 5209, 5773, 2731, 412, 1829, 7607, 8900, 2607, 139, 3458, 8051, 6098, 484, 3711, 2894, 2599, 4439, 345, 208, 7796, 3825, 1018, 4069, 8418, 7096, 698, 7617, 180, 8848, 3140, 3054, 320, 4085, 517, 0, 8171, 1610, 8431, 514, 6066, 2948, 4630, 7558, 151, 5492, 6394, 1711, 2920, 1355, 9902, 9794, 733, 426, 7884, 3851, 7960, 911, 3503, 8844, 3090, 4576, 5771, 6178, 8700, 8535, 161, 115, 3374, 3743, 5939, 5644, 3258, 2090, 2518, 3418]\n",
      "4448.066666666667\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "# Count how many times the source words occurs in the nbest list of disambiguated\n",
    "results = []\n",
    "counter = 0\n",
    "for word in ambiguous_words:\n",
    "    matches = 0\n",
    "    for target in nbest_disambiguated[counter]: \n",
    "        if (word in target.split(\" \")):\n",
    "            matches += 1\n",
    "    results.append(matches)  \n",
    "    counter += 1\n",
    "    \n",
    "print(results)\n",
    "print(sum(results)/330)\n",
    "print(sum(x > 0 for x in results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628352f6-7bc2-4f3e-b4c9-72eddb35be6b",
   "metadata": {},
   "source": [
    "## Count unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e6780900-1cee-48f6-bd26-7046ef6b1076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944.051515151515\n"
     ]
    }
   ],
   "source": [
    "# Count unique sentences in source nbest list for each source sentence of original; 46.06060606060606\n",
    "unique_sent = []\n",
    "for source_nbest in nbest_original:\n",
    "    num_values = len(set(source_nbest))\n",
    "    #print(num_values)\n",
    "    unique_sent.append(num_values)\n",
    "    \n",
    "#print(unique_sent)\n",
    "print(sum(unique_sent)/330) # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cbb3f87c-8a26-4e5b-afce-5014324f8ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162.3121212121214\n"
     ]
    }
   ],
   "source": [
    "# Count unique sentences in source nbest list for each source sentence of modified; 51.77272727272727\n",
    "unique_sent = []\n",
    "for source_nbest in nbest_disambiguated:\n",
    "    num_values = len(set(source_nbest))\n",
    "    #print(num_values)\n",
    "    unique_sent.append(num_values)\n",
    "    \n",
    "#print(unique_sent)\n",
    "print(sum(unique_sent)/330) # average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265064a1-379a-4b66-ad78-61dcd8735726",
   "metadata": {},
   "source": [
    "## Count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e7ef2-1c50-489a-b011-8c2b530d9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words in source nbest list for each source sentence of original; 22.593939393939394\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "unique_words = []\n",
    "counter = 0\n",
    "for source_nbest in nbest_original:\n",
    "    words = set()\n",
    "    for sent in source_nbest:\n",
    "        tokens = sp(sent)\n",
    "        for token in tokens:\n",
    "            if token.text not in stopwords:    # checking whether the word is a stop word\n",
    "                words.add(token.text)\n",
    "    num_values = len(words)\n",
    "    unique_words.append(num_values)\n",
    "    \n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "#print(unique_words)\n",
    "print(sum(unique_words)/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67691641-9d4a-4390-acb0-6cb693f3226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words in source nbest list for each source sentence of modified; 22.348484848484848\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "unique_words = []\n",
    "counter = 0\n",
    "for source_nbest in nbest_disambiguated:\n",
    "    words = set()\n",
    "    for sent in source_nbest:\n",
    "        tokens = sp(sent)\n",
    "        for token in tokens:\n",
    "            if token.text not in stopwords:    # checking whether the word is a stop word\n",
    "                words.add(token.text)\n",
    "    num_values = len(words)\n",
    "    unique_words.append(num_values)\n",
    "    \n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "#print(unique_words)\n",
    "print(sum(unique_words)/330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a31ec1-6057-4efc-9cb5-0958d64c2bc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word alignement (source-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9123d6e-caef-4b93-ae43-53cbde83624c",
   "metadata": {},
   "source": [
    "- Count how many unique ambiguous words are in total in source text\n",
    "- Extract the position of the first ambiguous word from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "85f8a95c-7ac7-4b7b-ad4b-46b336c11aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'editor', 'assistant', 'patient', 'tailor', 'mechanic', 'firefighter', 'mover', 'laborer', 'developer', 'salesperson', 'janitor', 'cleaner', 'teenager', 'therapist', 'CEO', 'manager', 'clerk', 'chief', 'bartender', 'secretary', 'undergraduate', 'housekeeper', 'driver', 'examiner', 'specialist', 'programmer', 'dietitian', 'accountant', 'guard', 'advisor', 'broker', 'receptionist', 'sheriff', 'farmer', 'customer', 'baker', 'scientist', 'auditor', 'carpenter', 'supervisor', 'painter', 'librarian', 'writer', 'cook', 'nurse', 'attendant', 'physician', 'counselor', 'analyst', 'practitioner', 'lawyer', 'hairdresser', 'worker'}\n",
      "53\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 6, 1, 4, 1, 1, 1, 1, 1, 3, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with source words\n",
    "words = set() # set forces uniqueness\n",
    "with open('words.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        words.add(line.strip())\n",
    "        \n",
    "ambiguous_words = set() # set forces uniqueness\n",
    "positions_ambiguous_words = []\n",
    "\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.add(token)\n",
    "                position = tokens.index(token)\n",
    "                positions_ambiguous_words.append(position)\n",
    "                break\n",
    "        \n",
    "print(ambiguous_words)\n",
    "print(len(ambiguous_words))\n",
    "print(positions_ambiguous_words)\n",
    "print(len(positions_ambiguous_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f2bbb-6aa4-47fa-89bd-3855b164c388",
   "metadata": {},
   "source": [
    "- Input to fast_align must be tokenized and aligned into parallel sentences. \n",
    "- Line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space (|||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f69765e4-f622-46fa-b68b-06fb80f11f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "         \n",
    "# List with nbest sentences for every source in original \n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "\n",
    "print(len(source))\n",
    "print(len(nbest_original))           \n",
    "        \n",
    "count = 0\n",
    "with open('original_100_source-target_en-de.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        for hyp in nbest_original[count]:\n",
    "            print(source[count] + ' ||| ' + hyp, end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "58ac58b0-41db-444a-a187-dc4266a09551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with disambiguated source sentences\n",
    "source = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "         \n",
    "# List with nbest sentences for every source in original \n",
    "nbest_disambiguated = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_disambiguated_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_disambiguated.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "\n",
    "print(len(source))\n",
    "print(len(nbest_disambiguated))           \n",
    "        \n",
    "count = 0\n",
    "with open('disambiguated_100_source-target_en-de.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        for hyp in nbest_disambiguated[count]:\n",
    "            print(source[count] + ' ||| ' + hyp, end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e0a48-e74f-468a-b949-a9cc13a2a519",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fast_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c1477d44-1efd-4dda-8549-8b29c438e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 1.02135\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.74575e+06\n",
      "  log_2 likelihood: -8.28937e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.18938\n",
      "       size counts: 95\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -946236\n",
      "  log_2 likelihood: -1.36513e+06\n",
      "     cross entropy: 4.92363\n",
      "        perplexity: 30.3501\n",
      "      posterior p0: 0.0237896\n",
      " posterior al-feat: -0.153666\n",
      "       size counts: 95\n",
      "  1  model al-feat: -0.164251 (tension=4)\n",
      "  2  model al-feat: -0.158913 (tension=4.2117)\n",
      "  3  model al-feat: -0.156355 (tension=4.31663)\n",
      "  4  model al-feat: -0.155067 (tension=4.37042)\n",
      "  5  model al-feat: -0.154402 (tension=4.39843)\n",
      "  6  model al-feat: -0.154054 (tension=4.41314)\n",
      "  7  model al-feat: -0.153871 (tension=4.4209)\n",
      "  8  model al-feat: -0.153774 (tension=4.42499)\n",
      "     final tension: 4.42716\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -784500\n",
      "  log_2 likelihood: -1.13179e+06\n",
      "     cross entropy: 4.08205\n",
      "        perplexity: 16.9364\n",
      "      posterior p0: 0.0256581\n",
      " posterior al-feat: -0.142297\n",
      "       size counts: 95\n",
      "  1  model al-feat: -0.153723 (tension=4.42716)\n",
      "  2  model al-feat: -0.148477 (tension=4.65569)\n",
      "  3  model al-feat: -0.145746 (tension=4.77929)\n",
      "  4  model al-feat: -0.144254 (tension=4.84827)\n",
      "  5  model al-feat: -0.143417 (tension=4.88741)\n",
      "  6  model al-feat: -0.142941 (tension=4.9098)\n",
      "  7  model al-feat: -0.142669 (tension=4.92268)\n",
      "  8  model al-feat: -0.142512 (tension=4.93012)\n",
      "     final tension: 4.93441\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -735435\n",
      "  log_2 likelihood: -1.06101e+06\n",
      "     cross entropy: 3.82675\n",
      "        perplexity: 14.1895\n",
      "      posterior p0: 0.0267819\n",
      " posterior al-feat: -0.138866\n",
      "       size counts: 95\n",
      "  1  model al-feat: -0.142421 (tension=4.93441)\n",
      "  2  model al-feat: -0.140934 (tension=5.00551)\n",
      "  3  model al-feat: -0.14008 (tension=5.04687)\n",
      "  4  model al-feat: -0.139582 (tension=5.07114)\n",
      "  5  model al-feat: -0.13929 (tension=5.08546)\n",
      "  6  model al-feat: -0.139117 (tension=5.09392)\n",
      "  7  model al-feat: -0.139015 (tension=5.09894)\n",
      "  8  model al-feat: -0.138955 (tension=5.10192)\n",
      "     final tension: 5.10368\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -719686\n",
      "  log_2 likelihood: -1.03829e+06\n",
      "     cross entropy: 3.7448\n",
      "        perplexity: 13.406\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 95\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 0.895488\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.83779e+06\n",
      "  log_2 likelihood: -8.42214e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.18861\n",
      "       size counts: 105\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -998605\n",
      "  log_2 likelihood: -1.44068e+06\n",
      "     cross entropy: 5.11421\n",
      "        perplexity: 34.6362\n",
      "      posterior p0: 0.0256002\n",
      " posterior al-feat: -0.164199\n",
      "       size counts: 105\n",
      "  1  model al-feat: -0.19094 (tension=4)\n",
      "  2  model al-feat: -0.176383 (tension=4.53483)\n",
      "  3  model al-feat: -0.170318 (tension=4.77852)\n",
      "  4  model al-feat: -0.167397 (tension=4.9009)\n",
      "  5  model al-feat: -0.165902 (tension=4.96486)\n",
      "  6  model al-feat: -0.165115 (tension=4.99893)\n",
      "  7  model al-feat: -0.164694 (tension=5.01726)\n",
      "  8  model al-feat: -0.164467 (tension=5.02716)\n",
      "     final tension: 5.03253\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -820823\n",
      "  log_2 likelihood: -1.1842e+06\n",
      "     cross entropy: 4.20372\n",
      "        perplexity: 18.4267\n",
      "      posterior p0: 0.0258705\n",
      " posterior al-feat: -0.155407\n",
      "       size counts: 105\n",
      "  1  model al-feat: -0.164344 (tension=5.03253)\n",
      "  2  model al-feat: -0.160343 (tension=5.21129)\n",
      "  3  model al-feat: -0.158201 (tension=5.31002)\n",
      "  4  model al-feat: -0.15701 (tension=5.36591)\n",
      "  5  model al-feat: -0.156333 (tension=5.39798)\n",
      "  6  model al-feat: -0.155944 (tension=5.41651)\n",
      "  7  model al-feat: -0.155719 (tension=5.42726)\n",
      "  8  model al-feat: -0.155589 (tension=5.43352)\n",
      "     final tension: 5.43716\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -772080\n",
      "  log_2 likelihood: -1.11388e+06\n",
      "     cross entropy: 3.95409\n",
      "        perplexity: 15.4989\n",
      "      posterior p0: 0.0271189\n",
      " posterior al-feat: -0.154071\n",
      "       size counts: 105\n",
      "  1  model al-feat: -0.155513 (tension=5.43716)\n",
      "  2  model al-feat: -0.154914 (tension=5.466)\n",
      "  3  model al-feat: -0.154565 (tension=5.48285)\n",
      "  4  model al-feat: -0.154362 (tension=5.49274)\n",
      "  5  model al-feat: -0.154242 (tension=5.49855)\n",
      "  6  model al-feat: -0.154172 (tension=5.50197)\n",
      "  7  model al-feat: -0.15413 (tension=5.50398)\n",
      "  8  model al-feat: -0.154106 (tension=5.50517)\n",
      "     final tension: 5.50587\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -757922\n",
      "  log_2 likelihood: -1.09345e+06\n",
      "     cross entropy: 3.88158\n",
      "        perplexity: 14.7392\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 105\n"
     ]
    }
   ],
   "source": [
    "!$FAST_ALIGN -i original_100_source-target_en-de.txt -d -o -v > original_100_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i disambiguated_100_source-target_en-de.txt -d -o -v > disambiguated_100_source-target_en-de_fast-aligned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2c4ce87a-7b1d-4a97-b4d9-c446de1df198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "9.554545454545455\n"
     ]
    }
   ],
   "source": [
    "# Extract target translated words to source words in original\n",
    "\n",
    "import re\n",
    "\n",
    "# List with original translations\n",
    "translations_original = []\n",
    "with open('hyp_original_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations_original.append(line.strip())\n",
    "\n",
    "              \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('original_100_source-target_en-de_fast-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            indices.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            indices.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for translation in translations_original:\n",
    "    tokens = translation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            translated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "    \n",
    "#print(translations_ambiguous_words)\n",
    "print(len(translations_ambiguous_words))\n",
    "\n",
    "unique_translations = 0\n",
    "for set_words in translations_ambiguous_words:\n",
    "    unique_translations += len(set_words)\n",
    "print(unique_translations/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "89ac1e4c-277c-4d1c-a336-409f6c777e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "10.157575757575758\n"
     ]
    }
   ],
   "source": [
    "# Extract target translated words to source words in disambiguated\n",
    "\n",
    "import re\n",
    "\n",
    "# List with original translations\n",
    "translations_disambiguated = []\n",
    "with open('hyp_disambiguated_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations_disambiguated.append(line.strip())\n",
    "\n",
    "              \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('disambiguated_100_source-target_en-de_fast-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] + 1 # exact position of ambiguous word; !!! add 1 because of gender word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            indices.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            indices.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for translation in translations_disambiguated:\n",
    "    tokens = translation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            translated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "    \n",
    "#print(translations_ambiguous_words)\n",
    "print(len(translations_ambiguous_words))\n",
    "\n",
    "unique_translations = 0\n",
    "for set_words in translations_ambiguous_words:\n",
    "    unique_translations += len(set_words)\n",
    "print(unique_translations/330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547be75-dfc4-46c3-ba3f-dfe56c4a3037",
   "metadata": {},
   "source": [
    "## awesome_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7e5d0cfa-eb33-4a71-af0c-9fa94a473c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 33000it [00:44, 739.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# ??? How to set model correctly\n",
    "# MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "!awesome-align \\\n",
    "    --output_file \"original_100_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_100_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9858f351-847d-4b0f-84ff-dde0dddfda7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 33000it [00:41, 786.27it/s]\n"
     ]
    }
   ],
   "source": [
    "!awesome-align \\\n",
    "    --output_file \"disambiguated_100_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"disambiguated_100_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "67dbd476-e0aa-4748-aac9-7fa91247661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "9.648484848484848\n"
     ]
    }
   ],
   "source": [
    "# Extract target translated words to source words in original\n",
    "\n",
    "import re\n",
    "\n",
    "# List with original translations\n",
    "translations_original = []\n",
    "with open('hyp_original_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations_original.append(line.strip())\n",
    "\n",
    "              \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('original_100_source-target_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            indices.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            indices.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for translation in translations_original:\n",
    "    tokens = translation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            translated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "    \n",
    "#print(translations_ambiguous_words)\n",
    "print(len(translations_ambiguous_words))\n",
    "\n",
    "unique_translations = 0\n",
    "for set_words in translations_ambiguous_words:\n",
    "    unique_translations += len(set_words)\n",
    "print(unique_translations/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4a1b191a-081d-4984-bdde-aae7f5b0808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to file\n",
    "\n",
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "ambiguous_words = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "\n",
    "count = 0                \n",
    "with open('unique-words_translations_original_100.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(translations_ambiguous_words[count]), end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "50ce4d32-09d2-426c-b1b0-33be3fa37c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "10.6\n"
     ]
    }
   ],
   "source": [
    "# Extract target translated words to source words in disambiguated\n",
    "\n",
    "import re\n",
    "\n",
    "# List with original translations\n",
    "translations_disambiguated = []\n",
    "with open('hyp_disambiguated_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations_disambiguated.append(line.strip())\n",
    "\n",
    "              \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('disambiguated_100_source-target_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] + 1 # exact position of ambiguous word; !!! add 1 because of gender word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            indices.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            indices.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for translation in translations_disambiguated:\n",
    "    tokens = translation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            translated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "    \n",
    "#print(translations_ambiguous_words)\n",
    "print(len(translations_ambiguous_words))\n",
    "\n",
    "unique_translations = 0\n",
    "for set_words in translations_ambiguous_words:\n",
    "    unique_translations += len(set_words)\n",
    "print(unique_translations/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "339d92d0-9fa7-4140-ab74-a6c82f9cf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to file\n",
    "\n",
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "ambiguous_words = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "\n",
    "count = 0                \n",
    "with open('unique-words_translations_disambiguated_100.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(translations_ambiguous_words[count]), end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab63c-5750-42b2-b80a-ea767942f356",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word alignement (translation-backtranslation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ac057-9ea8-4bff-a824-8f4498ed95a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fast_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e8452-0480-4b1c-a990-b89f331bc931",
   "metadata": {},
   "source": [
    "- Extract the position of the translated ambiguous word from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "adaea51e-5cb1-4809-8234-04966b3ed129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "positions_ambiguous_words_original = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('original_100_source-target_en-de_fast-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            positions_ambiguous_words_original.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            positions_ambiguous_words_original.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "print(len(positions_ambiguous_words_original))\n",
    "#print(positions_ambiguous_words_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ec8f7f6b-d115-4737-bff5-9b61f84d7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "positions_ambiguous_words_disambiguated = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('disambiguated_100_source-target_en-de_fast-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] + 1 # exact position of ambiguous word; !!! add 1 because of gender word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            positions_ambiguous_words_disambiguated.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            positions_ambiguous_words_disambiguated.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "print(len(positions_ambiguous_words_disambiguated))\n",
    "#print(positions_ambiguous_words_disambiguated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f6e57-70df-4156-99b5-859e397f52b7",
   "metadata": {},
   "source": [
    "- Input to fast_align must be tokenized and aligned into parallel sentences. \n",
    "- Line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space (|||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d564f6f5-8c8a-4794-91ca-a7868d574dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "33000\n"
     ]
    }
   ],
   "source": [
    "# List with original translated sentences\n",
    "translations = []\n",
    "with open('hyp_original_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations.append(line.strip())\n",
    "         \n",
    "# List with nbest sentences for every translation in original \n",
    "nbest_original = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "\n",
    "print(len(translations))\n",
    "print(len(nbest_original))          \n",
    "        \n",
    "count = 0\n",
    "with open('original_100_translation-back_en-de.txt', 'w') as fout:\n",
    "    while count < 33000:\n",
    "        for hyp in nbest_original[count]:\n",
    "            print(translations[count] + ' ||| ' + hyp, end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ef63648c-81bf-4624-ac2e-6d4148e85331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "33000\n"
     ]
    }
   ],
   "source": [
    "# List with disambiguated translated sentences\n",
    "translations = []\n",
    "with open('hyp_disambiguated_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations.append(line.strip())\n",
    "         \n",
    "# List with nbest sentences for every translation in original \n",
    "nbest_disambiguated = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_disambiguated_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_disambiguated.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "\n",
    "print(len(translations))\n",
    "print(len(nbest_disambiguated))           \n",
    "        \n",
    "count = 0\n",
    "with open('disambiguated_100_translation-back_en-de.txt', 'w') as fout:\n",
    "    while count < 33000:\n",
    "        for hyp in nbest_disambiguated[count]:\n",
    "            print(translations[count] + ' ||| ' + hyp, end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd83be3-c2a7-4245-844c-52f51e416a4d",
   "metadata": {},
   "source": [
    "- Word alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0a6552ff-349f-4048-b65b-4c4959901ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "expected target length = source length * 1.02112\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -5.79642e+08\n",
      "  log_2 likelihood: -8.36247e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.189295\n",
      "       size counts: 185\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -8.85109e+07\n",
      "  log_2 likelihood: -1.27694e+08\n",
      "     cross entropy: 4.5653\n",
      "        perplexity: 23.6752\n",
      "      posterior p0: 0.0264655\n",
      " posterior al-feat: -0.151796\n",
      "       size counts: 185\n",
      "  1  model al-feat: -0.166019 (tension=4)\n",
      "  2  model al-feat: -0.158931 (tension=4.28446)\n",
      "  3  model al-feat: -0.155535 (tension=4.42716)\n",
      "  4  model al-feat: -0.153797 (tension=4.50194)\n",
      "  5  model al-feat: -0.152878 (tension=4.54196)\n",
      "  6  model al-feat: -0.152385 (tension=4.56361)\n",
      "  7  model al-feat: -0.152117 (tension=4.57539)\n",
      "  8  model al-feat: -0.151971 (tension=4.58181)\n",
      "     final tension: 4.58532\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -7.18515e+07\n",
      "  log_2 likelihood: -1.0366e+08\n",
      "     cross entropy: 3.70603\n",
      "        perplexity: 13.0505\n",
      "      posterior p0: 0.0297905\n",
      " posterior al-feat: -0.136903\n",
      "       size counts: 185\n",
      "  1  model al-feat: -0.151892 (tension=4.58532)\n",
      "  2  model al-feat: -0.145322 (tension=4.8851)\n",
      "  3  model al-feat: -0.141815 (tension=5.05347)\n",
      "  4  model al-feat: -0.139828 (tension=5.15171)\n",
      "  5  model al-feat: -0.138665 (tension=5.21021)\n",
      "  6  model al-feat: -0.137971 (tension=5.24544)\n",
      "  7  model al-feat: -0.137553 (tension=5.2668)\n",
      "  8  model al-feat: -0.1373 (tension=5.27981)\n",
      "     final tension: 5.28774\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -6.65924e+07\n",
      "  log_2 likelihood: -9.60725e+07\n",
      "     cross entropy: 3.43477\n",
      "        perplexity: 10.8136\n",
      "      posterior p0: 0.0315803\n",
      " posterior al-feat: -0.13217\n",
      "       size counts: 185\n",
      "  1  model al-feat: -0.137146 (tension=5.28774)\n",
      "  2  model al-feat: -0.135233 (tension=5.38726)\n",
      "  3  model al-feat: -0.134076 (tension=5.44853)\n",
      "  4  model al-feat: -0.133363 (tension=5.48665)\n",
      "  5  model al-feat: -0.13292 (tension=5.51052)\n",
      "  6  model al-feat: -0.132643 (tension=5.52554)\n",
      "  7  model al-feat: -0.132468 (tension=5.535)\n",
      "  8  model al-feat: -0.132358 (tension=5.54098)\n",
      "     final tension: 5.54475\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -6.49417e+07\n",
      "  log_2 likelihood: -9.36911e+07\n",
      "     cross entropy: 3.34963\n",
      "        perplexity: 10.1939\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 185\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "expected target length = source length * 1.02805\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -5.92719e+08\n",
      "  log_2 likelihood: -8.55113e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.18832\n",
      "       size counts: 188\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -9.05816e+07\n",
      "  log_2 likelihood: -1.30682e+08\n",
      "     cross entropy: 4.56903\n",
      "        perplexity: 23.7364\n",
      "      posterior p0: 0.026791\n",
      " posterior al-feat: -0.149829\n",
      "       size counts: 188\n",
      "  1  model al-feat: -0.165214 (tension=4)\n",
      "  2  model al-feat: -0.157642 (tension=4.3077)\n",
      "  3  model al-feat: -0.153984 (tension=4.46396)\n",
      "  4  model al-feat: -0.152089 (tension=4.54706)\n",
      "  5  model al-feat: -0.151072 (tension=4.59225)\n",
      "  6  model al-feat: -0.150517 (tension=4.61711)\n",
      "  7  model al-feat: -0.150211 (tension=4.63086)\n",
      "  8  model al-feat: -0.150042 (tension=4.6385)\n",
      "     final tension: 4.64276\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -7.34235e+07\n",
      "  log_2 likelihood: -1.05928e+08\n",
      "     cross entropy: 3.70356\n",
      "        perplexity: 13.0281\n",
      "      posterior p0: 0.0297748\n",
      " posterior al-feat: -0.134217\n",
      "       size counts: 188\n",
      "  1  model al-feat: -0.149948 (tension=4.64276)\n",
      "  2  model al-feat: -0.143214 (tension=4.95737)\n",
      "  3  model al-feat: -0.139565 (tension=5.13731)\n",
      "  4  model al-feat: -0.137463 (tension=5.24427)\n",
      "  5  model al-feat: -0.13621 (tension=5.30918)\n",
      "  6  model al-feat: -0.13545 (tension=5.34905)\n",
      "  7  model al-feat: -0.134983 (tension=5.3737)\n",
      "  8  model al-feat: -0.134694 (tension=5.38902)\n",
      "     final tension: 5.39856\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -6.79155e+07\n",
      "  log_2 likelihood: -9.79814e+07\n",
      "     cross entropy: 3.42573\n",
      "        perplexity: 10.746\n",
      "      posterior p0: 0.0314576\n",
      " posterior al-feat: -0.12914\n",
      "       size counts: 188\n",
      "  1  model al-feat: -0.134515 (tension=5.39856)\n",
      "  2  model al-feat: -0.132518 (tension=5.50605)\n",
      "  3  model al-feat: -0.131287 (tension=5.57362)\n",
      "  4  model al-feat: -0.130514 (tension=5.61656)\n",
      "  5  model al-feat: -0.130023 (tension=5.64403)\n",
      "  6  model al-feat: -0.129709 (tension=5.66168)\n",
      "  7  model al-feat: -0.129507 (tension=5.67306)\n",
      "  8  model al-feat: -0.129377 (tension=5.6804)\n",
      "     final tension: 5.68514\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      "  log_e likelihood: -6.61635e+07\n",
      "  log_2 likelihood: -9.54537e+07\n",
      "     cross entropy: 3.33735\n",
      "        perplexity: 10.1075\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 188\n"
     ]
    }
   ],
   "source": [
    "!$FAST_ALIGN -i original_100_translation-back_en-de.txt -d -o -v > original_100_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i disambiguated_100_translation-back_en-de.txt -d -o -v > disambiguated_100_translation-back_en-de_fast-aligned.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d06b-82f7-4341-be31-eef67f4a415a",
   "metadata": {},
   "source": [
    "- Extract target backtranslated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ebc21367-64ff-4525-a39f-ba06df06621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "330\n",
      "92.68484848484849\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List with original backtranslations\n",
    "backtranslations_original = []\n",
    "with open('hyp_original_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        backtranslations_original.append(line.strip())\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = []\n",
    "with open('original_100_translation-back_en-de_fast-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        positions = positions_ambiguous_words_original[counter] # exact positions of ambiguous words\n",
    "        list_indices = []\n",
    "        for position in positions:\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                list_indices.extend([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                list_indices.extend([999])\n",
    "        indices.append(list_indices)\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "backtranslations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "for backtranslation in backtranslations_original:\n",
    "    tokens = backtranslation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            backtranslated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            backtranslations_ambiguous_words.append(backtranslated_ambiguous_words)\n",
    "            backtranslated_ambiguous_words = set()\n",
    "\n",
    "\n",
    "    \n",
    "#print(backtranslations_ambiguous_words)\n",
    "print(len(backtranslations_ambiguous_words))\n",
    "\n",
    "# Here we need to merge the sets for every 10 sets, because we want to see unique words in the nbest 100 backtranslation\n",
    "backtranslations_ambiguous_words_reduced = []\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "counter = 0\n",
    "for set_words in backtranslations_ambiguous_words:\n",
    "    backtranslated_ambiguous_words.update(set_words)\n",
    "    counter += 1\n",
    "    if (counter % 100 == 0):\n",
    "        backtranslations_ambiguous_words_reduced.append(backtranslated_ambiguous_words)\n",
    "        backtranslated_ambiguous_words = set()\n",
    "    \n",
    "print(len(backtranslations_ambiguous_words_reduced))   \n",
    "    \n",
    "unique_backtranslations = 0\n",
    "for set_words in backtranslations_ambiguous_words_reduced:\n",
    "    unique_backtranslations += len(set_words)\n",
    "print(unique_backtranslations/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8884ad88-b68d-4cd0-a38e-a4d2c89cdd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "330\n",
      "97.16666666666667\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List with disambiguated backtranslations\n",
    "backtranslations_disambiguated = []\n",
    "with open('hyp_disambiguated_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        backtranslations_disambiguated.append(line.strip())\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = []\n",
    "with open('disambiguated_100_translation-back_en-de_fast-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        positions = positions_ambiguous_words_disambiguated[counter] # exact positions of ambiguous words\n",
    "        list_indices = []\n",
    "        for position in positions:\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                list_indices.extend([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                list_indices.extend([999])\n",
    "        indices.append(list_indices)\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "backtranslations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "for backtranslation in backtranslations_disambiguated:\n",
    "    tokens = backtranslation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            backtranslated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            backtranslations_ambiguous_words.append(backtranslated_ambiguous_words)\n",
    "            backtranslated_ambiguous_words = set()\n",
    "    \n",
    "#print(backtranslations_ambiguous_words)\n",
    "print(len(backtranslations_ambiguous_words))\n",
    "\n",
    "# Here we need to merge the sets for every 10 sets, because we want to see unique words in the nbest 100 backtranslation\n",
    "backtranslations_ambiguous_words_reduced = []\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "counter = 0\n",
    "for set_words in backtranslations_ambiguous_words:\n",
    "    backtranslated_ambiguous_words.update(set_words)\n",
    "    counter += 1\n",
    "    if (counter % 100 == 0):\n",
    "        backtranslations_ambiguous_words_reduced.append(backtranslated_ambiguous_words)\n",
    "        backtranslated_ambiguous_words = set()\n",
    "\n",
    "print(len(backtranslations_ambiguous_words_reduced))   \n",
    "    \n",
    "unique_backtranslations = 0\n",
    "for set_words in backtranslations_ambiguous_words_reduced:\n",
    "    unique_backtranslations += len(set_words)\n",
    "print(unique_backtranslations/330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed81f30-6996-4a42-a7bf-945dd7e74c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## awesome_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d423a-639b-4f9e-b9a7-a191578edc4e",
   "metadata": {},
   "source": [
    "- Extract the position of the translated ambiguous word from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3542e7f6-f507-42b6-b5b0-3dc1f89deba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "positions_ambiguous_words_original = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('original_100_source-target_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            positions_ambiguous_words_original.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            positions_ambiguous_words_original.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "print(len(positions_ambiguous_words_original))\n",
    "#print(positions_ambiguous_words_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7c8a6727-b03f-4344-9870-7235334baf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "positions_ambiguous_words_disambiguated = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('disambiguated_100_source-target_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] + 1 # exact position of ambiguous word; !!! add 1 because of gender word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            positions_ambiguous_words_disambiguated.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            positions_ambiguous_words_disambiguated.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "print(len(positions_ambiguous_words_disambiguated))\n",
    "#print(positions_ambiguous_words_disambiguated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b6d21-3ddb-4904-ab00-b7c41e3a9016",
   "metadata": {},
   "source": [
    "- Word alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7344a012-1da5-4328-a801-29a8a45826a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 3300000it [1:07:16, 817.45it/s] \n"
     ]
    }
   ],
   "source": [
    "# ??? How to set model correctly\n",
    "# MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "!awesome-align \\\n",
    "    --output_file \"original_100_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_100_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "48f24f88-a35b-4b3f-bca7-f366c4dc583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 2704672it [57:31, 878.73it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 3300000it [1:09:50, 787.50it/s]\n"
     ]
    }
   ],
   "source": [
    "!awesome-align \\\n",
    "    --output_file \"disambiguated_100_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"disambiguated_100_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f0e8-a5b2-44d2-8732-5b68f5be0a6b",
   "metadata": {},
   "source": [
    "- Extract target backtranslated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "61b55b39-be61-4623-8ba3-c5f00ca7a088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "330\n",
      "84.96969696969697\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List with original backtranslations\n",
    "backtranslations_original = []\n",
    "with open('hyp_original_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        backtranslations_original.append(line.strip())\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = []\n",
    "with open('original_100_translation-back_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        positions = positions_ambiguous_words_original[counter] # exact positions of ambiguous words\n",
    "        list_indices = []\n",
    "        for position in positions:\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                list_indices.extend([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                list_indices.extend([999])\n",
    "        indices.append(list_indices)\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "backtranslations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "for backtranslation in backtranslations_original:\n",
    "    tokens = backtranslation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            backtranslated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            backtranslations_ambiguous_words.append(backtranslated_ambiguous_words)\n",
    "            backtranslated_ambiguous_words = set()\n",
    "\n",
    "\n",
    "    \n",
    "#print(backtranslations_ambiguous_words)\n",
    "print(len(backtranslations_ambiguous_words))\n",
    "\n",
    "# Here we need to merge the sets for every 10 sets, because we want to see unique words in the nbest 100 backtranslation\n",
    "backtranslations_ambiguous_words_reduced = []\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "counter = 0\n",
    "for set_words in backtranslations_ambiguous_words:\n",
    "    backtranslated_ambiguous_words.update(set_words)\n",
    "    counter += 1\n",
    "    if (counter % 100 == 0):\n",
    "        backtranslations_ambiguous_words_reduced.append(backtranslated_ambiguous_words)\n",
    "        backtranslated_ambiguous_words = set()\n",
    "    \n",
    "print(len(backtranslations_ambiguous_words_reduced))   \n",
    "    \n",
    "unique_backtranslations = 0\n",
    "for set_words in backtranslations_ambiguous_words_reduced:\n",
    "    unique_backtranslations += len(set_words)\n",
    "print(unique_backtranslations/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6900b10c-e42a-4c1a-9635-d2384a552751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to file\n",
    "\n",
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "ambiguous_words = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "\n",
    "count = 0                \n",
    "with open('unique-words_backtranslations_original_100.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(backtranslations_ambiguous_words_reduced[count]), end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5ba2b83c-62bd-4326-ba40-dd906662ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n",
      "330\n",
      "93.89090909090909\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List with disambiguated backtranslations\n",
    "backtranslations_disambiguated = []\n",
    "with open('hyp_disambiguated_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        backtranslations_disambiguated.append(line.strip())\n",
    "             \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = []\n",
    "with open('disambiguated_100_translation-back_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        positions = positions_ambiguous_words_disambiguated[counter] # exact positions of ambiguous words\n",
    "        list_indices = []\n",
    "        for position in positions:\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                list_indices.extend([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                list_indices.extend([999])\n",
    "        indices.append(list_indices)\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "backtranslations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "for backtranslation in backtranslations_disambiguated:\n",
    "    tokens = backtranslation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            backtranslated_ambiguous_words.add(tokens[ind])\n",
    "    lineNumber += 1\n",
    "    if (lineNumber % 100 == 0):\n",
    "            backtranslations_ambiguous_words.append(backtranslated_ambiguous_words)\n",
    "            backtranslated_ambiguous_words = set()\n",
    "    \n",
    "#print(backtranslations_ambiguous_words)\n",
    "print(len(backtranslations_ambiguous_words))\n",
    "\n",
    "# Here we need to merge the sets for every 10 sets, because we want to see unique words in the nbest 100 backtranslation\n",
    "backtranslations_ambiguous_words_reduced = []\n",
    "backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "counter = 0\n",
    "for set_words in backtranslations_ambiguous_words:\n",
    "    backtranslated_ambiguous_words.update(set_words)\n",
    "    counter += 1\n",
    "    if (counter % 100 == 0):\n",
    "        backtranslations_ambiguous_words_reduced.append(backtranslated_ambiguous_words)\n",
    "        backtranslated_ambiguous_words = set()\n",
    "\n",
    "print(len(backtranslations_ambiguous_words_reduced))   \n",
    "    \n",
    "unique_backtranslations = 0\n",
    "for set_words in backtranslations_ambiguous_words_reduced:\n",
    "    unique_backtranslations += len(set_words)\n",
    "print(unique_backtranslations/330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "c0a78096-b560-4860-ab0a-e0c30d3b9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to file\n",
    "\n",
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "ambiguous_words = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "\n",
    "count = 0                \n",
    "with open('unique-words_backtranslations_disambiguated_100.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(backtranslations_ambiguous_words_reduced[count]), end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7aa2d8-87dd-40fb-a52b-b0859436258f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (translation-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef887b3-6413-4801-92de-f3f0948d02b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tercom alignement (borrowed from Tu)\n",
    "- https://github.com/TuAnh23/Perturbation-basedQE/blob/master/align_and_analyse_ambiguous_trans.py#L54-L92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416baae-abdf-4ee4-9f62-a8519ed855de",
   "metadata": {},
   "source": [
    "### Extract target translated words to source words in original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77b7e76c-e6c4-4bfb-bbd8-df6e9b3a6b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300000\n",
      "3300000\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences; output 100 times to match backtranslation size\n",
    "source = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        for i in range(10000): # append the source sentence 10000 times to match backtranslations later\n",
    "            source.append(line.strip().split()) # split() tokenizes the sentence, because tercom expects tokens     \n",
    "\n",
    "print(len(source))\n",
    "\n",
    "# List with original backtranslations\n",
    "backtranslations = []\n",
    "with open('hyp_original_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        backtranslations.append(line.strip().split())\n",
    "        \n",
    "print(len(backtranslations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "fe0df752-6cdb-4dec-9ef6-d630f718d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Perturbation-basedQE' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TuAnh23/Perturbation-basedQE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "11ec5c90-a5ba-42dd-8ae5-a9be898c2948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity/Perturbation-basedQE\n"
     ]
    }
   ],
   "source": [
    "%cd ./Perturbation-basedQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0c1d2480-4101-4dcc-bcc1-15721c74f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import align_and_analyse_ambiguous_trans as tercom\n",
    "\n",
    "alignments = tercom.tercom_alignment(source, backtranslations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3b07e-fb94-439f-a185-7f0d1ae2b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#print(positions_ambiguous_words)\n",
    "\n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = []\n",
    "for align in alignments:\n",
    "    if (lineNumber == 10000):\n",
    "        lineNumber = 0\n",
    "        counter += 1\n",
    "    position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "    indices.append([item[1] for item in (item for item in align if not(pd.isna(item[0]))) if item[0] == position][0])\n",
    "    lineNumber += 1\n",
    "\n",
    "print(len(indices))\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for backtranslation in backtranslations:\n",
    "    if (lineNumber == 10000):\n",
    "        print(\"here\")\n",
    "        translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "        lineNumber = 0\n",
    "        translated_ambiguous_words = set()\n",
    "    backtranslation_index = backtranslations.index(backtranslation)\n",
    "    if not(pd.isna(indices[backtranslation_index])):\n",
    "        translated_ambiguous_words.add(backtranslation[indices[backtranslation_index]])\n",
    "    lineNumber += 1\n",
    "translations_ambiguous_words.append(translated_ambiguous_words) # last lines\n",
    "\n",
    "    \n",
    "#print(translations_ambiguous_words)\n",
    "print(len(translations_ambiguous_words))\n",
    "\n",
    "unique_translations = 0\n",
    "for set_words in translations_ambiguous_words:\n",
    "    unique_translations += len(set_words)\n",
    "print(unique_translations/330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6fc6e-1e3c-41b3-8b38-aacd4210525d",
   "metadata": {},
   "source": [
    "### Extract target translated words to source words in disambiguated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38e68b-9ead-4e71-a3d1-e09bb3531514",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4ea19-21c6-4437-a68d-93ff98ef8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with original source sentences; output 100 times to match backtranslation size\n",
    "source = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        for i in range(10000): # append the source sentence 100 times to match backtranslations later\n",
    "            source.append(line.strip().split()) # split() tokenizes the sentence, because tercom expects tokens     \n",
    "\n",
    "print(len(source))\n",
    "\n",
    "# List with original backtranslations\n",
    "backtranslations = []\n",
    "with open('hyp_disambiguated_back_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        backtranslations.append(line.strip().split())\n",
    "        \n",
    "print(len(backtranslations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a67a1-d4cf-4a60-9ffa-fae2e75d2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./Perturbation-basedQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87286b7b-b25c-493e-b2b4-fbb0b6bfa8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import align_and_analyse_ambiguous_trans as tercom\n",
    "\n",
    "alignments = tercom.tercom_alignment(source, backtranslations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c3475-7b8a-44b9-8eda-56a93757d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#print(positions_ambiguous_words)\n",
    "\n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = []\n",
    "for align in alignments:\n",
    "    if (lineNumber == 10000):\n",
    "        lineNumber = 0\n",
    "        counter += 1\n",
    "    position = positions_ambiguous_words[counter] + 1 # exact position of ambiguous word; skip gender word\n",
    "    indices.append([item[1] for item in (item for item in align if not(pd.isna(item[0]))) if item[0] == position][0])\n",
    "    lineNumber += 1\n",
    "\n",
    "print(len(indices))\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for backtranslation in backtranslations:\n",
    "    if (lineNumber == 10000):\n",
    "        translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "        lineNumber = 0\n",
    "        translated_ambiguous_words = set()\n",
    "    backtranslation_index = backtranslations.index(backtranslation)\n",
    "    if not(pd.isna(indices[backtranslation_index])):\n",
    "        translated_ambiguous_words.add(backtranslation[indices[backtranslation_index]])\n",
    "    lineNumber += 1\n",
    "translations_ambiguous_words.append(translated_ambiguous_words) # last lines\n",
    "\n",
    "    \n",
    "#(translations_ambiguous_words)\n",
    "print(len(translations_ambiguous_words))\n",
    "\n",
    "unique_translations = 0\n",
    "for set_words in translations_ambiguous_words:\n",
    "    unique_translations += len(set_words)\n",
    "print(unique_translations/330)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125d1e1-22bd-401c-aa6e-de29fc5f39a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a414ead-4d6d-4852-b8c3-05386758baf5",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44994020-bc7a-482f-b2dd-df939581b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alignment_indices_translation(filename_translations):\n",
    "    \"\"\"\n",
    "    Extract alignment indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_translations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "    \n",
    "    return indices_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276742f-0c1c-40ba-bb47-56523b968de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_original = extract_alignment_indices_translation('original_source-target_en-de_awesome-aligned.txt')\n",
    "indices_disambiguated = extract_alignment_indices_translation('disambiguated_source-target_en-de_awesome-aligned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0480ed7-f9fe-4720-babb-52de33459923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_translations(filename_tokenized, filename_translations, filename_out, indices_translation):\n",
    "    \"\"\"\n",
    "    Match alignment indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    translations = []\n",
    "    with open(filename_translations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                translations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 330\n",
    "    counter = 0\n",
    "    for i in range(0, 330): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 9):\n",
    "                alignments = indices_translation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(translations[counter + f])):\n",
    "                                 words_set.add(translations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 10\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 330:\n",
    "            print(source[count] + ' | ' + str(target_words[count]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e00bd-3e99-478d-95e4-ca5021303967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_translations(filename_tokenized, filename_translations, filename_out, indices_translation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    translations = []\n",
    "    with open(filename_translations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                translations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 330\n",
    "    counter = 0\n",
    "    for i in range(0, 330): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 9):\n",
    "                alignments = indices_translation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(translations[counter + f])):\n",
    "                                 words_set.add(translations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 10\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 330:\n",
    "            print(source[count] + ' | ' + str([len(target_set) for target_set in target_words[count]]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b914c-ed97-4ec6-9c0a-0edbecee8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_word_translations('tok.en_original.en', 'hyp_original.txt', 'translations_words_original.txt', indices_original)\n",
    "count_word_translations('tok.en_original.en', 'hyp_original.txt', 'translations_words_original_occurrence.txt', indices_original)\n",
    "\n",
    "extract_word_translations('tok.en_disambiguated.en', 'hyp_disambiguated.txt', 'translations_words_disambiguated.txt', indices_disambiguated)\n",
    "count_word_translations('tok.en_disambiguated.en', 'hyp_disambiguated.txt', 'translations_words_disambiguated_occurrence.txt', indices_disambiguated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec3323-4836-4d5f-9058-5d4881440b13",
   "metadata": {},
   "source": [
    "## Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef52ef-e6d1-4b18-b1b4-e0d16759c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alignment_indices_backtranslation(filename_translations, filename_backtranslations):\n",
    "    \"\"\"\n",
    "    Extract alignment indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_translations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "       \n",
    "    # Match alignement indices from translation to backtranslation\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices_backtranslation = []\n",
    "    with open(filename_backtranslations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 10):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for index_list in indices_translation[counter]:\n",
    "                index_matches = []\n",
    "                for index in index_list:\n",
    "                    regex = r\"\" + str(index) + r\"-(\\d)\"\n",
    "                    if re.findall(regex, line): \n",
    "                        index_matches.extend([int(i) for i in re.findall(regex, line)])\n",
    "                    else:\n",
    "                        index_matches.extend([999])\n",
    "                indices_line.append(index_matches)\n",
    "            indices_backtranslation.append(indices_line)\n",
    "            lineNumber += 1 \n",
    "    return indices_backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba1bd1-658a-4d5f-9292-2cac0dc02fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_original = extract_alignment_indices_backtranslation('original_source-target_en-de_awesome-aligned.txt', 'original_translation-back_en-de_awesome-aligned.txt')\n",
    "indices_disambiguated = extract_alignment_indices_backtranslation('disambiguated_source-target_en-de_awesome-aligned.txt', 'disambiguated_translation-back_en-de_awesome-aligned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4d58d-7e4d-4cfa-bb85-2be737e541ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_backtranslations(filename_tokenized, filename_backtranslations, filename_out, indices_backtranslation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(filename_backtranslations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                backtranslations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 330\n",
    "    counter = 0\n",
    "    for i in range(0, 330): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 99):\n",
    "                alignments = indices_backtranslation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(backtranslations[counter + f])):\n",
    "                                 words_set.add(backtranslations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 100\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 330:\n",
    "            print(source[count] + ' | ' + str(target_words[count]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef085c-981b-41c8-a822-4c293bc90bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_backtranslations(filename_tokenized, filename_backtranslations, filename_out, indices_backtranslation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(filename_backtranslations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                backtranslations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 330\n",
    "    counter = 0\n",
    "    for i in range(0, 330): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 99):\n",
    "                alignments = indices_backtranslation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(backtranslations[counter + f])):\n",
    "                                 words_set.add(backtranslations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 100\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 330:\n",
    "            print(source[count] + ' | ' + str([len(target_set) for target_set in target_words[count]]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a82e1-0a7a-4535-a562-ca5428fcef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_word_backtranslations('tok.en_original.en', 'hyp_original_back.txt', 'backtranslations_words_original.txt', indices_original)\n",
    "count_word_backtranslations('tok.en_original.en', 'hyp_original_back.txt', 'backtranslations_words_original_occurrence.txt', indices_original)\n",
    "\n",
    "extract_word_backtranslations('tok.en_disambiguated.en', 'hyp_disambiguated_back.txt', 'backtranslations_words_disambiguated.txt', indices_disambiguated)\n",
    "count_word_backtranslations('tok.en_disambiguated.en', 'hyp_disambiguated_back.txt', 'backtranslations_words_disambiguated_occurrence.txt', indices_disambiguated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b80a2-1eb0-457b-901f-38c5c4a62054",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gender statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "983d9262-6a6b-45a8-b4bf-ce2598ed9471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_female\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "56c85b2b-de5f-4973-b35f-05e366073d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class GENDER(Enum):\n",
    "    \"\"\"\n",
    "    Enumerate possible genders.\n",
    "    Ignore option resolves to words that should be ignored in particular language\n",
    "    \"\"\"\n",
    "    male = 0\n",
    "    female = 1\n",
    "    neutral = 2\n",
    "    unknown = 3\n",
    "    ignore = 4\n",
    "    \n",
    "DE_DETERMINERS = {\"der\": GENDER.male, \"ein\": GENDER.male, \"dem\": GENDER.male, #\"den\": GENDER.male, \n",
    "                  \"einen\": GENDER.male, \"des\": GENDER.male, \"er\": GENDER.male, \"seiner\": GENDER.male,\n",
    "                  \"ihn\": GENDER.male, \"seinen\": GENDER.male, \"ihm\": GENDER.male, \"ihren\": GENDER.male,\n",
    "                  \"die\": GENDER.female, \"eine\": GENDER.female, \"einer\": GENDER.female, \"seinem\": GENDER.male,\n",
    "                  \"ihrem\": GENDER.male, \"sein\": GENDER.male,\n",
    "                  \"sie\": GENDER.female, \"seine\": GENDER.female, \"ihrer\": GENDER.female, \n",
    "                  \"ihr\": GENDER.neutral, \"ihre\": GENDER.neutral, \"das\": GENDER.neutral,\n",
    "                  \"jemanden\": GENDER.neutral}\n",
    "\n",
    "def get_german_determiners(words):\n",
    "    \"\"\"\n",
    "    Get a list of (gender)\n",
    "    given a list of words.\n",
    "    \"\"\"\n",
    "    determiners = []\n",
    "    for (word_ind, word) in enumerate(words):\n",
    "        word = word.lower()\n",
    "        if word in DE_DETERMINERS:\n",
    "            determiners.append((DE_DETERMINERS[word].name))\n",
    "    return determiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ebd8cf96-02b7-4594-8b3a-f06047f00ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    }
   ],
   "source": [
    "dets = get_german_determiners([\"dem\"])\n",
    "print(dets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9d63e-efae-414e-95a6-767582b657a3",
   "metadata": {},
   "source": [
    "- Extract positions of ambiguos words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2f49afb8-3902-4bbb-92c0-e9a51883cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'editor', 'assistant', 'patient', 'tailor', 'mechanic', 'firefighter', 'mover', 'laborer', 'developer', 'salesperson', 'janitor', 'cleaner', 'teenager', 'therapist', 'CEO', 'manager', 'clerk', 'chief', 'bartender', 'secretary', 'undergraduate', 'housekeeper', 'driver', 'examiner', 'specialist', 'programmer', 'dietitian', 'accountant', 'guard', 'advisor', 'broker', 'receptionist', 'sheriff', 'farmer', 'customer', 'baker', 'scientist', 'auditor', 'carpenter', 'supervisor', 'painter', 'librarian', 'writer', 'cook', 'nurse', 'attendant', 'physician', 'counselor', 'analyst', 'practitioner', 'lawyer', 'hairdresser', 'worker'}\n",
      "53\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 6, 1, 4, 1, 1, 1, 1, 1, 3, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# List with source words\n",
    "words = set() # set forces uniqueness\n",
    "with open('words.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        words.add(line.strip())\n",
    "        \n",
    "ambiguous_words = set() # set forces uniqueness\n",
    "positions_ambiguous_words = []\n",
    "\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.add(token)\n",
    "                position = tokens.index(token)\n",
    "                positions_ambiguous_words.append(position)\n",
    "                break\n",
    "        \n",
    "print(ambiguous_words)\n",
    "print(len(ambiguous_words))\n",
    "print(positions_ambiguous_words)\n",
    "print(len(positions_ambiguous_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "15086705-85bb-4127-82c4-8c0e7f6e1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target translated words to source words in original\n",
    "\n",
    "import re\n",
    "\n",
    "# List with original translations\n",
    "translations_original = []\n",
    "with open('hyp_original_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations_original.append(line.strip())\n",
    "\n",
    "              \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('original_100_source-target_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            indices.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            indices.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for translation in translations_original:\n",
    "    if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "    tokens = translation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            translated_ambiguous_words.add(tokens[0]) # extract articles; currently assume index 0 for article position, TODO\n",
    "    lineNumber += 1\n",
    "translations_ambiguous_words.append(translated_ambiguous_words) # last lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "709d5aa4-d5cc-4dcd-93f4-96fe78dbb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to file\n",
    "\n",
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "ambiguous_words = []\n",
    "with open('tok.en_original.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "\n",
    "count = 0  \n",
    "genders = []\n",
    "male = []\n",
    "female = []\n",
    "with open('unique-words_translations_original_100_articles.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        #print(translations_ambiguous_words[count])\n",
    "        genders.append(set(get_german_determiners(translations_ambiguous_words[count])))\n",
    "        male.append(\"male\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "        female.append(\"female\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "        print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(get_german_determiners(translations_ambiguous_words[count])), end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3a9647cb-4aad-4136-8c97-a9d6e09abc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male'}, {'male', 'female'}, {'male'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male'}, {'male'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, set(), {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, set(), {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, set(), {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}]\n",
      "289\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True]\n",
      "326\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, False, False, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True]\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "print(genders)\n",
    "print(sum(1 for i in genders if ('male' in i and 'female' in i))) # both genders\n",
    "\n",
    "print(male)\n",
    "print(male.count(True)) # only male gender\n",
    "\n",
    "print(female)\n",
    "print(female.count(True)) # only female gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1037b9df-1384-4697-87f0-d6f77548a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target translated words to source words in disambiguated\n",
    "\n",
    "import re\n",
    "\n",
    "# List with original translations\n",
    "translations_disambiguated = []\n",
    "with open('hyp_disambiguated_100.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        translations_disambiguated.append(line.strip())\n",
    "\n",
    "              \n",
    "lineNumber = 0\n",
    "counter = 0\n",
    "indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "with open('disambiguated_100_source-target_en-de_awesome-aligned.txt', 'r') as alignments:\n",
    "    for line in alignments:\n",
    "        if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            counter += 1\n",
    "        position = positions_ambiguous_words[counter] + 1 # exact position of ambiguous word; !!! add 1 because of gender word\n",
    "        regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "        if re.findall(regex, line): \n",
    "            indices.append([int(index) for index in re.findall(regex, line)])\n",
    "        else:\n",
    "            indices.append([999])\n",
    "        lineNumber += 1\n",
    "        \n",
    "#print(len(indices))\n",
    "#print(indices)\n",
    "\n",
    "lineNumber = 0\n",
    "translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "translated_ambiguous_words = set() # set forces uniqueness\n",
    "for translation in translations_disambiguated:\n",
    "    if (lineNumber == 100):\n",
    "            lineNumber = 0\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "    tokens = translation.split(' ')\n",
    "    if 999 not in indices[lineNumber]:\n",
    "        for ind in indices[lineNumber]:\n",
    "            translated_ambiguous_words.add(tokens[0]) # extract articles; currently assume index 0 for article position, TODO\n",
    "    lineNumber += 1\n",
    "translations_ambiguous_words.append(translated_ambiguous_words) # last lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "94e39035-325d-4bf2-a594-7389631df1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to file\n",
    "\n",
    "# List with original source sentences\n",
    "source = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source.append(line.strip())\n",
    "        \n",
    "ambiguous_words = []\n",
    "with open('tok.en_disambiguated.en', 'r') as fin:\n",
    "    for line in fin:\n",
    "        tokens = line.split(' ')\n",
    "        for token in tokens:\n",
    "            if token in words:\n",
    "                ambiguous_words.append(token)\n",
    "                break\n",
    "\n",
    "count = 0    \n",
    "genders = []\n",
    "male = []\n",
    "female = []\n",
    "with open('unique-words_translations_disambiguated_100_articles.txt', 'w') as fout:\n",
    "    while count < 330:\n",
    "        #print(translations_ambiguous_words[count])\n",
    "        #print(get_german_determiners(translations_ambiguous_words[count]))\n",
    "        genders.append(set(get_german_determiners(translations_ambiguous_words[count])))\n",
    "        male.append(\"male\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "        female.append(\"female\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "        print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(get_german_determiners(translations_ambiguous_words[count])), end='\\n', file=fout)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0f63aeba-a6a7-4972-ba2d-a4411b15b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female', 'neutral'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female', 'neutral'}, {'female'}, {'male', 'female'}, {'female', 'neutral'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female', 'neutral'}, {'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female', 'neutral'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female', 'neutral'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, set(), {'male', 'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, set(), {'male', 'female', 'neutral'}, {'male', 'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'male', 'female'}, {'male'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female', 'neutral'}, {'female'}, {'female'}, {'female'}, set(), {'female'}, {'female'}, {'female'}, {'female'}, {'male', 'female'}, {'male', 'female'}, {'female'}, {'female'}]\n",
      "210\n",
      "[True, True, False, False, True, True, True, False, True, True, True, True, True, False, True, True, True, False, False, False, True, True, True, True, True, False, False, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, False, False, True, False, True, False, False, True, False, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, False, False, False, False, True, False, False, True, True, True, True, False, False, False, False, True, False, False, True, False, True, True, True, False, True, True, True, True, False, True, True, True, False, False, True, True, False, True, False, True, False, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, False, True, True, False, True, True, True, False, True, True, True, True, True, False, True, False, True, True, False, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, False, False, True, True, False, False, True, True, False, True, True, False, True, False, True, False, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, False, True, False, True, False, True, False, False, True, False, True, True, True, False, True, True, True, True, True, False, True, False, False, False, True, False, True, True, True, True, True, True, False, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, False, True, True, False, False, True, True, False, True, False, True, False, True, True, True, False, True, True, False, False, False, False, True, True, True, False, False, False, True, True, False, False, True, True, True, True, True, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, False, False]\n",
      "211\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True]\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "print(genders)\n",
    "print(sum(1 for i in genders if ('male' in i and 'female' in i))) # both genders\n",
    "\n",
    "print(male)\n",
    "print(male.count(True)) # only male gender\n",
    "\n",
    "print(female)\n",
    "print(female.count(True)) # only female gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89108ad5-a94a-4826-b8d9-6b07698d174d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
