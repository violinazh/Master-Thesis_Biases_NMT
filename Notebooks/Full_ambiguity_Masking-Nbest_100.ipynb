{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7eb9c1b6-7f4d-468a-984f-e7b61fa4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "PATH=\"/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_100/Masking\"\n",
    "FASTBPE=\"/home/vzhekova/fastBPE/fast\" # path to the fastBPE tool\n",
    "FAST_ALIGN=\"/home/vzhekova/fast_align/build/fast_align\" # path to the fast_align tool\n",
    "TERCOM = \"/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_male/Perturbation-basedQE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48aa5a31-2b6f-4744-bd0e-cd820e61740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# check if we can connect to the GPU with PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c199b4a0-7cab-4d4c-b270-e55ef77d010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_100/Masking\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ec0de-a7d8-4c97-a71f-48fb82d1ac2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Translation English-German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0534e037-1503-462b-aacc-bed90ec0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "NBEST = 100\n",
    "BEAM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df524a1-a957-45a8-ad21-a362d7abd060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-20 15:21:56 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_man_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 15:21:56 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 15:21:56 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 15:21:56 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 15:22:32 | INFO | fairseq.data.data_utils | loaded 335 examples from: data-bin_original_man_en-de/test.en-de.en\n",
      "2023-09-20 15:22:32 | INFO | fairseq.tasks.translation | data-bin_original_man_en-de test en-de 335 examples\n",
      "2023-09-20 15:24:44 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-20 15:24:44 | INFO | fairseq_cli.generate | Translated 335 sentences (3,109 tokens) in 89.1s (3.76 sentences/s, 34.88 tokens/s)\n",
      "2023-09-20 15:24:51 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_woman_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 15:24:52 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 15:24:52 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 15:24:52 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 15:25:27 | INFO | fairseq.data.data_utils | loaded 335 examples from: data-bin_original_woman_en-de/test.en-de.en\n",
      "2023-09-20 15:25:27 | INFO | fairseq.tasks.translation | data-bin_original_woman_en-de test en-de 335 examples\n",
      "2023-09-20 15:27:37 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-20 15:27:37 | INFO | fairseq_cli.generate | Translated 335 sentences (3,106 tokens) in 89.1s (3.76 sentences/s, 34.86 tokens/s)\n",
      "2023-09-20 15:27:42 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_girl_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 15:27:43 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 15:27:43 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 15:27:43 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 15:28:18 | INFO | fairseq.data.data_utils | loaded 335 examples from: data-bin_original_girl_en-de/test.en-de.en\n",
      "2023-09-20 15:28:18 | INFO | fairseq.tasks.translation | data-bin_original_girl_en-de test en-de 335 examples\n",
      "2023-09-20 15:31:17 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-20 15:31:17 | INFO | fairseq_cli.generate | Translated 335 sentences (3,095 tokens) in 112.8s (2.97 sentences/s, 27.44 tokens/s)\n",
      "2023-09-20 15:31:22 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_guy_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 15:31:22 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 15:31:22 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 15:31:22 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 15:31:57 | INFO | fairseq.data.data_utils | loaded 335 examples from: data-bin_original_guy_en-de/test.en-de.en\n",
      "2023-09-20 15:31:57 | INFO | fairseq.tasks.translation | data-bin_original_guy_en-de test en-de 335 examples\n",
      "2023-09-20 15:34:10 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-20 15:34:10 | INFO | fairseq_cli.generate | Translated 335 sentences (3,288 tokens) in 90.2s (3.71 sentences/s, 36.46 tokens/s)\n",
      "2023-09-20 15:34:16 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_boy_en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 15:34:16 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 15:34:16 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 15:34:16 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 15:34:51 | INFO | fairseq.data.data_utils | loaded 335 examples from: data-bin_original_boy_en-de/test.en-de.en\n",
      "2023-09-20 15:34:51 | INFO | fairseq.tasks.translation | data-bin_original_boy_en-de test en-de 335 examples\n",
      "2023-09-20 15:37:02 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-20 15:37:02 | INFO | fairseq_cli.generate | Translated 335 sentences (3,098 tokens) in 89.4s (3.75 sentences/s, 34.66 tokens/s)\n",
      "Finished translation.\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "# Beam search\n",
    "!fairseq-generate data-bin_original_man_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_man_en-de.decode_Beam_10.log\n",
    "\n",
    "!fairseq-generate data-bin_original_woman_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_woman_en-de.decode_Beam_10.log\n",
    "\n",
    "!fairseq-generate data-bin_original_girl_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_girl_en-de.decode_Beam_10.log\n",
    "\n",
    "!fairseq-generate data-bin_original_guy_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_guy_en-de.decode_Beam_10.log\n",
    "\n",
    "!fairseq-generate data-bin_original_boy_en-de  \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang de \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_boy_en-de.decode_Beam_10.log\n",
    "\n",
    "print('Finished translation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efb5c4a-0c37-4a6f-bd00-0aaea4da69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LC_ALL=C sort -V' sorts the results in natural order \n",
    "!grep ^H original_man_en-de.decode_Beam_10.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_man.txt\n",
    "!grep ^H original_woman_en-de.decode_Beam_10.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_woman.txt\n",
    "!grep ^H original_girl_en-de.decode_Beam_10.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_girl.txt\n",
    "!grep ^H original_guy_en-de.decode_Beam_10.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_guy.txt\n",
    "!grep ^H original_boy_en-de.decode_Beam_10.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_boy.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425c7dc-f10a-4b6f-81e5-f7d89f14d91f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Backtranslation German-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b245127-a93d-45e3-b3a7-b9791d8b4189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original_man.txt ...\n",
      "Read 248715 words (3369 unique) from text file.\n",
      "Applying BPE to hyp_original_man.txt ...\n",
      "Modified 248715 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original_woman.txt ...\n",
      "Read 251138 words (3532 unique) from text file.\n",
      "Applying BPE to hyp_original_woman.txt ...\n",
      "Modified 251138 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original_girl.txt ...\n",
      "Read 250246 words (3782 unique) from text file.\n",
      "Applying BPE to hyp_original_girl.txt ...\n",
      "Modified 250246 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original_guy.txt ...\n",
      "Read 244621 words (2302 unique) from text file.\n",
      "Applying BPE to hyp_original_guy.txt ...\n",
      "Modified 244621 words from text file.\n",
      "Loading codes from bpecodes.de ...\n",
      "Read 30000 codes from the codes file.\n",
      "Loading vocabulary from hyp_original_boy.txt ...\n",
      "Read 248694 words (3504 unique) from text file.\n",
      "Applying BPE to hyp_original_boy.txt ...\n",
      "Modified 248694 words from text file.\n",
      "Finished subword.\n"
     ]
    }
   ],
   "source": [
    "# Dividing tokenized text into subword units\n",
    "\n",
    "!$FASTBPE applybpe bpe.hyp_original_man.de hyp_original_man.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_original_woman.de hyp_original_woman.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_original_girl.de hyp_original_girl.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_original_guy.de hyp_original_guy.txt bpecodes.de\n",
    "!$FASTBPE applybpe bpe.hyp_original_boy.de hyp_original_boy.txt bpecodes.de\n",
    "\n",
    "print('Finished subword.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd3cf73-9c96-49df-b749-f2bbe337df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-20 15:58:01 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_man_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original_man', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-09-20 15:58:02 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-09-20 15:58:03 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original_man.de: 33500 sents, 308674 tokens, 0.0% replaced (by <unk>)\n",
      "2023-09-20 15:58:03 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_man_de-en\n",
      "2023-09-20 15:58:06 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_woman_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original_woman', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-09-20 15:58:07 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-09-20 15:58:08 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original_woman.de: 33500 sents, 311267 tokens, 0.0% replaced (by <unk>)\n",
      "2023-09-20 15:58:08 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_woman_de-en\n",
      "2023-09-20 15:58:11 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_girl_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original_girl', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-09-20 15:58:11 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-09-20 15:58:13 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original_girl.de: 33500 sents, 310463 tokens, 0.0% replaced (by <unk>)\n",
      "2023-09-20 15:58:13 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_girl_de-en\n",
      "2023-09-20 15:58:16 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_guy_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original_guy', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-09-20 15:58:16 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-09-20 15:58:17 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original_guy.de: 33500 sents, 312980 tokens, 0.0% replaced (by <unk>)\n",
      "2023-09-20 15:58:17 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_guy_de-en\n",
      "2023-09-20 15:58:21 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin_original_boy_de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='bpe.hyp_original_boy', tgtdict='/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref=None, use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=8)\n",
      "2023-09-20 15:58:21 | INFO | fairseq_cli.preprocess | [de] Dictionary: 42024 types\n",
      "2023-09-20 15:58:22 | INFO | fairseq_cli.preprocess | [de] bpe.hyp_original_boy.de: 33500 sents, 311072 tokens, 0.0% replaced (by <unk>)\n",
      "2023-09-20 15:58:22 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin_original_boy_de-en\n",
      "Finished preprocessing.\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original_man \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_man_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original_woman \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_woman_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original_girl \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_girl_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original_guy \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_guy_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --only-source \\\n",
    "    --testpref bpe.hyp_original_boy \\\n",
    "    --srcdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.de.txt \\\n",
    "    --tgtdict /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/dict.en.txt \\\n",
    "    --destdir data-bin_original_boy_de-en \\\n",
    "    --workers 8\n",
    "\n",
    "print('Finished preprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "852891a2-e6c1-4300-9000-cd8f2a4ad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS=\"/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble\"\n",
    "NBEST = 100\n",
    "BEAM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07103d7f-cbc3-456a-acaa-1618cdcf70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-20 16:01:41 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_man_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 16:01:41 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 16:01:41 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 16:01:41 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 16:02:17 | INFO | fairseq.data.data_utils | loaded 33,500 examples from: data-bin_original_man_de-en/test.de-en.de\n",
      "2023-09-20 16:02:17 | INFO | fairseq.tasks.translation | data-bin_original_man_de-en test de-en 33500 examples\n",
      "2023-09-20 20:18:10 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-20 20:18:10 | INFO | fairseq_cli.generate | Translated 33,500 sentences (284,996 tokens) in 9917.4s (3.38 sentences/s, 28.74 tokens/s)\n",
      "2023-09-20 20:18:17 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_woman_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-20 20:18:17 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-20 20:18:17 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-20 20:18:17 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-09-20 20:18:51 | INFO | fairseq.data.data_utils | loaded 33,500 examples from: data-bin_original_woman_de-en/test.de-en.de\n",
      "2023-09-20 20:18:51 | INFO | fairseq.tasks.translation | data-bin_original_woman_de-en test de-en 33500 examples\n",
      "2023-09-21 00:44:08 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-21 00:44:08 | INFO | fairseq_cli.generate | Translated 33,500 sentences (287,693 tokens) in 9913.1s (3.38 sentences/s, 29.02 tokens/s)\n",
      "2023-09-21 00:44:13 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_girl_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-21 00:44:14 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-21 00:44:14 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-21 00:44:14 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-09-21 00:44:49 | INFO | fairseq.data.data_utils | loaded 33,500 examples from: data-bin_original_girl_de-en/test.de-en.de\n",
      "2023-09-21 00:44:49 | INFO | fairseq.tasks.translation | data-bin_original_girl_de-en test de-en 33500 examples\n",
      " 26%|███████▍                     | 1069/4188 [44:38<2:14:53,  2.60s/it, wps=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 07:35:17 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-21 07:35:17 | INFO | fairseq_cli.generate | Translated 33,500 sentences (280,492 tokens) in 8264.5s (4.05 sentences/s, 33.94 tokens/s)\n",
      "2023-09-21 07:35:26 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'beam_mt': 0, 'nbest': 100, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}, 'task': {'_name': 'translation', 'data': 'data-bin_original_boy_de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-09-21 07:35:26 | INFO | fairseq.tasks.translation | [de] dictionary: 42024 types\n",
      "2023-09-21 07:35:26 | INFO | fairseq.tasks.translation | [en] dictionary: 42024 types\n",
      "2023-09-21 07:35:26 | INFO | fairseq_cli.generate | loading model(s) from /export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model1.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model2.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model3.pt:/export/data4/vzhekova/biases-data/De-En/wmt19.de-en.joined-dict.ensemble/model4.pt\n",
      "2023-09-21 07:36:01 | INFO | fairseq.data.data_utils | loaded 33,500 examples from: data-bin_original_boy_de-en/test.de-en.de\n",
      "2023-09-21 07:36:01 | INFO | fairseq.tasks.translation | data-bin_original_boy_de-en test de-en 33500 examples\n",
      " 58%|███████████████▋           | 2429/4188 [1:48:50<1:26:47,  2.96s/it, wps=22]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 11:01:16 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-09-21 11:01:16 | INFO | fairseq_cli.generate | Translated 33,500 sentences (285,366 tokens) in 8311.6s (4.03 sentences/s, 34.33 tokens/s)\n",
      "Finished translation.\n"
     ]
    }
   ],
   "source": [
    "# Generate backtranslations\n",
    "!fairseq-generate data-bin_original_man_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_man_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "!fairseq-generate data-bin_original_woman_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_woman_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "!fairseq-generate data-bin_original_girl_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_girl_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "!fairseq-generate data-bin_original_guy_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_guy_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "!fairseq-generate data-bin_original_boy_de-en  \\\n",
    "    --task translation \\\n",
    "    --source-lang de \\\n",
    "    --target-lang en \\\n",
    "    --path $MODELS/model1.pt:$MODELS/model2.pt:$MODELS/model3.pt:$MODELS/model4.pt \\\n",
    "    --beam $BEAM \\\n",
    "    --nbest $NBEST \\\n",
    "    --batch-size 8 \\\n",
    "    --memory-efficient-fp16 \\\n",
    "    --remove-bpe > original_boy_de-en.decode_Beam_10_backtranslation.log\n",
    "\n",
    "print('Finished translation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd64bb5b-f01d-43e6-9b52-ca4a000040c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LC_ALL=C sort -V' sorts the results in natural order \n",
    "!grep ^H original_man_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_man_back.txt\n",
    "!grep ^H original_woman_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_woman_back.txt\n",
    "!grep ^H original_girl_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_girl_back.txt\n",
    "!grep ^H original_guy_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_guy_back.txt\n",
    "!grep ^H original_boy_de-en.decode_Beam_10_backtranslation.log | LC_ALL=C sort -V | sed 's/^H-//g' | cut -f 3 | sed 's/ @@//g' > ./hyp_original_boy_back.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1cc9b77-86fc-4604-9d6c-6c6a3eea94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished detokenizing.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize text        \n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from __future__ import print_function\n",
    "\n",
    "md_en = MosesDetokenizer(lang='en')\n",
    "\n",
    "with open('hyp_original_man_back.txt', encoding='utf8') as fin, open('original_man_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_original_woman_back.txt', encoding='utf8') as fin, open('original_woman_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_original_girl_back.txt', encoding='utf8') as fin, open('original_girl_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_original_guy_back.txt', encoding='utf8') as fin, open('original_guy_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "        \n",
    "with open('hyp_original_boy_back.txt', encoding='utf8') as fin, open('original_boy_back.txt','w', encoding='utf8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = md_en.detokenize(line.split(), return_str=True)\n",
    "        print(tokens, end='\\n', file=fout)\n",
    "\n",
    "print('Finished detokenizing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a4d60-80fb-4ce1-b305-4dcaa5ddbbb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Statistics on translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab7a65c-57d3-4bd9-883c-b6166d8dc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n",
      "335\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source_man = []\n",
    "with open('en_original_man.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_man.append(line.strip())\n",
    "        \n",
    "source_woman = []\n",
    "with open('en_original_woman.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_woman.append(line.strip())\n",
    "        \n",
    "source_girl = []\n",
    "with open('en_original_girl.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_girl.append(line.strip())\n",
    "        \n",
    "source_guy = []\n",
    "with open('en_original_guy.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_guy.append(line.strip())\n",
    "        \n",
    "source_boy = []\n",
    "with open('en_original_boy.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_boy.append(line.strip())\n",
    "    \n",
    "# List with nbest sentences for every source in original\n",
    "nbest_original_man = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_man.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original_man.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_woman = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_woman.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original_woman.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_girl = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_girl.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original_girl.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_guy = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_guy.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original_guy.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_boy = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('hyp_original_boy.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 100):\n",
    "            nbest_original_boy.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "        \n",
    "            \n",
    "print(len(source_man))\n",
    "print(len(nbest_original_man))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce374900-43dd-484f-bd18-8f8c05e524d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b14270ea-45c3-41ba-a00a-8e3eba27ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique sentences in source nbest list for each source sentence\n",
    "def count_unique_sentences(nbest_sentences):\n",
    "    unique_sent = []\n",
    "    for source_nbest in nbest_sentences:\n",
    "        num_values = len(set(source_nbest))\n",
    "        #print(num_values)\n",
    "        unique_sent.append(num_values)\n",
    "\n",
    "    #print(unique_sent)\n",
    "    return sum(unique_sent)/len(nbest_sentences) # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e76e4e7a-74ad-405a-8fac-1bfd927999ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.37910447761195\n",
      "99.3223880597015\n",
      "99.27164179104477\n",
      "99.62388059701493\n",
      "99.38805970149254\n",
      "Average: \n",
      "99.39701492537314\n"
     ]
    }
   ],
   "source": [
    "# Value should be 10, because beam search generates 10 unique sentences\n",
    "print(count_unique_sentences(nbest_original_man))\n",
    "print(count_unique_sentences(nbest_original_woman))\n",
    "print(count_unique_sentences(nbest_original_girl))\n",
    "print(count_unique_sentences(nbest_original_guy))\n",
    "print(count_unique_sentences(nbest_original_boy))\n",
    "print(\"Average: \")\n",
    "print((count_unique_sentences(nbest_original_man) + count_unique_sentences(nbest_original_woman) + \n",
    "      count_unique_sentences(nbest_original_girl) + count_unique_sentences(nbest_original_guy) + count_unique_sentences(nbest_original_boy))/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c0c73-c1d3-4ca9-9f3f-151c0e86f17f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fc503a0-e64c-479f-9366-86e8a53c0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words in source nbest list for each source sentence of original\n",
    "# !!! Method is slow\n",
    "import spacy\n",
    "\n",
    "def count_unique_words(nbest_sentences):\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    \n",
    "    unique_words = []\n",
    "    normalizer = 0 # should normalize based on total number of words, because disambiguated sentences have more words overall\n",
    "    counter = 0\n",
    "    for source_nbest in nbest_sentences:\n",
    "        words = set()\n",
    "        for sent in source_nbest:\n",
    "            tokens = sp(sent)\n",
    "            normalizer += len(tokens)\n",
    "            for token in tokens:\n",
    "                # if token.text not in stopwords:    # checking whether the word is a stop word\n",
    "                    words.add(token.text)\n",
    "        num_values = len(words)\n",
    "        unique_words.append(num_values)\n",
    "\n",
    "        counter += 1\n",
    "        #print(counter)\n",
    "\n",
    "    #print(unique_words)\n",
    "    print('Normalizer: ' + str(normalizer/len(nbest_sentences)))\n",
    "    return (sum(unique_words)/len(nbest_sentences), (sum(unique_words)/len(nbest_sentences))/(normalizer/len(nbest_sentences))) # (average, norm average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef4217fa-a2ad-464c-b6a6-7ad3b2dabf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: 74.26865671641791\n",
      "(14.797014925373134, 0.19923633440514468)\n",
      "Normalizer: 74.6\n",
      "(15.0, 0.20107238605898126)\n",
      "Normalizer: 74.4955223880597\n",
      "(15.507462686567164, 0.20816637281615644)\n",
      "Normalizer: 73.65074626865672\n",
      "(14.35223880597015, 0.1948688850160094)\n",
      "Normalizer: 74.1731343283582\n",
      "(15.364179104477612, 0.20713940759819707)\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words(nbest_original_man))\n",
    "print(count_unique_words(nbest_original_woman))\n",
    "print(count_unique_words(nbest_original_girl))\n",
    "print(count_unique_words(nbest_original_guy))\n",
    "print(count_unique_words(nbest_original_boy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95394217-96d3-4ea1-aa9b-65cec09c3be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics on backtranslations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b8b4177-38a5-4294-8c2a-619c63fab031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n",
      "335\n"
     ]
    }
   ],
   "source": [
    "# List with original source sentences\n",
    "source_man = []\n",
    "with open('en_original_man.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_man.append(line.strip())\n",
    "        \n",
    "source_woman = []\n",
    "with open('en_original_woman.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_woman.append(line.strip())\n",
    "        \n",
    "source_girl = []\n",
    "with open('en_original_girl.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_girl.append(line.strip())\n",
    "        \n",
    "source_guy = []\n",
    "with open('en_original_guy.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_guy.append(line.strip())\n",
    "        \n",
    "source_boy = []\n",
    "with open('en_original_boy.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        source_boy.append(line.strip())\n",
    "    \n",
    "# List with nbest sentences for every source in original\n",
    "nbest_original_man = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_man_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original_man.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_woman = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_woman_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original_woman.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_girl = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_girl_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original_girl.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_guy = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_guy_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original_guy.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "            \n",
    "nbest_original_boy = []\n",
    "counter = 0\n",
    "temp = []\n",
    "with open('original_boy_back.txt', 'r') as fin:\n",
    "    for line in fin:\n",
    "        temp.append(line.strip())\n",
    "        counter += 1\n",
    "        if (counter == 10000):\n",
    "            nbest_original_boy.append(temp)\n",
    "            counter = 0\n",
    "            temp = []\n",
    "        \n",
    "            \n",
    "print(len(source_man))\n",
    "print(len(nbest_original_man))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f66cfb-d1bc-4b83-8f19-8ab4b21303db",
   "metadata": {},
   "source": [
    "## Source sentence reoccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "618bb74e-385d-443a-b376-e949e99e2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many of the source sentences reoccur in the backtranslation\n",
    "def count_sentence_reoccurrence(source_sentences, nbest_sentences):\n",
    "    results = []\n",
    "    counter = 0\n",
    "    for sent in source_sentences:\n",
    "        matches = 0\n",
    "        for target in nbest_sentences[counter]: \n",
    "            if (sent == target):\n",
    "                matches += 1\n",
    "        results.append(matches)  \n",
    "        counter += 1\n",
    "\n",
    "    #print(sum(results)/len(source_sentences)) # average reoccurence per sentence for 100 backtranslations; x/100\n",
    "\n",
    "    #return sum(x > 0 for x in results)\n",
    "    return sum(results)/len(source_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b61cd08-a1ec-475a-a2d7-694d11b0dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.14626865671642\n",
      "71.25970149253732\n",
      "74.34925373134328\n",
      "62.808955223880595\n",
      "74.12835820895522\n",
      "Average: \n",
      "70.93850746268656\n"
     ]
    }
   ],
   "source": [
    "print(count_sentence_reoccurrence(source_man, nbest_original_man))\n",
    "print(count_sentence_reoccurrence(source_woman, nbest_original_woman))\n",
    "print(count_sentence_reoccurrence(source_girl, nbest_original_girl))\n",
    "print(count_sentence_reoccurrence(source_guy, nbest_original_guy))\n",
    "print(count_sentence_reoccurrence(source_boy, nbest_original_boy))\n",
    "print(\"Average: \")\n",
    "print((count_sentence_reoccurrence(source_man, nbest_original_man) + count_sentence_reoccurrence(source_woman, nbest_original_woman) + \n",
    "      count_sentence_reoccurrence(source_girl, nbest_original_girl) + count_sentence_reoccurrence(source_guy, nbest_original_guy) +\n",
    "      count_sentence_reoccurrence(source_boy, nbest_original_boy))/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecb949-c44c-42bd-8076-31893262c777",
   "metadata": {},
   "source": [
    "## Ambiguous source words reoccurrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba9666d2-f3b2-4017-8214-3113ff7cff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many of the ambiguous words reoccur in the backtranslation\n",
    "def count_words_reoccurrence(ambiguous_words, nbest_sentences):\n",
    "    results = []\n",
    "    counter = 0\n",
    "    for word in ambiguous_words:\n",
    "        matches = 0\n",
    "        for target in nbest_sentences[counter]: \n",
    "            if (word in target.split(\" \")):\n",
    "                matches += 1\n",
    "        results.append(matches)  \n",
    "        counter += 1\n",
    "    \n",
    "    #print(sum(results)/len(ambiguous_words)) # average reoccurence per sentence for 100 backtranslations; x/100\n",
    "\n",
    "    #return sum(x > 0 for x in results)\n",
    "    return sum(results)/len(ambiguous_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93567f32-0885-419c-baf3-b4eb772c4e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7015.307462686567\n",
      "5640.391044776119\n",
      "7441.8537313432835\n",
      "3548.6238805970147\n",
      "5983.722388059701\n",
      "Average: \n",
      "5925.979701492537\n"
     ]
    }
   ],
   "source": [
    "print(count_words_reoccurrence([\"man\"] * 335, nbest_original_man))\n",
    "print(count_words_reoccurrence([\"woman\"] * 335, nbest_original_woman))\n",
    "print(count_words_reoccurrence([\"girl\"] * 335, nbest_original_girl))\n",
    "print(count_words_reoccurrence([\"guy\"] * 335, nbest_original_guy))\n",
    "print(count_words_reoccurrence([\"boy\"] * 335, nbest_original_boy))\n",
    "print(\"Average: \")\n",
    "print((count_words_reoccurrence([\"man\"] * 335, nbest_original_man) + count_words_reoccurrence([\"woman\"] * 335, nbest_original_woman) +\n",
    "      count_words_reoccurrence([\"girl\"] * 335, nbest_original_girl) + count_words_reoccurrence([\"guy\"] * 335, nbest_original_guy) + \n",
    "      count_words_reoccurrence([\"boy\"] * 335, nbest_original_boy))/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628352f6-7bc2-4f3e-b4c9-72eddb35be6b",
   "metadata": {},
   "source": [
    "## Count unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd2f4488-4207-430b-8e53-00ba33560ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3203.1164179104476\n",
      "3528.6089552238805\n",
      "3497.716417910448\n",
      "3074.0119402985074\n",
      "3183.9671641791047\n",
      "Average: \n",
      "3297.4841791044773\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_sentences(nbest_original_man))\n",
    "print(count_unique_sentences(nbest_original_woman))\n",
    "print(count_unique_sentences(nbest_original_girl))\n",
    "print(count_unique_sentences(nbest_original_guy))\n",
    "print(count_unique_sentences(nbest_original_boy))\n",
    "print(\"Average: \")\n",
    "print((count_unique_sentences(nbest_original_man) + count_unique_sentences(nbest_original_woman) + \n",
    "      count_unique_sentences(nbest_original_girl) + count_unique_sentences(nbest_original_guy) + count_unique_sentences(nbest_original_boy))/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265064a1-379a-4b66-ad78-61dcd8735726",
   "metadata": {},
   "source": [
    "## Count unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe3fa560-d25d-401f-a776-c3080761d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: 717.4835820895522\n",
      "(32.48656716417911, 0.04527848159196529)\n",
      "Normalizer: 718.3492537313433\n",
      "(32.28059701492537, 0.04493719015819852)\n",
      "Normalizer: 730.1014925373134\n",
      "(34.31044776119403, 0.04699407974356458)\n",
      "Normalizer: 721.6417910447761\n",
      "(31.253731343283583, 0.04330920372285419)\n",
      "Normalizer: 721.534328358209\n",
      "(32.06865671641791, 0.044445087996557914)\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words(nbest_original_man))\n",
    "print(count_unique_words(nbest_original_woman))\n",
    "print(count_unique_words(nbest_original_girl))\n",
    "print(count_unique_words(nbest_original_guy))\n",
    "print(count_unique_words(nbest_original_boy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a31ec1-6057-4efc-9cb5-0958d64c2bc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (source-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f2bbb-6aa4-47fa-89bd-3855b164c388",
   "metadata": {},
   "source": [
    "- Input to fast_align must be tokenized and aligned into parallel sentences. \n",
    "- Line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space (|||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3cba97b-037d-4e23-8f09-e1470cf6c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alignment_input(sentencesN, sourceIn, targetIn, output):\n",
    "    # List with original source sentences\n",
    "    source = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    # List with nbest sentences for every source in original \n",
    "    target = []\n",
    "    counter = 0\n",
    "    temp = []\n",
    "    with open(targetIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            temp.append(line.strip())\n",
    "            counter += 1\n",
    "            if (counter == 100):\n",
    "                target.append(temp)\n",
    "                counter = 0\n",
    "                temp = []\n",
    "\n",
    "    #print(len(source))\n",
    "    #print(len(target))           \n",
    "\n",
    "    count = 0\n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            for hyp in target[count]:\n",
    "                print(source[count] + ' ||| ' + hyp, end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b13acbaf-db12-49d2-a25b-1ce4d3615989",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_alignment_input(335, 'tok.en_original_man.en', 'hyp_original_man.txt', 'original_man_source-target_en-de.txt')\n",
    "build_alignment_input(335, 'tok.en_original_woman.en', 'hyp_original_woman.txt', 'original_woman_source-target_en-de.txt')\n",
    "build_alignment_input(335, 'tok.en_original_girl.en', 'hyp_original_girl.txt', 'original_girl_source-target_en-de.txt')\n",
    "build_alignment_input(335, 'tok.en_original_guy.en', 'hyp_original_guy.txt', 'original_guy_source-target_en-de.txt')\n",
    "build_alignment_input(335, 'tok.en_original_boy.en', 'hyp_original_boy.txt', 'original_boy_source-target_en-de.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e0a48-e74f-468a-b949-a9cc13a2a519",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fast_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1477d44-1efd-4dda-8549-8b29c438e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 1.04276\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.15419e+06\n",
      "  log_2 likelihood: -7.43592e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.19742\n",
      "       size counts: 81\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -772651\n",
      "  log_2 likelihood: -1.1147e+06\n",
      "     cross entropy: 4.48183\n",
      "        perplexity: 22.3443\n",
      "      posterior p0: 0.0343571\n",
      " posterior al-feat: -0.16318\n",
      "       size counts: 81\n",
      "  1  model al-feat: -0.159921 (tension=4)\n",
      "  2  model al-feat: -0.161566 (tension=3.93483)\n",
      "  3  model al-feat: -0.162389 (tension=3.90256)\n",
      "  4  model al-feat: -0.162794 (tension=3.88674)\n",
      "  5  model al-feat: -0.162992 (tension=3.87904)\n",
      "  6  model al-feat: -0.163089 (tension=3.87529)\n",
      "  7  model al-feat: -0.163135 (tension=3.87347)\n",
      "  8  model al-feat: -0.163158 (tension=3.87259)\n",
      "     final tension: 3.87216\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -667372\n",
      "  log_2 likelihood: -962814\n",
      "     cross entropy: 3.87115\n",
      "        perplexity: 14.633\n",
      "      posterior p0: 0.0365483\n",
      " posterior al-feat: -0.154625\n",
      "       size counts: 81\n",
      "  1  model al-feat: -0.163169 (tension=3.87216)\n",
      "  2  model al-feat: -0.158847 (tension=4.04303)\n",
      "  3  model al-feat: -0.156768 (tension=4.12746)\n",
      "  4  model al-feat: -0.155727 (tension=4.17031)\n",
      "  5  model al-feat: -0.155195 (tension=4.19233)\n",
      "  6  model al-feat: -0.154921 (tension=4.20373)\n",
      "  7  model al-feat: -0.154779 (tension=4.20964)\n",
      "  8  model al-feat: -0.154705 (tension=4.21272)\n",
      "     final tension: 4.21432\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -633106\n",
      "  log_2 likelihood: -913379\n",
      "     cross entropy: 3.67239\n",
      "        perplexity: 12.7497\n",
      "      posterior p0: 0.0370762\n",
      " posterior al-feat: -0.151536\n",
      "       size counts: 81\n",
      "  1  model al-feat: -0.154667 (tension=4.21432)\n",
      "  2  model al-feat: -0.153176 (tension=4.27694)\n",
      "  3  model al-feat: -0.152403 (tension=4.30975)\n",
      "  4  model al-feat: -0.151996 (tension=4.3271)\n",
      "  5  model al-feat: -0.151781 (tension=4.33631)\n",
      "  6  model al-feat: -0.151667 (tension=4.34122)\n",
      "  7  model al-feat: -0.151606 (tension=4.34383)\n",
      "  8  model al-feat: -0.151573 (tension=4.34523)\n",
      "     final tension: 4.34597\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -618944\n",
      "  log_2 likelihood: -892947\n",
      "     cross entropy: 3.59024\n",
      "        perplexity: 12.044\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 81\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 1.05416\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.2044e+06\n",
      "  log_2 likelihood: -7.50836e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.196797\n",
      "       size counts: 76\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -779854\n",
      "  log_2 likelihood: -1.12509e+06\n",
      "     cross entropy: 4.47997\n",
      "        perplexity: 22.3155\n",
      "      posterior p0: 0.034603\n",
      " posterior al-feat: -0.161017\n",
      "       size counts: 76\n",
      "  1  model al-feat: -0.158118 (tension=4)\n",
      "  2  model al-feat: -0.159568 (tension=3.94203)\n",
      "  3  model al-feat: -0.160299 (tension=3.91305)\n",
      "  4  model al-feat: -0.160663 (tension=3.89869)\n",
      "  5  model al-feat: -0.160843 (tension=3.89161)\n",
      "  6  model al-feat: -0.160931 (tension=3.88813)\n",
      "  7  model al-feat: -0.160975 (tension=3.88642)\n",
      "  8  model al-feat: -0.160996 (tension=3.88558)\n",
      "     final tension: 3.88516\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -673258\n",
      "  log_2 likelihood: -971306\n",
      "     cross entropy: 3.86762\n",
      "        perplexity: 14.5972\n",
      "      posterior p0: 0.0363014\n",
      " posterior al-feat: -0.151947\n",
      "       size counts: 76\n",
      "  1  model al-feat: -0.161007 (tension=3.88516)\n",
      "  2  model al-feat: -0.15648 (tension=4.06635)\n",
      "  3  model al-feat: -0.154279 (tension=4.15701)\n",
      "  4  model al-feat: -0.153163 (tension=4.20364)\n",
      "  5  model al-feat: -0.152585 (tension=4.22795)\n",
      "  6  model al-feat: -0.152283 (tension=4.24071)\n",
      "  7  model al-feat: -0.152125 (tension=4.24743)\n",
      "  8  model al-feat: -0.152041 (tension=4.25098)\n",
      "     final tension: 4.25285\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -638002\n",
      "  log_2 likelihood: -920442\n",
      "     cross entropy: 3.66509\n",
      "        perplexity: 12.6853\n",
      "      posterior p0: 0.0364341\n",
      " posterior al-feat: -0.148743\n",
      "       size counts: 76\n",
      "  1  model al-feat: -0.151997 (tension=4.25285)\n",
      "  2  model al-feat: -0.150473 (tension=4.31793)\n",
      "  3  model al-feat: -0.149672 (tension=4.35254)\n",
      "  4  model al-feat: -0.149244 (tension=4.37111)\n",
      "  5  model al-feat: -0.149014 (tension=4.38113)\n",
      "  6  model al-feat: -0.14889 (tension=4.38655)\n",
      "  7  model al-feat: -0.148823 (tension=4.38948)\n",
      "  8  model al-feat: -0.148786 (tension=4.39107)\n",
      "     final tension: 4.39193\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -623514\n",
      "  log_2 likelihood: -899540\n",
      "     cross entropy: 3.58186\n",
      "        perplexity: 11.9742\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 76\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 1.04921\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.18591e+06\n",
      "  log_2 likelihood: -7.48169e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.19691\n",
      "       size counts: 77\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -780661\n",
      "  log_2 likelihood: -1.12626e+06\n",
      "     cross entropy: 4.50059\n",
      "        perplexity: 22.6367\n",
      "      posterior p0: 0.034757\n",
      " posterior al-feat: -0.161054\n",
      "       size counts: 77\n",
      "  1  model al-feat: -0.158557 (tension=4)\n",
      "  2  model al-feat: -0.159813 (tension=3.95006)\n",
      "  3  model al-feat: -0.160442 (tension=3.92524)\n",
      "  4  model al-feat: -0.160754 (tension=3.91299)\n",
      "  5  model al-feat: -0.160907 (tension=3.90698)\n",
      "  6  model al-feat: -0.160982 (tension=3.90403)\n",
      "  7  model al-feat: -0.161019 (tension=3.90258)\n",
      "  8  model al-feat: -0.161037 (tension=3.90187)\n",
      "     final tension: 3.90153\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -673115\n",
      "  log_2 likelihood: -971099\n",
      "     cross entropy: 3.88058\n",
      "        perplexity: 14.7289\n",
      "      posterior p0: 0.0365585\n",
      " posterior al-feat: -0.151945\n",
      "       size counts: 77\n",
      "  1  model al-feat: -0.161046 (tension=3.90153)\n",
      "  2  model al-feat: -0.156486 (tension=4.08354)\n",
      "  3  model al-feat: -0.154275 (tension=4.17436)\n",
      "  4  model al-feat: -0.153157 (tension=4.22095)\n",
      "  5  model al-feat: -0.152579 (tension=4.24518)\n",
      "  6  model al-feat: -0.152278 (tension=4.25787)\n",
      "  7  model al-feat: -0.152121 (tension=4.26454)\n",
      "  8  model al-feat: -0.152038 (tension=4.26805)\n",
      "     final tension: 4.2699\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -637772\n",
      "  log_2 likelihood: -920110\n",
      "     cross entropy: 3.67682\n",
      "        perplexity: 12.7889\n",
      "      posterior p0: 0.0368901\n",
      " posterior al-feat: -0.148677\n",
      "       size counts: 77\n",
      "  1  model al-feat: -0.151994 (tension=4.2699)\n",
      "  2  model al-feat: -0.150437 (tension=4.33623)\n",
      "  3  model al-feat: -0.14962 (tension=4.37142)\n",
      "  4  model al-feat: -0.149185 (tension=4.39028)\n",
      "  5  model al-feat: -0.148951 (tension=4.40043)\n",
      "  6  model al-feat: -0.148826 (tension=4.40591)\n",
      "  7  model al-feat: -0.148758 (tension=4.40887)\n",
      "  8  model al-feat: -0.148721 (tension=4.41047)\n",
      "     final tension: 4.41134\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -623204\n",
      "  log_2 likelihood: -899093\n",
      "     cross entropy: 3.59284\n",
      "        perplexity: 12.0657\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 77\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 1.02245\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.06935e+06\n",
      "  log_2 likelihood: -7.31352e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.197996\n",
      "       size counts: 78\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -744376\n",
      "  log_2 likelihood: -1.07391e+06\n",
      "     cross entropy: 4.39009\n",
      "        perplexity: 20.9675\n",
      "      posterior p0: 0.0328196\n",
      " posterior al-feat: -0.163063\n",
      "       size counts: 78\n",
      "  1  model al-feat: -0.16205 (tension=4)\n",
      "  2  model al-feat: -0.162573 (tension=3.97973)\n",
      "  3  model al-feat: -0.162827 (tension=3.96994)\n",
      "  4  model al-feat: -0.16295 (tension=3.96522)\n",
      "  5  model al-feat: -0.163008 (tension=3.96295)\n",
      "  6  model al-feat: -0.163037 (tension=3.96186)\n",
      "  7  model al-feat: -0.16305 (tension=3.96133)\n",
      "  8  model al-feat: -0.163057 (tension=3.96108)\n",
      "     final tension: 3.96096\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -638089\n",
      "  log_2 likelihood: -920567\n",
      "     cross entropy: 3.76324\n",
      "        perplexity: 13.5784\n",
      "      posterior p0: 0.0346567\n",
      " posterior al-feat: -0.154676\n",
      "       size counts: 78\n",
      "  1  model al-feat: -0.16306 (tension=3.96096)\n",
      "  2  model al-feat: -0.158779 (tension=4.12863)\n",
      "  3  model al-feat: -0.156739 (tension=4.21069)\n",
      "  4  model al-feat: -0.155727 (tension=4.25195)\n",
      "  5  model al-feat: -0.155215 (tension=4.27296)\n",
      "  6  model al-feat: -0.154953 (tension=4.28373)\n",
      "  7  model al-feat: -0.154819 (tension=4.28927)\n",
      "  8  model al-feat: -0.15475 (tension=4.29212)\n",
      "     final tension: 4.29359\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -604013\n",
      "  log_2 likelihood: -871406\n",
      "     cross entropy: 3.56227\n",
      "        perplexity: 11.8127\n",
      "      posterior p0: 0.0349054\n",
      " posterior al-feat: -0.152076\n",
      "       size counts: 78\n",
      "  1  model al-feat: -0.154714 (tension=4.29359)\n",
      "  2  model al-feat: -0.153444 (tension=4.34636)\n",
      "  3  model al-feat: -0.152791 (tension=4.37372)\n",
      "  4  model al-feat: -0.152451 (tension=4.38802)\n",
      "  5  model al-feat: -0.152273 (tension=4.39553)\n",
      "  6  model al-feat: -0.15218 (tension=4.39947)\n",
      "  7  model al-feat: -0.152131 (tension=4.40155)\n",
      "  8  model al-feat: -0.152105 (tension=4.40265)\n",
      "     final tension: 4.40322\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -590564\n",
      "  log_2 likelihood: -852003\n",
      "     cross entropy: 3.48295\n",
      "        perplexity: 11.1808\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 78\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................\n",
      "expected target length = source length * 1.04219\n",
      "ITERATION 1\n",
      ".................................\n",
      "  log_e likelihood: -5.15375e+06\n",
      "  log_2 likelihood: -7.43529e+06\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.197244\n",
      "       size counts: 77\n",
      "ITERATION 2\n",
      ".................................\n",
      "  log_e likelihood: -775148\n",
      "  log_2 likelihood: -1.1183e+06\n",
      "     cross entropy: 4.4967\n",
      "        perplexity: 22.5757\n",
      "      posterior p0: 0.0342188\n",
      " posterior al-feat: -0.162004\n",
      "       size counts: 77\n",
      "  1  model al-feat: -0.159563 (tension=4)\n",
      "  2  model al-feat: -0.160799 (tension=3.95118)\n",
      "  3  model al-feat: -0.161414 (tension=3.92709)\n",
      "  4  model al-feat: -0.161716 (tension=3.91529)\n",
      "  5  model al-feat: -0.161864 (tension=3.90953)\n",
      "  6  model al-feat: -0.161936 (tension=3.90673)\n",
      "  7  model al-feat: -0.161971 (tension=3.90536)\n",
      "  8  model al-feat: -0.161988 (tension=3.9047)\n",
      "     final tension: 3.90438\n",
      "ITERATION 3\n",
      ".................................\n",
      "  log_e likelihood: -668165\n",
      "  log_2 likelihood: -963958\n",
      "     cross entropy: 3.87608\n",
      "        perplexity: 14.6831\n",
      "      posterior p0: 0.0360111\n",
      " posterior al-feat: -0.153183\n",
      "       size counts: 77\n",
      "  1  model al-feat: -0.161996 (tension=3.90438)\n",
      "  2  model al-feat: -0.157549 (tension=4.08064)\n",
      "  3  model al-feat: -0.155406 (tension=4.16797)\n",
      "  4  model al-feat: -0.15433 (tension=4.21243)\n",
      "  5  model al-feat: -0.153779 (tension=4.23537)\n",
      "  6  model al-feat: -0.153493 (tension=4.24729)\n",
      "  7  model al-feat: -0.153345 (tension=4.2535)\n",
      "  8  model al-feat: -0.153267 (tension=4.25674)\n",
      "     final tension: 4.25844\n",
      "ITERATION 4\n",
      ".................................\n",
      "  log_e likelihood: -633391\n",
      "  log_2 likelihood: -913791\n",
      "     cross entropy: 3.67436\n",
      "        perplexity: 12.7671\n",
      "      posterior p0: 0.0363868\n",
      " posterior al-feat: -0.150075\n",
      "       size counts: 77\n",
      "  1  model al-feat: -0.153227 (tension=4.25844)\n",
      "  2  model al-feat: -0.151734 (tension=4.32147)\n",
      "  3  model al-feat: -0.150956 (tension=4.35463)\n",
      "  4  model al-feat: -0.150545 (tension=4.37224)\n",
      "  5  model al-feat: -0.150327 (tension=4.38163)\n",
      "  6  model al-feat: -0.15021 (tension=4.38665)\n",
      "  7  model al-feat: -0.150148 (tension=4.38934)\n",
      "  8  model al-feat: -0.150114 (tension=4.39078)\n",
      "     final tension: 4.39156\n",
      "ITERATION 5 (FINAL)\n",
      ".................................\n",
      "  log_e likelihood: -619194\n",
      "  log_2 likelihood: -893308\n",
      "     cross entropy: 3.592\n",
      "        perplexity: 12.0587\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 77\n",
      "Finished alignment.\n"
     ]
    }
   ],
   "source": [
    "!$FAST_ALIGN -i original_man_source-target_en-de.txt -d -o -v > original_man_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_woman_source-target_en-de.txt -d -o -v > original_woman_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_girl_source-target_en-de.txt -d -o -v > original_girl_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_guy_source-target_en-de.txt -d -o -v > original_guy_source-target_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_boy_source-target_en-de.txt -d -o -v > original_boy_source-target_en-de_fast-aligned.txt\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2de69fff-155f-44b2-a456-134fce70042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Count unique translated words to the ambiguous words in translations per source sentence\n",
    "def count_unique_words_alignment_translations(position_word, sentencesN, sourceIn, translationsIn, alignmentsIn, output):\n",
    "    \n",
    "    # Get positions of ambigous words\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position_word)\n",
    "        \n",
    "    # List with translations\n",
    "    translations = []\n",
    "    with open(translationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            translations.append(line.strip())\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Extract alginments of ambiguous words\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 100):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                indices.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                indices.append([999])\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = set() # set forces uniqueness\n",
    "    for translation in translations:\n",
    "        tokens = translation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                #print(lineNumber)\n",
    "                #print(tokens[ind])\n",
    "                #print(ind)\n",
    "                translated_ambiguous_words.add(tokens[ind])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "                translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "                translated_ambiguous_words = set()\n",
    "\n",
    "    #print(translations_ambiguous_words)\n",
    "    #print(len(translations_ambiguous_words))\n",
    "    \n",
    "    # Add results to file\n",
    "    ambiguous_words = []\n",
    "    source = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position_word])\n",
    "\n",
    "    count = 0                \n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(translations_ambiguous_words[count]), end='\\n', file=fout)\n",
    "            count += 1\n",
    "\n",
    "    unique_translations = 0\n",
    "    for set_words in translations_ambiguous_words:\n",
    "        \n",
    "        ############################################################\n",
    "        # remove gender info; removing \"in\" and \"e\" endings in words\n",
    "        set_words_new = set()\n",
    "        for word in set_words:\n",
    "            word_new = re.sub(\"in$|e$\", \"\", word)\n",
    "            #print(word_new)\n",
    "            set_words_new.add(word_new)\n",
    "        #print(set_words_new)\n",
    "        ############################################################\n",
    "        \n",
    "        unique_translations += len(set_words_new)\n",
    "        \n",
    "    #print(unique_translations)\n",
    "    return unique_translations/sentencesN # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57d33e67-ceeb-4660-ad40-94619a21f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.232835820895523\n",
      "======\n",
      "5.352238805970149\n",
      "======\n",
      "7.483582089552239\n",
      "======\n",
      "9.57313432835821\n",
      "======\n",
      "9.259701492537314\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_man.en', 'hyp_original_man.txt', 'original_man_source-target_en-de_fast-aligned.txt', 'unique-words_translations_original_man.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_woman.en', 'hyp_original_woman.txt', 'original_woman_source-target_en-de_fast-aligned.txt', 'unique-words_translations_original_woman.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_girl.en', 'hyp_original_girl.txt', 'original_girl_source-target_en-de_fast-aligned.txt', 'unique-words_translations_original_girl.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_guy.en', 'hyp_original_guy.txt', 'original_guy_source-target_en-de_fast-aligned.txt', 'unique-words_translations_original_guy.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_boy.en', 'hyp_original_boy.txt', 'original_boy_source-target_en-de_fast-aligned.txt', 'unique-words_translations_original_boy.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547be75-dfc4-46c3-ba3f-dfe56c4a3037",
   "metadata": {},
   "source": [
    "## awesome_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e5d0cfa-eb33-4a71-af0c-9fa94a473c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 33500it [00:35, 954.26it/s] \n",
      "Loading the dataset...\n",
      "Extracting: 33500it [00:35, 953.53it/s] \n",
      "Loading the dataset...\n",
      "Extracting: 33500it [00:34, 974.20it/s] \n",
      "Loading the dataset...\n",
      "Extracting: 33500it [00:34, 970.45it/s] \n",
      "Loading the dataset...\n",
      "Extracting: 33500it [00:35, 935.12it/s] \n",
      "Finished alignment.\n"
     ]
    }
   ],
   "source": [
    "# ??? How to set model correctly\n",
    "# MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "!awesome-align \\\n",
    "    --output_file \"original_man_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_man_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_woman_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_woman_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_girl_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_girl_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_guy_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_guy_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_boy_source-target_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_boy_source-target_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e72d55fd-918f-4b24-af93-df1cf3b09577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4208955223880597\n",
      "======\n",
      "3.2597014925373133\n",
      "======\n",
      "4.7701492537313435\n",
      "======\n",
      "9.188059701492538\n",
      "======\n",
      "6.647761194029851\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_man.en', 'hyp_original_man.txt', 'original_man_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_original_man.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_woman.en', 'hyp_original_woman.txt', 'original_woman_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_original_woman.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_girl.en', 'hyp_original_girl.txt', 'original_girl_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_original_girl.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_guy.en', 'hyp_original_guy.txt', 'original_guy_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_original_guy.txt'))\n",
    "print('======')\n",
    "print(count_unique_words_alignment_translations(1, 335, 'tok.en_original_boy.en', 'hyp_original_boy.txt', 'original_boy_source-target_en-de_awesome-aligned.txt', 'unique-words_translations_original_boy.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab63c-5750-42b2-b80a-ea767942f356",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (translation-backtranslation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ac057-9ea8-4bff-a824-8f4498ed95a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## fast_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f6e57-70df-4156-99b5-859e397f52b7",
   "metadata": {},
   "source": [
    "- Input to fast_align must be tokenized and aligned into parallel sentences. \n",
    "- Line is a source language sentence and its target language translation, separated by a triple pipe symbol with leading and trailing white space (|||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66afe2b0-c7ce-441b-8071-5a19daf8f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_alignment_input(33500, 'hyp_original_man.txt', 'hyp_original_man_back.txt', 'original_man_translation-back_en-de.txt')\n",
    "build_alignment_input(33500, 'hyp_original_woman.txt', 'hyp_original_woman_back.txt', 'original_woman_translation-back_en-de.txt')\n",
    "build_alignment_input(33500, 'hyp_original_girl.txt', 'hyp_original_girl_back.txt', 'original_girl_translation-back_en-de.txt')\n",
    "build_alignment_input(33500, 'hyp_original_guy.txt', 'hyp_original_guy_back.txt', 'original_guy_translation-back_en-de.txt')\n",
    "build_alignment_input(33500, 'hyp_original_boy.txt', 'hyp_original_boy_back.txt', 'original_boy_translation-back_en-de.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd83be3-c2a7-4245-844c-52f51e416a4d",
   "metadata": {},
   "source": [
    "- Word alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a6552ff-349f-4048-b65b-4c4959901ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "expected target length = source length * 0.99933\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.07827e+08\n",
      "  log_2 likelihood: -7.32639e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.197774\n",
      "       size counts: 172\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -7.53512e+07\n",
      "  log_2 likelihood: -1.08709e+08\n",
      "     cross entropy: 4.43616\n",
      "        perplexity: 21.648\n",
      "      posterior p0: 0.034383\n",
      " posterior al-feat: -0.166341\n",
      "       size counts: 172\n",
      "  1  model al-feat: -0.168561 (tension=4)\n",
      "  2  model al-feat: -0.16739 (tension=4.0444)\n",
      "  3  model al-feat: -0.166841 (tension=4.06538)\n",
      "  4  model al-feat: -0.16658 (tension=4.07536)\n",
      "  5  model al-feat: -0.166455 (tension=4.08014)\n",
      "  6  model al-feat: -0.166396 (tension=4.08242)\n",
      "  7  model al-feat: -0.166367 (tension=4.08351)\n",
      "  8  model al-feat: -0.166354 (tension=4.08404)\n",
      "     final tension: 4.08429\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.46416e+07\n",
      "  log_2 likelihood: -9.3258e+07\n",
      "     cross entropy: 3.80565\n",
      "        perplexity: 13.9835\n",
      "      posterior p0: 0.0355622\n",
      " posterior al-feat: -0.155728\n",
      "       size counts: 172\n",
      "  1  model al-feat: -0.166347 (tension=4.08429)\n",
      "  2  model al-feat: -0.160944 (tension=4.29668)\n",
      "  3  model al-feat: -0.15838 (tension=4.40101)\n",
      "  4  model al-feat: -0.157099 (tension=4.45406)\n",
      "  5  model al-feat: -0.156442 (tension=4.48148)\n",
      "  6  model al-feat: -0.156102 (tension=4.49578)\n",
      "  7  model al-feat: -0.155924 (tension=4.50325)\n",
      "  8  model al-feat: -0.155831 (tension=4.50718)\n",
      "     final tension: 4.50924\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.11745e+07\n",
      "  log_2 likelihood: -8.82561e+07\n",
      "     cross entropy: 3.60153\n",
      "        perplexity: 12.1386\n",
      "      posterior p0: 0.0361473\n",
      " posterior al-feat: -0.151274\n",
      "       size counts: 172\n",
      "  1  model al-feat: -0.155782 (tension=4.50924)\n",
      "  2  model al-feat: -0.153664 (tension=4.59939)\n",
      "  3  model al-feat: -0.152558 (tension=4.64719)\n",
      "  4  model al-feat: -0.151969 (tension=4.67287)\n",
      "  5  model al-feat: -0.151651 (tension=4.68676)\n",
      "  6  model al-feat: -0.151479 (tension=4.6943)\n",
      "  7  model al-feat: -0.151386 (tension=4.6984)\n",
      "  8  model al-feat: -0.151335 (tension=4.70064)\n",
      "     final tension: 4.70186\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.97928e+07\n",
      "  log_2 likelihood: -8.62628e+07\n",
      "     cross entropy: 3.52019\n",
      "        perplexity: 11.4732\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 172\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "expected target length = source length * 0.995062\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.11007e+08\n",
      "  log_2 likelihood: -7.37227e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.19716\n",
      "       size counts: 165\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -7.63217e+07\n",
      "  log_2 likelihood: -1.10109e+08\n",
      "     cross entropy: 4.46533\n",
      "        perplexity: 22.0902\n",
      "      posterior p0: 0.0340133\n",
      " posterior al-feat: -0.165995\n",
      "       size counts: 165\n",
      "  1  model al-feat: -0.169201 (tension=4)\n",
      "  2  model al-feat: -0.167504 (tension=4.06412)\n",
      "  3  model al-feat: -0.166713 (tension=4.0943)\n",
      "  4  model al-feat: -0.166339 (tension=4.10866)\n",
      "  5  model al-feat: -0.16616 (tension=4.11554)\n",
      "  6  model al-feat: -0.166074 (tension=4.11884)\n",
      "  7  model al-feat: -0.166033 (tension=4.12043)\n",
      "  8  model al-feat: -0.166013 (tension=4.12119)\n",
      "     final tension: 4.12155\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.53731e+07\n",
      "  log_2 likelihood: -9.43134e+07\n",
      "     cross entropy: 3.82477\n",
      "        perplexity: 14.17\n",
      "      posterior p0: 0.0349605\n",
      " posterior al-feat: -0.155357\n",
      "       size counts: 165\n",
      "  1  model al-feat: -0.166004 (tension=4.12155)\n",
      "  2  model al-feat: -0.160603 (tension=4.33449)\n",
      "  3  model al-feat: -0.158033 (tension=4.43941)\n",
      "  4  model al-feat: -0.156745 (tension=4.49293)\n",
      "  5  model al-feat: -0.156082 (tension=4.52068)\n",
      "  6  model al-feat: -0.155738 (tension=4.53519)\n",
      "  7  model al-feat: -0.155557 (tension=4.54281)\n",
      "  8  model al-feat: -0.155463 (tension=4.54682)\n",
      "     final tension: 4.54893\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.18614e+07\n",
      "  log_2 likelihood: -8.92472e+07\n",
      "     cross entropy: 3.61931\n",
      "        perplexity: 12.2891\n",
      "      posterior p0: 0.035389\n",
      " posterior al-feat: -0.150912\n",
      "       size counts: 165\n",
      "  1  model al-feat: -0.155413 (tension=4.54893)\n",
      "  2  model al-feat: -0.153306 (tension=4.63895)\n",
      "  3  model al-feat: -0.152202 (tension=4.68683)\n",
      "  4  model al-feat: -0.151612 (tension=4.71264)\n",
      "  5  model al-feat: -0.151293 (tension=4.72665)\n",
      "  6  model al-feat: -0.15112 (tension=4.73427)\n",
      "  7  model al-feat: -0.151025 (tension=4.73844)\n",
      "  8  model al-feat: -0.150974 (tension=4.74071)\n",
      "     final tension: 4.74196\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.04645e+07\n",
      "  log_2 likelihood: -8.72318e+07\n",
      "     cross entropy: 3.53758\n",
      "        perplexity: 11.6123\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 165\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "expected target length = source length * 1.01292\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.1745e+08\n",
      "  log_2 likelihood: -7.46522e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.196713\n",
      "       size counts: 162\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -7.64972e+07\n",
      "  log_2 likelihood: -1.10362e+08\n",
      "     cross entropy: 4.41988\n",
      "        perplexity: 21.405\n",
      "      posterior p0: 0.0347652\n",
      " posterior al-feat: -0.164694\n",
      "       size counts: 162\n",
      "  1  model al-feat: -0.166554 (tension=4)\n",
      "  2  model al-feat: -0.165587 (tension=4.03721)\n",
      "  3  model al-feat: -0.165126 (tension=4.05509)\n",
      "  4  model al-feat: -0.164903 (tension=4.06373)\n",
      "  5  model al-feat: -0.164795 (tension=4.06792)\n",
      "  6  model al-feat: -0.164743 (tension=4.06995)\n",
      "  7  model al-feat: -0.164718 (tension=4.07094)\n",
      "  8  model al-feat: -0.164705 (tension=4.07142)\n",
      "     final tension: 4.07166\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.5632e+07\n",
      "  log_2 likelihood: -9.4687e+07\n",
      "     cross entropy: 3.7921\n",
      "        perplexity: 13.8528\n",
      "      posterior p0: 0.0361756\n",
      " posterior al-feat: -0.153769\n",
      "       size counts: 162\n",
      "  1  model al-feat: -0.164699 (tension=4.07166)\n",
      "  2  model al-feat: -0.159213 (tension=4.29027)\n",
      "  3  model al-feat: -0.156576 (tension=4.39917)\n",
      "  4  model al-feat: -0.155241 (tension=4.45532)\n",
      "  5  model al-feat: -0.154547 (tension=4.48476)\n",
      "  6  model al-feat: -0.154182 (tension=4.50033)\n",
      "  7  model al-feat: -0.153988 (tension=4.50859)\n",
      "  8  model al-feat: -0.153886 (tension=4.51299)\n",
      "     final tension: 4.51534\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.20733e+07\n",
      "  log_2 likelihood: -8.95529e+07\n",
      "     cross entropy: 3.58649\n",
      "        perplexity: 12.0127\n",
      "      posterior p0: 0.0368146\n",
      " posterior al-feat: -0.149016\n",
      "       size counts: 162\n",
      "  1  model al-feat: -0.153831 (tension=4.51534)\n",
      "  2  model al-feat: -0.151607 (tension=4.61165)\n",
      "  3  model al-feat: -0.15043 (tension=4.66348)\n",
      "  4  model al-feat: -0.149793 (tension=4.69177)\n",
      "  5  model al-feat: -0.149444 (tension=4.70731)\n",
      "  6  model al-feat: -0.149253 (tension=4.71589)\n",
      "  7  model al-feat: -0.149147 (tension=4.72063)\n",
      "  8  model al-feat: -0.149088 (tension=4.72326)\n",
      "     final tension: 4.72471\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.06266e+07\n",
      "  log_2 likelihood: -8.74656e+07\n",
      "     cross entropy: 3.5029\n",
      "        perplexity: 11.3365\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 162\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "expected target length = source length * 1.01015\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.03655e+08\n",
      "  log_2 likelihood: -7.2662e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.198353\n",
      "       size counts: 161\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -7.40803e+07\n",
      "  log_2 likelihood: -1.06875e+08\n",
      "     cross entropy: 4.39747\n",
      "        perplexity: 21.0751\n",
      "      posterior p0: 0.0318418\n",
      " posterior al-feat: -0.165234\n",
      "       size counts: 161\n",
      "  1  model al-feat: -0.166545 (tension=4)\n",
      "  2  model al-feat: -0.165856 (tension=4.02621)\n",
      "  3  model al-feat: -0.16553 (tension=4.03863)\n",
      "  4  model al-feat: -0.165376 (tension=4.04455)\n",
      "  5  model al-feat: -0.165302 (tension=4.04738)\n",
      "  6  model al-feat: -0.165267 (tension=4.04873)\n",
      "  7  model al-feat: -0.16525 (tension=4.04937)\n",
      "  8  model al-feat: -0.165242 (tension=4.04968)\n",
      "     final tension: 4.04983\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.30616e+07\n",
      "  log_2 likelihood: -9.09787e+07\n",
      "     cross entropy: 3.74339\n",
      "        perplexity: 13.3928\n",
      "      posterior p0: 0.0334023\n",
      " posterior al-feat: -0.154297\n",
      "       size counts: 161\n",
      "  1  model al-feat: -0.165238 (tension=4.04983)\n",
      "  2  model al-feat: -0.159662 (tension=4.26864)\n",
      "  3  model al-feat: -0.157023 (tension=4.37593)\n",
      "  4  model al-feat: -0.155705 (tension=4.43044)\n",
      "  5  model al-feat: -0.155031 (tension=4.4586)\n",
      "  6  model al-feat: -0.154681 (tension=4.47327)\n",
      "  7  model al-feat: -0.154499 (tension=4.48095)\n",
      "  8  model al-feat: -0.154403 (tension=4.48497)\n",
      "     final tension: 4.48709\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.949e+07\n",
      "  log_2 likelihood: -8.58259e+07\n",
      "     cross entropy: 3.53137\n",
      "        perplexity: 11.5624\n",
      "      posterior p0: 0.034627\n",
      " posterior al-feat: -0.149898\n",
      "       size counts: 161\n",
      "  1  model al-feat: -0.154353 (tension=4.48709)\n",
      "  2  model al-feat: -0.152259 (tension=4.57619)\n",
      "  3  model al-feat: -0.151165 (tension=4.62341)\n",
      "  4  model al-feat: -0.150583 (tension=4.64877)\n",
      "  5  model al-feat: -0.150269 (tension=4.66248)\n",
      "  6  model al-feat: -0.1501 (tension=4.66992)\n",
      "  7  model al-feat: -0.150008 (tension=4.67396)\n",
      "  8  model al-feat: -0.149958 (tension=4.67616)\n",
      "     final tension: 4.67736\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.81405e+07\n",
      "  log_2 likelihood: -8.3879e+07\n",
      "     cross entropy: 3.45127\n",
      "        perplexity: 10.9379\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 161\n",
      "ARG=i\n",
      "ARG=d\n",
      "ARG=o\n",
      "ARG=v\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "expected target length = source length * 1.00905\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.11953e+08\n",
      "  log_2 likelihood: -7.38592e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.08\n",
      " posterior al-feat: -0.197254\n",
      "       size counts: 164\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -7.53807e+07\n",
      "  log_2 likelihood: -1.08751e+08\n",
      "     cross entropy: 4.40213\n",
      "        perplexity: 21.1433\n",
      "      posterior p0: 0.0341486\n",
      " posterior al-feat: -0.164527\n",
      "       size counts: 164\n",
      "  1  model al-feat: -0.16702 (tension=4)\n",
      "  2  model al-feat: -0.165716 (tension=4.04988)\n",
      "  3  model al-feat: -0.165099 (tension=4.07366)\n",
      "  4  model al-feat: -0.164803 (tension=4.08509)\n",
      "  5  model al-feat: -0.16466 (tension=4.09062)\n",
      "  6  model al-feat: -0.164591 (tension=4.09329)\n",
      "  7  model al-feat: -0.164558 (tension=4.09458)\n",
      "  8  model al-feat: -0.164542 (tension=4.09521)\n",
      "     final tension: 4.09552\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.45454e+07\n",
      "  log_2 likelihood: -9.31193e+07\n",
      "     cross entropy: 3.76936\n",
      "        perplexity: 13.6361\n",
      "      posterior p0: 0.0354648\n",
      " posterior al-feat: -0.153479\n",
      "       size counts: 164\n",
      "  1  model al-feat: -0.164534 (tension=4.09552)\n",
      "  2  model al-feat: -0.158972 (tension=4.31661)\n",
      "  3  model al-feat: -0.156306 (tension=4.42645)\n",
      "  4  model al-feat: -0.154959 (tension=4.48298)\n",
      "  5  model al-feat: -0.15426 (tension=4.51257)\n",
      "  6  model al-feat: -0.153893 (tension=4.52818)\n",
      "  7  model al-feat: -0.153699 (tension=4.53646)\n",
      "  8  model al-feat: -0.153596 (tension=4.54086)\n",
      "     final tension: 4.5432\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -6.09983e+07\n",
      "  log_2 likelihood: -8.8002e+07\n",
      "     cross entropy: 3.56222\n",
      "        perplexity: 11.8123\n",
      "      posterior p0: 0.0359589\n",
      " posterior al-feat: -0.148868\n",
      "       size counts: 164\n",
      "  1  model al-feat: -0.153542 (tension=4.5432)\n",
      "  2  model al-feat: -0.151379 (tension=4.63667)\n",
      "  3  model al-feat: -0.150235 (tension=4.68689)\n",
      "  4  model al-feat: -0.149618 (tension=4.71424)\n",
      "  5  model al-feat: -0.149281 (tension=4.72923)\n",
      "  6  model al-feat: -0.149096 (tension=4.73748)\n",
      "  7  model al-feat: -0.148994 (tension=4.74204)\n",
      "  8  model al-feat: -0.148938 (tension=4.74455)\n",
      "     final tension: 4.74594\n",
      "ITERATION 5 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      ".................................................. [350000]\n",
      ".................................................. [400000]\n",
      ".................................................. [450000]\n",
      ".................................................. [500000]\n",
      ".................................................. [550000]\n",
      ".................................................. [600000]\n",
      ".................................................. [650000]\n",
      ".................................................. [700000]\n",
      ".................................................. [750000]\n",
      ".................................................. [800000]\n",
      ".................................................. [850000]\n",
      ".................................................. [900000]\n",
      ".................................................. [950000]\n",
      ".................................................. [1000000]\n",
      ".................................................. [1050000]\n",
      ".................................................. [1100000]\n",
      ".................................................. [1150000]\n",
      ".................................................. [1200000]\n",
      ".................................................. [1250000]\n",
      ".................................................. [1300000]\n",
      ".................................................. [1350000]\n",
      ".................................................. [1400000]\n",
      ".................................................. [1450000]\n",
      ".................................................. [1500000]\n",
      ".................................................. [1550000]\n",
      ".................................................. [1600000]\n",
      ".................................................. [1650000]\n",
      ".................................................. [1700000]\n",
      ".................................................. [1750000]\n",
      ".................................................. [1800000]\n",
      ".................................................. [1850000]\n",
      ".................................................. [1900000]\n",
      ".................................................. [1950000]\n",
      ".................................................. [2000000]\n",
      ".................................................. [2050000]\n",
      ".................................................. [2100000]\n",
      ".................................................. [2150000]\n",
      ".................................................. [2200000]\n",
      ".................................................. [2250000]\n",
      ".................................................. [2300000]\n",
      ".................................................. [2350000]\n",
      ".................................................. [2400000]\n",
      ".................................................. [2450000]\n",
      ".................................................. [2500000]\n",
      ".................................................. [2550000]\n",
      ".................................................. [2600000]\n",
      ".................................................. [2650000]\n",
      ".................................................. [2700000]\n",
      ".................................................. [2750000]\n",
      ".................................................. [2800000]\n",
      ".................................................. [2850000]\n",
      ".................................................. [2900000]\n",
      ".................................................. [2950000]\n",
      ".................................................. [3000000]\n",
      ".................................................. [3050000]\n",
      ".................................................. [3100000]\n",
      ".................................................. [3150000]\n",
      ".................................................. [3200000]\n",
      ".................................................. [3250000]\n",
      ".................................................. [3300000]\n",
      ".................................................. [3350000]\n",
      "  log_e likelihood: -5.95773e+07\n",
      "  log_2 likelihood: -8.59519e+07\n",
      "     cross entropy: 3.47923\n",
      "        perplexity: 11.152\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 164\n",
      "Finished alignment.\n"
     ]
    }
   ],
   "source": [
    "!$FAST_ALIGN -i original_man_translation-back_en-de.txt -d -o -v > original_man_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_woman_translation-back_en-de.txt -d -o -v > original_woman_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_girl_translation-back_en-de.txt -d -o -v > original_girl_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_guy_translation-back_en-de.txt -d -o -v > original_guy_translation-back_en-de_fast-aligned.txt\n",
    "!$FAST_ALIGN -i original_boy_translation-back_en-de.txt -d -o -v > original_boy_translation-back_en-de_fast-aligned.txt\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d06b-82f7-4341-be31-eef67f4a415a",
   "metadata": {},
   "source": [
    "- Extract target backtranslated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cb16ca6-8c3b-4f4c-b547-2a72f013ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Count unique translated words to the ambiguous words in backtranslations per source sentence\n",
    "def count_unique_words_alignment_backtranslations(position_word, sentencesN, sourceIn, backtranslationsIn, alignmentsIn_translation, alignmentsIn_backtranslation, output):\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position_word)\n",
    "       \n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    positions_ambiguous_words_translations = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn_translation, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 100):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                positions_ambiguous_words_translations.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                positions_ambiguous_words_translations.append([999])\n",
    "            lineNumber += 1\n",
    "    \n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(backtranslationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            backtranslations.append(line.strip())\n",
    "\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = []\n",
    "    with open(alignmentsIn_backtranslation, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 100):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            positions = positions_ambiguous_words_translations[counter] # exact positions of ambiguous words\n",
    "            list_indices = []\n",
    "            for position in positions:\n",
    "                regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    list_indices.extend([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    list_indices.extend([999])\n",
    "            indices.append(list_indices)\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    backtranslations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "    for backtranslation in backtranslations:\n",
    "        tokens = backtranslation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                #print(lineNumber)\n",
    "                #print(tokens[ind])\n",
    "                #print(ind)\n",
    "                backtranslated_ambiguous_words.add(tokens[ind])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "                backtranslations_ambiguous_words.append(backtranslated_ambiguous_words)\n",
    "                backtranslated_ambiguous_words = set()\n",
    "\n",
    "\n",
    "\n",
    "    #print(backtranslations_ambiguous_words)\n",
    "    print(len(backtranslations_ambiguous_words))\n",
    "\n",
    "    # Here we need to merge the sets for every 10 sets, because we want to see unique words in the nbest 100 backtranslation\n",
    "    backtranslations_ambiguous_words_reduced = []\n",
    "    backtranslated_ambiguous_words = set() # set forces uniqueness\n",
    "    counter = 0\n",
    "    for set_words in backtranslations_ambiguous_words:\n",
    "        backtranslated_ambiguous_words.update(set_words)\n",
    "        counter += 1\n",
    "        if (counter % 100 == 0):\n",
    "            backtranslations_ambiguous_words_reduced.append(backtranslated_ambiguous_words)\n",
    "            backtranslated_ambiguous_words = set()\n",
    "\n",
    "    print(len(backtranslations_ambiguous_words_reduced)) \n",
    "    \n",
    "    # Add results to file\n",
    "\n",
    "    ambiguous_words = []\n",
    "    source = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position_word])\n",
    "\n",
    "    count = 0                \n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(backtranslations_ambiguous_words_reduced[count]), end='\\n', file=fout)\n",
    "            count += 1\n",
    "\n",
    "    unique_backtranslations = 0\n",
    "    for set_words in backtranslations_ambiguous_words_reduced:\n",
    "        unique_backtranslations += len(set_words)\n",
    "        \n",
    "    return unique_backtranslations/sentencesN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d20f678b-e0ad-43b9-8374-fc67c7535c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33500\n",
      "335\n",
      "57.19402985074627\n",
      "33500\n",
      "335\n",
      "60.08955223880597\n",
      "33500\n",
      "335\n",
      "77.84776119402986\n",
      "33500\n",
      "335\n",
      "82.69253731343284\n",
      "33500\n",
      "335\n",
      "70.99402985074627\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_man.en', 'hyp_original_man_back.txt', 'original_man_source-target_en-de_fast-aligned.txt', 'original_man_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_original_man.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_woman.en', 'hyp_original_woman_back.txt', 'original_woman_source-target_en-de_fast-aligned.txt', 'original_woman_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_original_woman.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_girl.en', 'hyp_original_girl_back.txt', 'original_girl_source-target_en-de_fast-aligned.txt', 'original_girl_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_original_girl.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_guy.en', 'hyp_original_guy_back.txt', 'original_guy_source-target_en-de_fast-aligned.txt', 'original_guy_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_original_guy.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_boy.en', 'hyp_original_boy_back.txt', 'original_boy_source-target_en-de_fast-aligned.txt', 'original_boy_translation-back_en-de_fast-aligned.txt', 'unique-words_backtranslations_original_boy.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed81f30-6996-4a42-a7bf-945dd7e74c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## awesome_align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d423a-639b-4f9e-b9a7-a191578edc4e",
   "metadata": {},
   "source": [
    "- Extract the position of the translated ambiguous word from each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b6d21-3ddb-4904-ab00-b7c41e3a9016",
   "metadata": {},
   "source": [
    "- Word alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7344a012-1da5-4328-a801-29a8a45826a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Extracting: 3350000it [55:45, 1001.33it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 1124672it [18:09, 1108.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 3350000it [55:00, 1014.97it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 2369760it [38:39, 1082.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 3350000it [54:53, 1017.09it/s]\n",
      "Loading the dataset...\n",
      "Extracting: 107520it [02:03, 834.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 1245248it [22:48, 884.84it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 3350000it [59:34, 937.25it/s] \n",
      "Loading the dataset...\n",
      "Extracting: 2246528it [37:32, 991.27it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: 3350000it [55:35, 1004.33it/s]\n",
      "Finished alignment.\n"
     ]
    }
   ],
   "source": [
    "# ??? How to set model correctly\n",
    "# MODELS=\"/export/data4/vzhekova/biases-data/En-De/wmt19.en-de.joined-dict.ensemble\"\n",
    "!awesome-align \\\n",
    "    --output_file \"original_man_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_man_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_woman_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_woman_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_girl_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_girl_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_guy_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_guy_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "!awesome-align \\\n",
    "    --output_file \"original_boy_translation-back_en-de_awesome-aligned.txt\" \\\n",
    "    --data_file \"original_boy_translation-back_en-de.txt\" \\\n",
    "    --model_name_or_path bert-base-multilingual-cased \\\n",
    "    --extraction 'softmax' \\\n",
    "    --batch_size 32\n",
    "\n",
    "print(\"Finished alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f0e8-a5b2-44d2-8732-5b68f5be0a6b",
   "metadata": {},
   "source": [
    "- Extract target backtranslated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0a78096-b560-4860-ab0a-e0c30d3b9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33500\n",
      "335\n",
      "36.561194029850746\n",
      "33500\n",
      "335\n",
      "39.05671641791045\n",
      "33500\n",
      "335\n",
      "53.16417910447761\n",
      "33500\n",
      "335\n",
      "69.29850746268657\n",
      "33500\n",
      "335\n",
      "46.02089552238806\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_man.en', 'hyp_original_man_back.txt', 'original_man_source-target_en-de_awesome-aligned.txt', 'original_man_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_original_man.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_woman.en', 'hyp_original_woman_back.txt', 'original_woman_source-target_en-de_awesome-aligned.txt', 'original_woman_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_original_woman.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_girl.en', 'hyp_original_girl_back.txt', 'original_girl_source-target_en-de_awesome-aligned.txt', 'original_girl_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_original_girl.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_guy.en', 'hyp_original_guy_back.txt', 'original_guy_source-target_en-de_awesome-aligned.txt', 'original_guy_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_original_guy.txt'))\n",
    "print(count_unique_words_alignment_backtranslations(1, 335, 'tok.en_original_boy.en', 'hyp_original_boy_back.txt', 'original_boy_source-target_en-de_awesome-aligned.txt', 'original_boy_translation-back_en-de_awesome-aligned.txt', 'unique-words_backtranslations_original_boy.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7aa2d8-87dd-40fb-a52b-b0859436258f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word alignement (translation-translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef887b3-6413-4801-92de-f3f0948d02b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tercom alignement (borrowed from Tu)\n",
    "- https://github.com/TuAnh23/Perturbation-basedQE/blob/master/align_and_analyse_ambiguous_trans.py#L54-L92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab774e7-fd99-417a-90c5-a88658d65ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TuAnh23/Perturbation-basedQE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9bc7a300-37fa-49b3-b1d0-2a65451a5440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity_male/Perturbation-basedQE\n"
     ]
    }
   ],
   "source": [
    "%cd $TERCOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39a6f345-4e59-45e4-98f7-1ff906fce45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import align_and_analyse_ambiguous_trans as tercom\n",
    "import pandas as pd\n",
    "\n",
    "def count_unique_words_tercom_alignment(position, sentencesN, sourceIn, backtranslationsIn):\n",
    "    # List with source sentences; output 100 times to match backtranslation size\n",
    "    source = []\n",
    "    with open(PATH + \"/\" + sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            for i in range(100): # append the source sentence 100 times to match backtranslations later\n",
    "                source.append(line.strip().split()) # split() tokenizes the sentence, because tercom expects tokens     \n",
    "\n",
    "    print(len(source))\n",
    "\n",
    "    # List with original backtranslations\n",
    "    backtranslations = []\n",
    "    with open(PATH + \"/\" + backtranslationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            backtranslations.append(line.strip().split())\n",
    "\n",
    "    print(len(backtranslations))\n",
    "    \n",
    "    # Generate alignments\n",
    "    alignments = tercom.tercom_alignment(source, backtranslations)\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "        \n",
    "    # Extract target translated words to source words\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = []\n",
    "    for align in alignments:\n",
    "        position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "        indices.append([item[1] for item in (item for item in align if not(pd.isna(item[0]))) if item[0] == position][0])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "            counter += 1\n",
    "\n",
    "    print(len(indices))\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = set() # set forces uniqueness\n",
    "    for backtranslation in backtranslations:\n",
    "        backtranslation_index = backtranslations.index(backtranslation)\n",
    "        if not(pd.isna(indices[backtranslation_index])):\n",
    "            translated_ambiguous_words.add(backtranslation[indices[backtranslation_index]])\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "            translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "            translated_ambiguous_words = set()\n",
    "\n",
    "    #print(translations_ambiguous_words)\n",
    "    #print(len(translations_ambiguous_words))\n",
    "\n",
    "    unique_translations = 0\n",
    "    for set_words in translations_ambiguous_words:\n",
    "        unique_translations += len(set_words)\n",
    "        \n",
    "    return unique_translations/sentencesN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc5013c1-cf50-428b-be6f-db240fcd69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33500\n",
      "33500\n",
      "33500\n",
      "4.95820895522388\n",
      "33500\n",
      "33500\n",
      "33500\n",
      "6.582089552238806\n",
      "33500\n",
      "33500\n",
      "33500\n",
      "4.116417910447761\n",
      "33500\n",
      "33500\n",
      "33500\n",
      "10.480597014925372\n",
      "33500\n",
      "33500\n",
      "33500\n",
      "6.447761194029851\n"
     ]
    }
   ],
   "source": [
    "print(count_unique_words_tercom_alignment(1, 335, 'tok.en_original_man.en', 'hyp_original_man_back.txt'))\n",
    "print(count_unique_words_tercom_alignment(1, 335, 'tok.en_original_woman.en', 'hyp_original_woman_back.txt'))\n",
    "print(count_unique_words_tercom_alignment(1, 335, 'tok.en_original_girl.en', 'hyp_original_girl_back.txt'))\n",
    "print(count_unique_words_tercom_alignment(1, 335, 'tok.en_original_guy.en', 'hyp_original_guy_back.txt'))\n",
    "print(count_unique_words_tercom_alignment(1, 335, 'tok.en_original_boy.en', 'hyp_original_boy_back.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8fe19b8-d41f-4058-a373-048e35d1bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/data4/vzhekova/biases-data/Test_De/Statistics/Full_ambiguity/Masking\n"
     ]
    }
   ],
   "source": [
    "%cd $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125d1e1-22bd-401c-aa6e-de29fc5f39a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Word occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b475d6e-6cf6-4d0a-acd8-92bdc22c1557",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba32e36e-44c1-4d7f-9602-a4583aa5525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_translations(filename_tokenized, filename_translations, filename_out, filename_alignments):\n",
    "    \"\"\"\n",
    "    Match alignment indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_alignments, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    translations = []\n",
    "    with open(filename_translations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                translations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 335\n",
    "    counter = 0\n",
    "    for i in range(0, 335): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 99):\n",
    "                alignments = indices_translation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(translations[counter + f])):\n",
    "                                 words_set.add(translations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 100\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 335:\n",
    "            print(source[count] + ' | ' + str(target_words[count]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "721f40e3-99fa-4f8f-bfa1-a9125ed91fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_translations(filename_tokenized, filename_translations, filename_out, filename_alignments):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_alignments, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    translations = []\n",
    "    with open(filename_translations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                translations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 335\n",
    "    counter = 0\n",
    "    for i in range(0, 335): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 99):\n",
    "                alignments = indices_translation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(translations[counter + f])):\n",
    "                                 words_set.add(translations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 100\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0\n",
    "    occurrences = []\n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 335:\n",
    "            occurrences.append([len(target_set) for target_set in target_words[count]])\n",
    "            print(source[count] + ' | ' + str([len(target_set) for target_set in target_words[count]]), end='\\n', file=fout)\n",
    "            count += 1\n",
    "    \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf1e0e-af7d-41bc-8c30-b75fbe1c5dbe",
   "metadata": {},
   "source": [
    "- Investigate the variability of the remaining sentence without the ambiguos word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f128e21-b4a6-47cf-a288-34705c6e5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueness_rest_of_sentence(position, sentencesN, occurrences):\n",
    "    \n",
    "    # Get positions of ambigous words\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "        \n",
    "    # Sum over all sentences and build average     \n",
    "    counter = 0\n",
    "    sum_sent = 0\n",
    "    for occur in occurrences:\n",
    "        position = positions_ambiguous_words[counter]\n",
    "        sum_sent += (sum(occur) - occur[position])/(len(occur) - 1) # sum for every sentence\n",
    "        counter += 1\n",
    "        \n",
    "    return sum_sent/sentencesN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dadbd1b8-92da-4661-bbf7-357a63515b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.445409266375024\n",
      "7.826364687892338\n",
      "8.400750740820985\n",
      "4.993479001731856\n",
      "7.4889848820577525\n"
     ]
    }
   ],
   "source": [
    "extract_word_translations('tok.en_original_man.en', 'hyp_original_man.txt', 'translations_words_original_man.txt', 'original_man_source-target_en-de_awesome-aligned.txt')\n",
    "occurrence_original_man = count_word_translations('tok.en_original_man.en', 'hyp_original_man.txt', 'translations_words_original_man_occurrence.txt', 'original_man_source-target_en-de_awesome-aligned.txt')\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrence_original_man))\n",
    "\n",
    "extract_word_translations('tok.en_original_woman.en', 'hyp_original_woman.txt', 'translations_words_original_woman.txt', 'original_woman_source-target_en-de_awesome-aligned.txt')\n",
    "occurrence_original_woman = count_word_translations('tok.en_original_woman.en', 'hyp_original_woman.txt', 'translations_words_original_woman_occurrence.txt', 'original_woman_source-target_en-de_awesome-aligned.txt')\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrence_original_woman))\n",
    "\n",
    "extract_word_translations('tok.en_original_girl.en', 'hyp_original_girl.txt', 'translations_words_original_girl.txt', 'original_girl_source-target_en-de_awesome-aligned.txt')\n",
    "occurrence_original_girl = count_word_translations('tok.en_original_girl.en', 'hyp_original_girl.txt', 'translations_words_original_girl_occurrence.txt', 'original_girl_source-target_en-de_awesome-aligned.txt')\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrence_original_girl))\n",
    "\n",
    "extract_word_translations('tok.en_original_guy.en', 'hyp_original_guy.txt', 'translations_words_original_guy.txt', 'original_guy_source-target_en-de_awesome-aligned.txt')\n",
    "occurrence_original_guy = count_word_translations('tok.en_original_guy.en', 'hyp_original_guy.txt', 'translations_words_original_guy_occurrence.txt', 'original_guy_source-target_en-de_awesome-aligned.txt')\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrence_original_guy))\n",
    "\n",
    "extract_word_translations('tok.en_original_boy.en', 'hyp_original_boy.txt', 'translations_words_original_boy.txt', 'original_boy_source-target_en-de_awesome-aligned.txt')\n",
    "occurrence_original_boy = count_word_translations('tok.en_original_boy.en', 'hyp_original_boy.txt', 'translations_words_original_boy_occurrence.txt', 'original_boy_source-target_en-de_awesome-aligned.txt')\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrence_original_boy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14aa275-bbed-46e9-8fbd-da11f424ce1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efbbd367-55cd-4cbc-95c1-9f52966ec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alignment_indices_backtranslation(filename_translations, filename_backtranslations):\n",
    "    \"\"\"\n",
    "    Extract alignment indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract alignement indices from translation\n",
    "    indices_translation = []\n",
    "    with open(filename_translations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for i in range(0, len(alignement_tokens)):    \n",
    "                regex = r\"\" + str(i) + r\"-(\\d)\"\n",
    "                if re.findall(regex, line): \n",
    "                    indices_line.append([int(index) for index in re.findall(regex, line)])\n",
    "                else:\n",
    "                    indices_line.append([999])\n",
    "            indices_translation.append(indices_line)\n",
    "       \n",
    "    # Match alignement indices from translation to backtranslation\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices_backtranslation = []\n",
    "    with open(filename_backtranslations, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 100):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            alignement_tokens = line.split()\n",
    "            indices_line = []\n",
    "            for index_list in indices_translation[counter]:\n",
    "                index_matches = []\n",
    "                for index in index_list:\n",
    "                    regex = r\"\" + str(index) + r\"-(\\d)\"\n",
    "                    if re.findall(regex, line): \n",
    "                        index_matches.extend([int(i) for i in re.findall(regex, line)])\n",
    "                    else:\n",
    "                        index_matches.extend([999])\n",
    "                indices_line.append(index_matches)\n",
    "            indices_backtranslation.append(indices_line)\n",
    "            lineNumber += 1 \n",
    "    return indices_backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8811a5e-caac-4ca9-ab1f-df546bf83c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_backtranslations(filename_tokenized, filename_backtranslations, filename_out, indices_backtranslation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(filename_backtranslations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                backtranslations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 335\n",
    "    counter = 0\n",
    "    for i in range(0, 335): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 9999):\n",
    "                alignments = indices_backtranslation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(backtranslations[counter + f])):\n",
    "                                 words_set.add(backtranslations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 10000\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "\n",
    "    count = 0                \n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 335:\n",
    "            print(source[count] + ' | ' + str(target_words[count]), end='\\n', file=fout)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68636e2c-4d46-42de-8e44-5d061468fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_backtranslations(filename_tokenized, filename_backtranslations, filename_out, indices_backtranslation):\n",
    "    \"\"\"\n",
    "    Match alignement indices from translation to backtranslation\n",
    "    \"\"\"\n",
    "    \n",
    "    # List with lengths of the source sentences\n",
    "    source_lengths = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source_lengths.append(len(line.strip().split()))\n",
    "\n",
    "    #print(source_lengths)\n",
    "\n",
    "    # List with backtranslations\n",
    "    backtranslations = []\n",
    "    with open(filename_backtranslations, 'r') as fin:\n",
    "         for line in fin:\n",
    "                backtranslations.append(line.split())\n",
    "\n",
    "    #print(backtranslations)\n",
    "\n",
    "    target_words = [] # list containing lists with translation sets for every word in the source sentences; length 335\n",
    "    counter = 0\n",
    "    for i in range(0, 335): # for every source sentence\n",
    "        source_sent = []\n",
    "        for j in range(0, source_lengths[i]): # for every word in the source sentence\n",
    "            words_set = set()\n",
    "            for  f in range(0, 9999):\n",
    "                alignments = indices_backtranslation[counter + f]        \n",
    "                if (j < len(alignments)):\n",
    "                    for index in alignments[j]:\n",
    "                        if index != 999:\n",
    "                            if (index < len(backtranslations[counter + f])):\n",
    "                                 words_set.add(backtranslations[counter + f][index])\n",
    "            source_sent.append(words_set)\n",
    "        target_words.append(source_sent)\n",
    "        counter += 10000\n",
    "\n",
    "    #print(len(target_words))\n",
    "\n",
    "    # Add results to file\n",
    "\n",
    "    # List with source sentences\n",
    "    source = []\n",
    "    with open(filename_tokenized, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "       \n",
    "    count = 0\n",
    "    occurrences = []\n",
    "    with open(filename_out, 'w') as fout:\n",
    "        while count < 335:\n",
    "            occurrences.append([len(target_set) for target_set in target_words[count]])\n",
    "            print(source[count] + ' | ' + str([len(target_set) for target_set in target_words[count]]), end='\\n', file=fout)\n",
    "            count += 1\n",
    "    \n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98fd42d2-828f-4757-9267-e5983003ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.35787714724059\n",
      "67.18688137489275\n",
      "72.19634030636233\n",
      "52.57545587570169\n",
      "63.96415509668363\n"
     ]
    }
   ],
   "source": [
    "indices_original_man = extract_alignment_indices_backtranslation('original_man_source-target_en-de_awesome-aligned.txt', 'original_man_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_original_man.en', 'hyp_original_man_back.txt', 'backtranslations_words_original_man.txt', indices_original_man)\n",
    "occurrences_original_man = count_word_backtranslations('tok.en_original_man.en', 'hyp_original_man_back.txt', 'backtranslations_words_original_man_occurrence.txt', indices_original_man)\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrences_original_man))\n",
    "\n",
    "indices_original_woman = extract_alignment_indices_backtranslation('original_woman_source-target_en-de_awesome-aligned.txt', 'original_woman_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_original_woman.en', 'hyp_original_woman_back.txt', 'backtranslations_words_original_woman.txt', indices_original_woman)\n",
    "occurrences_original_woman = count_word_backtranslations('tok.en_original_woman.en', 'hyp_original_woman_back.txt', 'backtranslations_words_original_woman_occurrence.txt', indices_original_woman)\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrences_original_woman))\n",
    "\n",
    "indices_original_girl = extract_alignment_indices_backtranslation('original_girl_source-target_en-de_awesome-aligned.txt', 'original_girl_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_original_girl.en', 'hyp_original_girl_back.txt', 'backtranslations_words_original_girl.txt', indices_original_girl)\n",
    "occurrences_original_girl = count_word_backtranslations('tok.en_original_girl.en', 'hyp_original_girl_back.txt', 'backtranslations_words_original_girl_occurrence.txt', indices_original_girl)\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrences_original_girl))\n",
    "\n",
    "indices_original_guy = extract_alignment_indices_backtranslation('original_guy_source-target_en-de_awesome-aligned.txt', 'original_guy_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_original_guy.en', 'hyp_original_guy_back.txt', 'backtranslations_words_original_guy.txt', indices_original_guy)\n",
    "occurrences_original_guy = count_word_backtranslations('tok.en_original_guy.en', 'hyp_original_guy_back.txt', 'backtranslations_words_original_guy_occurrence.txt', indices_original_guy)\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrences_original_guy))\n",
    "\n",
    "indices_original_boy = extract_alignment_indices_backtranslation('original_boy_source-target_en-de_awesome-aligned.txt', 'original_boy_translation-back_en-de_awesome-aligned.txt')\n",
    "extract_word_backtranslations('tok.en_original_boy.en', 'hyp_original_boy_back.txt', 'backtranslations_words_original_boy.txt', indices_original_boy)\n",
    "occurrences_original_boy = count_word_backtranslations('tok.en_original_boy.en', 'hyp_original_boy_back.txt', 'backtranslations_words_original_boy_occurrence.txt', indices_original_boy)\n",
    "print(uniqueness_rest_of_sentence(1, 335, occurrences_original_boy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa9d8d-3fd6-4a95-9591-8af3b42f4b37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Gender statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56c85b2b-de5f-4973-b35f-05e366073d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class GENDER(Enum):\n",
    "    \"\"\"\n",
    "    Enumerate possible genders.\n",
    "    Ignore option resolves to words that should be ignored in particular language\n",
    "    \"\"\"\n",
    "    male = 0\n",
    "    female = 1\n",
    "    neutral = 2\n",
    "    unknown = 3\n",
    "    ignore = 4\n",
    "    \n",
    "# ??? These are not always correct; 'der' could be Dativ or Genitiv for female, 'die' could be plural\n",
    "# !!! There isn't always an article\n",
    "DE_DETERMINERS = {\"der\": GENDER.male, \"ein\": GENDER.male, \"dem\": GENDER.male, \"den\": GENDER.male, \n",
    "                  \"einen\": GENDER.male, \"des\": GENDER.male, \"er\": GENDER.male, \"seiner\": GENDER.male,\n",
    "                  \"ihn\": GENDER.male, \"seinen\": GENDER.male, \"ihm\": GENDER.male, \"ihren\": GENDER.male,\n",
    "                  \"die\": GENDER.female, \"eine\": GENDER.female, \"einer\": GENDER.female, \"seinem\": GENDER.male,\n",
    "                  \"ihrem\": GENDER.male, \"sein\": GENDER.male,\n",
    "                  \"sie\": GENDER.female, \"seine\": GENDER.female, \"ihrer\": GENDER.female, \n",
    "                  \"ihr\": GENDER.neutral, \"ihre\": GENDER.neutral, \"das\": GENDER.neutral,\n",
    "                  \"jemanden\": GENDER.neutral}\n",
    "\n",
    "def get_german_determiners(words):\n",
    "    \"\"\"\n",
    "    Get a list of (gender)\n",
    "    given a list of words.\n",
    "    \"\"\"\n",
    "    determiners = []\n",
    "    for (word_ind, word) in enumerate(words):\n",
    "        word = word.lower()\n",
    "        if word in DE_DETERMINERS:\n",
    "            determiners.append((DE_DETERMINERS[word].name))\n",
    "    return determiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebd8cf96-02b7-4594-8b3a-f06047f00ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male']\n"
     ]
    }
   ],
   "source": [
    "dets = get_german_determiners([\"dem\"])\n",
    "print(dets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9d63e-efae-414e-95a6-767582b657a3",
   "metadata": {},
   "source": [
    "- Calculate gender based on the articles of unique words: how many of the sentences produce both genders, female and male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20718016-7517-4085-bf89-45e457eaa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract articles of target tranlsated words\n",
    "def extract_articles(position, sentencesN, translationsIn, alignmentsIn, sourceIn, output):\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "    \n",
    "    # List with original translations\n",
    "    translations_original = []\n",
    "    with open(translationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            translations_original.append(line.strip())\n",
    "\n",
    "\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 100):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                indices.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                indices.append([999])\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = set() # set forces uniqueness\n",
    "    for translation in translations_original:\n",
    "        tokens = translation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                translated_ambiguous_words.add(tokens[0]) # extract articles; currently assume index 0 for article position, TODO\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "                translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "                translated_ambiguous_words = set()\n",
    "\n",
    "\n",
    "    #print(len(translations_ambiguous_words))\n",
    "    \n",
    "    # Add results to file\n",
    "\n",
    "    # List with original source sentences\n",
    "    source = []\n",
    "    ambiguous_words = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position])\n",
    "\n",
    "    count = 0  \n",
    "    genders = []\n",
    "    male = []\n",
    "    female = []\n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            #print(translations_ambiguous_words[count])\n",
    "            genders.append(set(get_german_determiners(translations_ambiguous_words[count])))\n",
    "            male.append(\"male\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            female.append(\"female\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(get_german_determiners(translations_ambiguous_words[count])), end='\\n', file=fout)\n",
    "            count += 1\n",
    "            \n",
    "    return (sum(1 for i in genders if ('male' in i and 'female' in i)), \n",
    "            male.count(True), female.count(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2dcc149-f38f-419d-b61e-97aa7b657426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 335, 44)\n",
      "(45, 45, 335)\n",
      "(94, 97, 256)\n",
      "(110, 335, 110)\n",
      "(93, 335, 93)\n"
     ]
    }
   ],
   "source": [
    "print(extract_articles(1, 335, 'hyp_original_man.txt', 'original_man_source-target_en-de_awesome-aligned.txt', 'tok.en_original_man.en', 'unique-words_translations_original_man_articles.txt'))\n",
    "print(extract_articles(1, 335, 'hyp_original_woman.txt', 'original_woman_source-target_en-de_awesome-aligned.txt', 'tok.en_original_woman.en', 'unique-words_translations_original_woman_articles.txt'))\n",
    "print(extract_articles(1, 335, 'hyp_original_girl.txt', 'original_girl_source-target_en-de_awesome-aligned.txt', 'tok.en_original_girl.en', 'unique-words_translations_original_girl_articles.txt'))\n",
    "print(extract_articles(1, 335, 'hyp_original_guy.txt', 'original_guy_source-target_en-de_awesome-aligned.txt', 'tok.en_original_guy.en', 'unique-words_translations_original_guy_articles.txt'))\n",
    "print(extract_articles(1, 335, 'hyp_original_boy.txt', 'original_boy_source-target_en-de_awesome-aligned.txt', 'tok.en_original_boy.en', 'unique-words_translations_original_boy_articles.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a57d8-d077-489d-b209-2d0b5147e7de",
   "metadata": {},
   "source": [
    "- Calculate gender in percentage for each sentence: percent of \"male\" vs. female in translations for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "697e9d85-f8ee-443e-b423-00882b38d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract articles of target tranlsated words\n",
    "def extract_articles_percent(position, sentencesN, translationsIn, alignmentsIn, sourceIn, output):\n",
    "    \n",
    "    # Extract the position of the translated ambiguous word from each sentence\n",
    "    positions_ambiguous_words = []\n",
    "    for i in range(sentencesN):\n",
    "        positions_ambiguous_words.append(position)\n",
    "    \n",
    "    # List with original translations\n",
    "    translations_original = []\n",
    "    with open(translationsIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            translations_original.append(line.strip())\n",
    "\n",
    "\n",
    "    lineNumber = 0\n",
    "    counter = 0\n",
    "    indices = [] # a list of lists of indices of translated words for each ambiguous word\n",
    "    with open(alignmentsIn, 'r') as alignments:\n",
    "        for line in alignments:\n",
    "            if (lineNumber == 100):\n",
    "                lineNumber = 0\n",
    "                counter += 1\n",
    "            position = positions_ambiguous_words[counter] # exact position of ambiguous word\n",
    "            regex = r\"\" + str(position) + r\"-(\\d)\"\n",
    "            if re.findall(regex, line): \n",
    "                indices.append([int(index) for index in re.findall(regex, line)])\n",
    "            else:\n",
    "                indices.append([999])\n",
    "            lineNumber += 1\n",
    "\n",
    "    #print(len(indices))\n",
    "    #print(indices)\n",
    "\n",
    "    lineNumber = 0\n",
    "    translations_ambiguous_words = [] # a list of set of translations to each ambiguous word in source\n",
    "    translated_ambiguous_words = [] \n",
    "    for translation in translations_original:\n",
    "        tokens = translation.split(' ')\n",
    "        if 999 not in indices[lineNumber]:\n",
    "            for ind in indices[lineNumber]:\n",
    "                translated_ambiguous_words.append(tokens[0]) # extract articles; currently assume index 0 for article position, TODO\n",
    "        lineNumber += 1\n",
    "        if (lineNumber % 100 == 0):\n",
    "                translations_ambiguous_words.append(translated_ambiguous_words)\n",
    "                translated_ambiguous_words = []\n",
    "\n",
    "\n",
    "    #print(len(translations_ambiguous_words))\n",
    "    \n",
    "    # Add results to file\n",
    "\n",
    "    # List with original source sentences\n",
    "    source = []\n",
    "    ambiguous_words = []\n",
    "    with open(sourceIn, 'r') as fin:\n",
    "        for line in fin:\n",
    "            source.append(line.strip())\n",
    "            tokens = line.split(' ')\n",
    "            ambiguous_words.append(tokens[position])\n",
    "\n",
    "    count = 0  \n",
    "    genders = []\n",
    "    male = []\n",
    "    female = []\n",
    "    with open(output, 'w') as fout:\n",
    "        while count < sentencesN:\n",
    "            #print(translations_ambiguous_words[count])\n",
    "            genders.append(get_german_determiners(translations_ambiguous_words[count]))\n",
    "            male.append(\"male\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            female.append(\"female\" in get_german_determiners(translations_ambiguous_words[count]))\n",
    "            print(source[count] + ' | ' + ambiguous_words[count] + ' | ' + str(get_german_determiners(translations_ambiguous_words[count])), end='\\n', file=fout)\n",
    "            count += 1\n",
    "     \n",
    "    #print(genders)\n",
    "    return (sum([i.count('male')/100 for i in genders])/sentencesN*100, \n",
    "            sum([i.count('female')/100 for i in genders])/sentencesN*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d9e30d9-b142-4917-9809-dc649469e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83.80000000000004, 0.4626865671641793)\n",
      "(0.5940298507462689, 88.57014925373133)\n",
      "(0.7552238805970143, 5.659701492537309)\n",
      "(78.58805970149255, 0.97910447761194)\n",
      "(86.4805970149254, 0.7522388059701495)\n"
     ]
    }
   ],
   "source": [
    "print(extract_articles_percent(1, 335, 'hyp_original_man.txt', 'original_man_source-target_en-de_awesome-aligned.txt', 'tok.en_original_man.en', 'unique-words_translations_original_man_articles.txt'))\n",
    "print(extract_articles_percent(1, 335, 'hyp_original_woman.txt', 'original_woman_source-target_en-de_awesome-aligned.txt', 'tok.en_original_woman.en', 'unique-words_translations_original_woman_articles.txt'))\n",
    "print(extract_articles_percent(1, 335, 'hyp_original_girl.txt', 'original_girl_source-target_en-de_awesome-aligned.txt', 'tok.en_original_girl.en', 'unique-words_translations_original_girl_articles.txt'))\n",
    "print(extract_articles_percent(1, 335, 'hyp_original_guy.txt', 'original_guy_source-target_en-de_awesome-aligned.txt', 'tok.en_original_guy.en', 'unique-words_translations_original_guy_articles.txt'))\n",
    "print(extract_articles_percent(1, 335, 'hyp_original_boy.txt', 'original_boy_source-target_en-de_awesome-aligned.txt', 'tok.en_original_boy.en', 'unique-words_translations_original_boy_articles.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa3d3c-5905-4fa2-bcd7-7f9425f829ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
